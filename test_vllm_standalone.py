#!/usr/bin/env python3
"""
ç‹¬ç«‹çš„ vLLM Provider æµ‹è¯•è„šæœ¬
ç›´æ¥æµ‹è¯• vLLM çš„ OpenAI å…¼å®¹æ¥å£
"""

import asyncio
import aiohttp
import requests
import json
from typing import List, Dict, Any, Optional, AsyncGenerator

class SimpleVLLMClient:
    """ç®€åŒ–çš„ vLLM å®¢æˆ·ç«¯ï¼Œç”¨äºæµ‹è¯•"""
    
    def __init__(self, base_url: str, model: str, api_key: str = "EMPTY"):
        self.base_url = base_url.rstrip('/')
        self.model = model
        self.api_key = api_key
        self.headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
    
    def is_available(self) -> bool:
        """æ£€æŸ¥æœåŠ¡æ˜¯å¦å¯ç”¨"""
        try:
            response = requests.get(f"{self.base_url}/models", timeout=5)
            return response.status_code == 200
        except Exception:
            return False
    
    def list_models(self) -> List[str]:
        """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
        try:
            response = requests.get(f"{self.base_url}/models", headers=self.headers, timeout=10)
            if response.status_code == 200:
                data = response.json()
                return [model["id"] for model in data.get("data", [])]
            return []
        except Exception as e:
            print(f"è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")
            return []
    
    def chat(self, messages: List[Dict[str, str]], **kwargs) -> str:
        """èŠå¤©æ¥å£"""
        payload = {
            "model": self.model,
            "messages": messages,
            "temperature": kwargs.get("temperature", 0.0),
            "max_tokens": kwargs.get("max_tokens", 256),
            "top_p": kwargs.get("top_p", 1.0)
        }
        
        try:
            response = requests.post(
                f"{self.base_url}/chat/completions",
                headers=self.headers,
                json=payload,
                timeout=30
            )
            
            if response.status_code == 200:
                data = response.json()
                return data["choices"][0]["message"]["content"]
            else:
                raise Exception(f"API é”™è¯¯: {response.status_code} - {response.text}")
                
        except Exception as e:
            raise Exception(f"èŠå¤©è¯·æ±‚å¤±è´¥: {e}")
    
    def stream(self, messages: List[Dict[str, str]], **kwargs):
        """æµå¼èŠå¤©æ¥å£"""
        payload = {
            "model": self.model,
            "messages": messages,
            "temperature": kwargs.get("temperature", 0.0),
            "max_tokens": kwargs.get("max_tokens", 256),
            "top_p": kwargs.get("top_p", 1.0),
            "stream": True
        }
        
        try:
            response = requests.post(
                f"{self.base_url}/chat/completions",
                headers=self.headers,
                json=payload,
                stream=True,
                timeout=30
            )
            
            if response.status_code != 200:
                raise Exception(f"API é”™è¯¯: {response.status_code} - {response.text}")
            
            for line in response.iter_lines():
                if line:
                    line = line.decode('utf-8')
                    if line.startswith('data: '):
                        data_str = line[6:]
                        if data_str.strip() == '[DONE]':
                            break
                        try:
                            data = json.loads(data_str)
                            if 'choices' in data and len(data['choices']) > 0:
                                delta = data['choices'][0].get('delta', {})
                                if 'content' in delta:
                                    yield delta['content']
                        except json.JSONDecodeError:
                            continue
                            
        except Exception as e:
            raise Exception(f"æµå¼è¯·æ±‚å¤±è´¥: {e}")
    
    async def async_chat(self, messages: List[Dict[str, str]], **kwargs) -> str:
        """å¼‚æ­¥èŠå¤©æ¥å£"""
        payload = {
            "model": self.model,
            "messages": messages,
            "temperature": kwargs.get("temperature", 0.0),
            "max_tokens": kwargs.get("max_tokens", 256),
            "top_p": kwargs.get("top_p", 1.0)
        }
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{self.base_url}/chat/completions",
                    headers=self.headers,
                    json=payload,
                    timeout=aiohttp.ClientTimeout(total=30)
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        return data["choices"][0]["message"]["content"]
                    else:
                        text = await response.text()
                        raise Exception(f"API é”™è¯¯: {response.status} - {text}")
                        
        except Exception as e:
            raise Exception(f"å¼‚æ­¥èŠå¤©è¯·æ±‚å¤±è´¥: {e}")
    
    async def async_stream(self, messages: List[Dict[str, str]], **kwargs) -> AsyncGenerator[str, None]:
        """å¼‚æ­¥æµå¼èŠå¤©æ¥å£"""
        payload = {
            "model": self.model,
            "messages": messages,
            "temperature": kwargs.get("temperature", 0.0),
            "max_tokens": kwargs.get("max_tokens", 256),
            "top_p": kwargs.get("top_p", 1.0),
            "stream": True
        }
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{self.base_url}/chat/completions",
                    headers=self.headers,
                    json=payload,
                    timeout=aiohttp.ClientTimeout(total=30)
                ) as response:
                    if response.status != 200:
                        text = await response.text()
                        raise Exception(f"API é”™è¯¯: {response.status} - {text}")
                    
                    async for line in response.content:
                        line = line.decode('utf-8').strip()
                        if line.startswith('data: '):
                            data_str = line[6:]
                            if data_str.strip() == '[DONE]':
                                break
                            try:
                                data = json.loads(data_str)
                                if 'choices' in data and len(data['choices']) > 0:
                                    delta = data['choices'][0].get('delta', {})
                                    if 'content' in delta:
                                        yield delta['content']
                            except json.JSONDecodeError:
                                continue
                                
        except Exception as e:
            raise Exception(f"å¼‚æ­¥æµå¼è¯·æ±‚å¤±è´¥: {e}")

def test_vllm_basic():
    """æµ‹è¯•åŸºæœ¬åŠŸèƒ½"""
    print("=== vLLM åŸºæœ¬åŠŸèƒ½æµ‹è¯• ===")
    
    client = SimpleVLLMClient(
        base_url="http://127.0.0.1:8001/v1",
        model="qwen2_5_0_5b"
    )
    
    try:
        # æµ‹è¯•è¿æ¥
        if not client.is_available():
            print("âœ— vLLM æœåŠ¡ä¸å¯ç”¨")
            print("è¯·ç¡®ä¿ vLLM æœåŠ¡å·²å¯åŠ¨:")
            print("python -m vllm.entrypoints.openai.api_server \\")
            print("  --model Qwen/Qwen2.5-0.5B-Instruct \\")
            print("  --served-model-name qwen2_5_0_5b \\")
            print("  --dtype float16 \\")
            print("  --max-model-len 4096 \\")
            print("  --gpu-memory-utilization 0.80 \\")
            print("  --port 8001")
            return False
        
        print("âœ“ vLLM æœåŠ¡è¿æ¥æˆåŠŸ")
        
        # æµ‹è¯•æ¨¡å‹åˆ—è¡¨
        models = client.list_models()
        print(f"âœ“ å¯ç”¨æ¨¡å‹: {models}")
        
        # æµ‹è¯•èŠå¤©
        messages = [{"role": "user", "content": "ä½ å¥½ï¼Œè¯·ç®€å•ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"}]
        response = client.chat(messages, max_tokens=128)
        print(f"âœ“ èŠå¤©æµ‹è¯•æˆåŠŸ: {response[:100]}...")
        
        # æµ‹è¯•æµå¼è¾“å‡º
        print("\n=== æµå¼è¾“å‡ºæµ‹è¯• ===")
        messages = [{"role": "user", "content": "è¯·ç”¨ä¸€å¥è¯ä»‹ç»äººå·¥æ™ºèƒ½ã€‚"}]
        for chunk in client.stream(messages, max_tokens=64):
            print(chunk, end='', flush=True)
        print("\nâœ“ æµå¼è¾“å‡ºæµ‹è¯•å®Œæˆ")
        
        return True
        
    except Exception as e:
        print(f"âœ— æµ‹è¯•å¤±è´¥: {e}")
        return False

async def test_vllm_async():
    """æµ‹è¯•å¼‚æ­¥åŠŸèƒ½"""
    print("\n=== vLLM å¼‚æ­¥åŠŸèƒ½æµ‹è¯• ===")
    
    client = SimpleVLLMClient(
        base_url="http://127.0.0.1:8001/v1",
        model="qwen2_5_0_5b"
    )
    
    try:
        if not client.is_available():
            print("âœ— vLLM æœåŠ¡ä¸å¯ç”¨ï¼Œè·³è¿‡å¼‚æ­¥æµ‹è¯•")
            return False
        
        # æµ‹è¯•å¼‚æ­¥èŠå¤©
        messages = [{"role": "user", "content": "è¯·ç”¨ä¸€å¥è¯ä»‹ç»æœºå™¨å­¦ä¹ ã€‚"}]
        response = await client.async_chat(messages, max_tokens=64)
        print(f"âœ“ å¼‚æ­¥èŠå¤©æµ‹è¯•æˆåŠŸ: {response}")
        
        # æµ‹è¯•å¼‚æ­¥æµå¼è¾“å‡º
        print("\n=== å¼‚æ­¥æµå¼è¾“å‡ºæµ‹è¯• ===")
        messages = [{"role": "user", "content": "è¯·ç®€å•è§£é‡Šä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ã€‚"}]
        async for chunk in client.async_stream(messages, max_tokens=64):
            print(chunk, end='', flush=True)
        print("\nâœ“ å¼‚æ­¥æµå¼è¾“å‡ºæµ‹è¯•å®Œæˆ")
        
        return True
        
    except Exception as e:
        print(f"âœ— å¼‚æ­¥æµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("å¼€å§‹ vLLM ç‹¬ç«‹æµ‹è¯•...\n")
    
    # åŸºæœ¬åŠŸèƒ½æµ‹è¯•
    basic_success = test_vllm_basic()
    
    # å¼‚æ­¥åŠŸèƒ½æµ‹è¯•
    async_success = asyncio.run(test_vllm_async())
    
    print("\n=== æµ‹è¯•æ€»ç»“ ===")
    print(f"åŸºæœ¬åŠŸèƒ½: {'âœ“ é€šè¿‡' if basic_success else 'âœ— å¤±è´¥'}")
    print(f"å¼‚æ­¥åŠŸèƒ½: {'âœ“ é€šè¿‡' if async_success else 'âœ— å¤±è´¥'}")
    
    if basic_success and async_success:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼vLLM æœåŠ¡å·¥ä½œæ­£å¸¸ã€‚")
        return 0
    else:
        print("\nâŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ vLLM æœåŠ¡çŠ¶æ€ã€‚")
        return 1

if __name__ == "__main__":
    exit(main())