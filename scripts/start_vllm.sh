#!/bin/bash

# vLLM ÊúçÂä°ÂêØÂä®ËÑöÊú¨
# ÊîØÊåÅÂêØÂä®‰∏çÂêåËßÑÊ®°ÁöÑÊ®°ÂûãÂíåÂ§öGPUÈÖçÁΩÆ

set -e

# ÈªòËÆ§ÈÖçÁΩÆ
DEFAULT_MODEL="Qwen/Qwen2.5-0.5B-Instruct"
DEFAULT_SERVED_NAME="qwen2_5_0_5b"
DEFAULT_PORT=8001
DEFAULT_GPU_MEMORY=0.80
DEFAULT_MAX_MODEL_LEN=4096
DEFAULT_DTYPE="float16"

# È¢úËâ≤ÂÆö‰πâ
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Êó•ÂøóÁõÆÂΩï
LOG_DIR="./logs/vllm"
mkdir -p "$LOG_DIR"

# ÊâìÂç∞Â∏ÆÂä©‰ø°ÊÅØ
show_help() {
    echo "vLLM ÊúçÂä°ÂêØÂä®ËÑöÊú¨"
    echo ""
    echo "Áî®Ê≥ï: $0 [ÈÄâÈ°π] [ÂëΩ‰ª§]"
    echo ""
    echo "ÂëΩ‰ª§:"
    echo "  start-tiny     ÂêØÂä® Qwen2.5-0.5B Ê®°Âûã (ÈªòËÆ§)"
    echo "  start-small    ÂêØÂä® Qwen2.5-1.5B Ê®°Âûã"
    echo "  start-medium   ÂêØÂä® Qwen2.5-7B Ê®°Âûã"
    echo "  start-large    ÂêØÂä® Qwen2.5-14B Ê®°Âûã"
    echo "  start-custom   ÂêØÂä®Ëá™ÂÆö‰πâÊ®°Âûã"
    echo "  stop           ÂÅúÊ≠¢ vLLM ÊúçÂä°"
    echo "  status         Êü•ÁúãÊúçÂä°Áä∂ÊÄÅ"
    echo "  test           ÊµãËØïÊúçÂä°ÂèØÁî®ÊÄß"
    echo "  logs           Êü•ÁúãÊó•Âøó"
    echo "  benchmark      ËøêË°åÂéãÊµã"
    echo "  help           ÊòæÁ§∫Ê≠§Â∏ÆÂä©‰ø°ÊÅØ"
    echo ""
    echo "ÈÄâÈ°π:"
    echo "  --model MODEL              Ê®°ÂûãË∑ØÂæÑÊàñÂêçÁß∞"
    echo "  --served-name NAME          ÊúçÂä°Ê®°ÂûãÂêçÁß∞"
    echo "  --port PORT                 ÊúçÂä°Á´ØÂè£ (ÈªòËÆ§: 8001)"
    echo "  --gpu-memory RATIO          GPUÂÜÖÂ≠ò‰ΩøÁî®Áéá (ÈªòËÆ§: 0.80)"
    echo "  --max-model-len LENGTH      ÊúÄÂ§ßÊ®°ÂûãÈïøÂ∫¶ (ÈªòËÆ§: 4096)"
    echo "  --tensor-parallel SIZE      Âº†ÈáèÂπ∂Ë°åÂ§ßÂ∞è (Â§öGPU)"
    echo "  --dtype DTYPE               Êï∞ÊçÆÁ±ªÂûã (ÈªòËÆ§: float16)"
    echo "  --background                ÂêéÂè∞ËøêË°å"
    echo ""
    echo "Á§∫‰æã:"
    echo "  $0 start-tiny                    # ÂêØÂä®Â∞èÊ®°Âûã"
    echo "  $0 start-medium --port 8002      # Âú®8002Á´ØÂè£ÂêØÂä®‰∏≠Á≠âÊ®°Âûã"
    echo "  $0 start-custom --model /path/to/model --tensor-parallel 2"
    echo "  $0 test --port 8001              # ÊµãËØï8001Á´ØÂè£ÁöÑÊúçÂä°"
}

# Ëß£ÊûêÂëΩ‰ª§Ë°åÂèÇÊï∞
MODEL="$DEFAULT_MODEL"
SERVED_NAME="$DEFAULT_SERVED_NAME"
PORT="$DEFAULT_PORT"
GPU_MEMORY="$DEFAULT_GPU_MEMORY"
MAX_MODEL_LEN="$DEFAULT_MAX_MODEL_LEN"
DTYPE="$DEFAULT_DTYPE"
TENSOR_PARALLEL=""
BACKGROUND=false
COMMAND="start-tiny"

while [[ $# -gt 0 ]]; do
    case $1 in
        --model)
            MODEL="$2"
            shift 2
            ;;
        --served-name)
            SERVED_NAME="$2"
            shift 2
            ;;
        --port)
            PORT="$2"
            shift 2
            ;;
        --gpu-memory)
            GPU_MEMORY="$2"
            shift 2
            ;;
        --max-model-len)
            MAX_MODEL_LEN="$2"
            shift 2
            ;;
        --tensor-parallel)
            TENSOR_PARALLEL="$2"
            shift 2
            ;;
        --dtype)
            DTYPE="$2"
            shift 2
            ;;
        --background)
            BACKGROUND=true
            shift
            ;;
        start-tiny|start-small|start-medium|start-large|start-custom|stop|status|test|logs|benchmark|help)
            COMMAND="$1"
            shift
            ;;
        *)
            echo "Êú™Áü•ÂèÇÊï∞: $1"
            show_help
            exit 1
            ;;
    esac
done

# Ê£ÄÊü• vLLM ÊòØÂê¶ÂÆâË£Ö
check_vllm() {
    if ! python -c "import vllm" 2>/dev/null; then
        echo -e "${RED}ÈîôËØØ: vLLM Êú™ÂÆâË£Ö${NC}"
        echo "ËØ∑ÂÆâË£Ö vLLM: pip install vllm"
        exit 1
    fi
}

# Ê£ÄÊü• GPU
check_gpu() {
    if ! nvidia-smi >/dev/null 2>&1; then
        echo -e "${YELLOW}Ë≠¶Âëä: Êú™Ê£ÄÊµãÂà∞ NVIDIA GPU${NC}"
        echo "vLLM Â∞Ü‰ΩøÁî® CPU Ê®°ÂºèÔºåÊÄßËÉΩÂèØËÉΩËæÉ‰Ωé"
    else
        echo -e "${GREEN}Ê£ÄÊµãÂà∞ NVIDIA GPU:${NC}"
        nvidia-smi --query-gpu=name,memory.total --format=csv,noheader,nounits | nl
    fi
}

# Ëé∑ÂèñËøõÁ®ãID
get_vllm_pid() {
    local port=$1
    pgrep -f "vllm.entrypoints.openai.api_server.*--port $port" || echo ""
}

# ÂêØÂä® vLLM ÊúçÂä°
start_vllm() {
    local model=$1
    local served_name=$2
    local port=$3
    
    echo -e "${BLUE}ÂêØÂä® vLLM ÊúçÂä°...${NC}"
    echo "Ê®°Âûã: $model"
    echo "ÊúçÂä°ÂêçÁß∞: $served_name"
    echo "Á´ØÂè£: $port"
    echo "GPUÂÜÖÂ≠ò‰ΩøÁî®Áéá: $GPU_MEMORY"
    echo "ÊúÄÂ§ßÊ®°ÂûãÈïøÂ∫¶: $MAX_MODEL_LEN"
    echo "Êï∞ÊçÆÁ±ªÂûã: $DTYPE"
    
    # Ê£ÄÊü•Á´ØÂè£ÊòØÂê¶Ë¢´Âç†Áî®
    if lsof -i :$port >/dev/null 2>&1; then
        echo -e "${RED}ÈîôËØØ: Á´ØÂè£ $port Â∑≤Ë¢´Âç†Áî®${NC}"
        echo "ËØ∑‰ΩøÁî®ÂÖ∂‰ªñÁ´ØÂè£ÊàñÂÅúÊ≠¢Âç†Áî®ËØ•Á´ØÂè£ÁöÑÊúçÂä°"
        exit 1
    fi
    
    # ÊûÑÂª∫ÂêØÂä®ÂëΩ‰ª§
    local cmd="python -m vllm.entrypoints.openai.api_server"
    cmd="$cmd --model $model"
    cmd="$cmd --served-model-name $served_name"
    cmd="$cmd --dtype $DTYPE"
    cmd="$cmd --max-model-len $MAX_MODEL_LEN"
    cmd="$cmd --gpu-memory-utilization $GPU_MEMORY"
    cmd="$cmd --port $port"
    
    # Ê∑ªÂä†Âº†ÈáèÂπ∂Ë°å
    if [[ -n "$TENSOR_PARALLEL" ]]; then
        cmd="$cmd --tensor-parallel-size $TENSOR_PARALLEL"
        echo "Âº†ÈáèÂπ∂Ë°åÂ§ßÂ∞è: $TENSOR_PARALLEL"
    fi
    
    local log_file="$LOG_DIR/vllm_${served_name}_${port}.log"
    
    echo "Êó•ÂøóÊñá‰ª∂: $log_file"
    echo "ÂêØÂä®ÂëΩ‰ª§: $cmd"
    echo ""
    
    if [[ "$BACKGROUND" == "true" ]]; then
        echo -e "${YELLOW}ÂêéÂè∞ÂêØÂä®‰∏≠...${NC}"
        nohup $cmd > "$log_file" 2>&1 &
        local pid=$!
        echo "ËøõÁ®ãID: $pid"
        echo "‰ΩøÁî® '$0 logs --port $port' Êü•ÁúãÊó•Âøó"
        echo "‰ΩøÁî® '$0 status --port $port' Ê£ÄÊü•Áä∂ÊÄÅ"
        
        # Á≠âÂæÖÊúçÂä°ÂêØÂä®
        echo "Á≠âÂæÖÊúçÂä°ÂêØÂä®..."
        for i in {1..30}; do
            if curl -s "http://127.0.0.1:$port/v1/models" >/dev/null 2>&1; then
                echo -e "${GREEN}‚úÖ vLLM ÊúçÂä°ÂêØÂä®ÊàêÂäüÔºÅ${NC}"
                echo "API Âú∞ÂùÄ: http://127.0.0.1:$port/v1"
                return 0
            fi
            sleep 2
            echo -n "."
        done
        echo -e "\n${YELLOW}‚ö†Ô∏è  ÊúçÂä°ÂêØÂä®Êó∂Èó¥ËæÉÈïøÔºåËØ∑Ê£ÄÊü•Êó•Âøó${NC}"
    else
        echo -e "${YELLOW}ÂâçÂè∞ÂêØÂä®‰∏≠... (Êåâ Ctrl+C ÂÅúÊ≠¢)${NC}"
        $cmd
    fi
}

# ÂÅúÊ≠¢ vLLM ÊúçÂä°
stop_vllm() {
    local port=${1:-$PORT}
    echo -e "${BLUE}ÂÅúÊ≠¢Á´ØÂè£ $port ‰∏äÁöÑ vLLM ÊúçÂä°...${NC}"
    
    local pid=$(get_vllm_pid $port)
    if [[ -n "$pid" ]]; then
        echo "ÊâæÂà∞ËøõÁ®ã ID: $pid"
        kill $pid
        sleep 2
        
        # Âº∫Âà∂ÊùÄÊ≠ªÂ¶ÇÊûúËøòÂú®ËøêË°å
        if kill -0 $pid 2>/dev/null; then
            echo "Âº∫Âà∂ÂÅúÊ≠¢ËøõÁ®ã..."
            kill -9 $pid
        fi
        
        echo -e "${GREEN}‚úÖ vLLM ÊúçÂä°Â∑≤ÂÅúÊ≠¢${NC}"
    else
        echo -e "${YELLOW}‚ö†Ô∏è  Êú™ÊâæÂà∞ËøêË°åÂú®Á´ØÂè£ $port ÁöÑ vLLM ÊúçÂä°${NC}"
    fi
}

# Êü•ÁúãÊúçÂä°Áä∂ÊÄÅ
show_status() {
    local port=${1:-$PORT}
    echo -e "${BLUE}Ê£ÄÊü•Á´ØÂè£ $port ‰∏äÁöÑ vLLM ÊúçÂä°Áä∂ÊÄÅ...${NC}"
    
    local pid=$(get_vllm_pid $port)
    if [[ -n "$pid" ]]; then
        echo -e "${GREEN}‚úÖ ÊúçÂä°Ê≠£Âú®ËøêË°å${NC}"
        echo "ËøõÁ®ãID: $pid"
        echo "Á´ØÂè£: $port"
        
        # Ê£ÄÊü•APIÂèØÁî®ÊÄß
        if curl -s "http://127.0.0.1:$port/v1/models" >/dev/null 2>&1; then
            echo -e "${GREEN}‚úÖ API ÂèØËÆøÈóÆ${NC}"
            echo "API Âú∞ÂùÄ: http://127.0.0.1:$port/v1"
        else
            echo -e "${YELLOW}‚ö†Ô∏è  API ÊöÇ‰∏çÂèØËÆøÈóÆÔºàÂèØËÉΩÊ≠£Âú®ÂêØÂä®‰∏≠Ôºâ${NC}"
        fi
    else
        echo -e "${RED}‚ùå ÊúçÂä°Êú™ËøêË°å${NC}"
    fi
}

# ÊµãËØïÊúçÂä°
test_service() {
    local port=${1:-$PORT}
    echo -e "${BLUE}ÊµãËØïÁ´ØÂè£ $port ‰∏äÁöÑ vLLM ÊúçÂä°...${NC}"
    
    # ÊµãËØïÊ®°ÂûãÂàóË°®
    echo "1. ÊµãËØïÊ®°ÂûãÂàóË°® API..."
    if curl -s "http://127.0.0.1:$port/v1/models" | jq . 2>/dev/null; then
        echo -e "${GREEN}‚úÖ Ê®°ÂûãÂàóË°® API Ê≠£Â∏∏${NC}"
    else
        echo -e "${RED}‚ùå Ê®°ÂûãÂàóË°® API Â§±Ë¥•${NC}"
        return 1
    fi
    
    # ÊµãËØïËÅäÂ§©ÂÆåÊàê
    echo "\n2. ÊµãËØïËÅäÂ§©ÂÆåÊàê API..."
    local test_response=$(curl -s "http://127.0.0.1:$port/v1/chat/completions" \
        -H "Content-Type: application/json" \
        -H "Authorization: Bearer EMPTY" \
        -d '{
            "model": "'$SERVED_NAME'",
            "messages": [{"role": "user", "content": "Hello!"}],
            "max_tokens": 50,
            "temperature": 0.0
        }')
    
    if echo "$test_response" | jq -e '.choices[0].message.content' >/dev/null 2>&1; then
        echo -e "${GREEN}‚úÖ ËÅäÂ§©ÂÆåÊàê API Ê≠£Â∏∏${NC}"
        echo "ÂìçÂ∫î: $(echo "$test_response" | jq -r '.choices[0].message.content')"
    else
        echo -e "${RED}‚ùå ËÅäÂ§©ÂÆåÊàê API Â§±Ë¥•${NC}"
        echo "ÂìçÂ∫î: $test_response"
        return 1
    fi
    
    echo -e "\n${GREEN}üéâ ÊâÄÊúâÊµãËØïÈÄöËøáÔºÅ${NC}"
}

# Êü•ÁúãÊó•Âøó
show_logs() {
    local port=${1:-$PORT}
    local log_file="$LOG_DIR/vllm_${SERVED_NAME}_${port}.log"
    
    if [[ -f "$log_file" ]]; then
        echo -e "${BLUE}ÊòæÁ§∫Êó•Âøó: $log_file${NC}"
        tail -f "$log_file"
    else
        echo -e "${YELLOW}‚ö†Ô∏è  Êó•ÂøóÊñá‰ª∂‰∏çÂ≠òÂú®: $log_file${NC}"
        echo "ÂèØÁî®ÁöÑÊó•ÂøóÊñá‰ª∂:"
        ls -la "$LOG_DIR/" 2>/dev/null || echo "Êó†Êó•ÂøóÊñá‰ª∂"
    fi
}

# ËøêË°åÂéãÊµã
run_benchmark() {
    local port=${1:-$PORT}
    echo -e "${BLUE}ËøêË°å vLLM ÂéãÊµã...${NC}"
    
    if [[ -f "benchmark_vllm.py" ]]; then
        python benchmark_vllm.py --base-url "http://127.0.0.1:$port/v1" --model "$SERVED_NAME" --progressive
    else
        echo -e "${RED}ÈîôËØØ: Êú™ÊâæÂà∞ benchmark_vllm.py${NC}"
        echo "ËØ∑Á°Æ‰øùÂéãÊµãËÑöÊú¨Â≠òÂú®"
    fi
}

# ‰∏ªÈÄªËæë
case $COMMAND in
    start-tiny)
        check_vllm
        check_gpu
        MODEL="Qwen/Qwen2.5-0.5B-Instruct"
        SERVED_NAME="qwen2_5_0_5b"
        BACKGROUND=true
        start_vllm "$MODEL" "$SERVED_NAME" "$PORT"
        ;;
    start-small)
        check_vllm
        check_gpu
        MODEL="Qwen/Qwen2.5-1.5B-Instruct"
        SERVED_NAME="qwen2_5_1_5b"
        PORT=${PORT:-8002}
        BACKGROUND=true
        start_vllm "$MODEL" "$SERVED_NAME" "$PORT"
        ;;
    start-medium)
        check_vllm
        check_gpu
        MODEL="Qwen/Qwen2.5-7B-Instruct"
        SERVED_NAME="qwen2_5_7b"
        PORT=${PORT:-8003}
        BACKGROUND=true
        start_vllm "$MODEL" "$SERVED_NAME" "$PORT"
        ;;
    start-large)
        check_vllm
        check_gpu
        MODEL="Qwen/Qwen2.5-14B-Instruct"
        SERVED_NAME="qwen2_5_14b"
        PORT=${PORT:-8004}
        TENSOR_PARALLEL=${TENSOR_PARALLEL:-2}
        BACKGROUND=true
        start_vllm "$MODEL" "$SERVED_NAME" "$PORT"
        ;;
    start-custom)
        check_vllm
        check_gpu
        BACKGROUND=true
        start_vllm "$MODEL" "$SERVED_NAME" "$PORT"
        ;;
    stop)
        stop_vllm "$PORT"
        ;;
    status)
        show_status "$PORT"
        ;;
    test)
        test_service "$PORT"
        ;;
    logs)
        show_logs "$PORT"
        ;;
    benchmark)
        run_benchmark "$PORT"
        ;;
    help)
        show_help
        ;;
    *)
        echo "Êú™Áü•ÂëΩ‰ª§: $COMMAND"
        show_help
        exit 1
        ;;
esac