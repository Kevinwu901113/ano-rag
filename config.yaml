# RAG System Configuration

# Document Processing
document:
  chunk_size: 128
  overlap: 50
  batch_size: 32
  supported_formats: ["json", "jsonl", "docx"]
  
# Embedding Model
embedding:
  model_name: "BAAI/bge-m3"  # or "nomic-ai/nomic-embed-text-v1"
  batch_size: 64
  max_length: 512
  device: "cuda"
  normalize: true
  
# Clustering
clustering:
  algorithm: "hdbscan"  # or "kmeans", "dbscan"
  min_cluster_size: 5
  min_samples: 3
  metric: "euclidean"
  use_gpu: true
  
# Graph Construction
graph:
  k_hop: 3
  edge_types:
    - "reference"
    - "entity_coexistence"
    - "context_relation"
    - "topic_link"
  centrality_weight: 0.3
  path_value_weight: 0.2
  similarity_threshold: 0.7
  entity_cooccurrence_threshold: 2
  context_window: 3
  batch_size: 64
  max_relations_per_note: 10
  weights:
    reference: 1.0
    entity_coexistence: 0.8
    context_relation: 0.6
    topic_relation: 0.7
    semantic_similarity: 0.5
    personal_relation: 0.9

# Multi-hop Reasoning
multi_hop:
  enabled: true
  max_reasoning_hops: 3
  max_paths: 10
  min_path_score: 0.3
  path_diversity_threshold: 0.7
  relation_types:
    causal:
      weight: 1.2
      confidence_threshold: 0.7
    temporal:
      weight: 1.1
      confidence_threshold: 0.6
    definition:
      weight: 1.3
      confidence_threshold: 0.8
  llm_relation_extraction:
    enabled: true
    batch_size: 16
    max_pairs_per_batch: 50
  topic_group_llm:
    enabled: true
    min_group_size: 3
    max_notes: 5
  
# Query Processing
query:
  rewrite_enabled: true
  split_multi_queries: true
  placeholder_split: false
  add_prior_knowledge: false  # switch for LLM hallucination prevention
  parallel_processing: true
  
# Context Scheduler Weights
context_scheduler:
  semantic_weight: 0.3      # t1
  graph_weight: 0.25        # t2  
  topic_weight: 0.2         # t3
  feedback_weight: 0.15     # t4
  redundancy_penalty: 0.1   # t5
  top_n_notes: 10
  
# LLM Configuration
llm:
  # For atomic note generation and query rewriting
  local_model:
    base_url: "http://localhost:11434"
    model: "gemma3:4b-it-fp16"
    temperature: 0.1
    max_tokens: 2048
    
  # For final generation and feedback (Ollama required)
  ollama:
    base_url: "http://localhost:11434"
    model: "gemma3:4b-it-fp16"
    temperature: 0.7
    max_tokens: 4096
    # LightRAG-inspired configuration
    num_ctx: 32768          # Context window size
    max_async: 4            # Max concurrent requests
    timeout: 60             # Request timeout in seconds
    
# Vector Store
vector_store:
  index_type: "Flat"  # faiss index types: Flat, IVFFlat, IVFPQ, HNSW, LSH
  dimension: 1024
  similarity_metric: "cosine"
  top_k: 50
  similarity_threshold: 0.1  # 降低相似度阈值以增加召回
  
# Storage Paths
storage:
  vector_db_path: null  # 将在运行时设置为工作目录下的子目录
  graph_db_path: null   # 将在运行时设置为工作目录下的子目录
  processed_docs_path: null  # 将在运行时设置为工作目录下的子目录
  cache_path: null      # 将在运行时设置为工作目录下的子目录
  source_docs_dir: "./data"  # 源文档目录保持不变
  result_root: "./result"
  work_dir: null
  
# Evaluation
eval:
  datasets_path: null  # 将在运行时设置为工作目录下的子目录
  metrics: ["precision", "recall", "f1", "bleu", "rouge"]
  batch_size: 16
  

# Performance
performance:
  use_gpu: true
  use_cudf: true
  num_workers: 4
  cache_enabled: true
