# =========================
# ANO-RAG minimal config
# =========================

system:
  project_name: "ano-rag"
  seed: 42
  device: "cuda"

document:
  chunk_size: 256
  overlap: 32

embedding:
  model_name: "/home/wjk/workplace/anorag/models/embedding/BAAI_bge-m3"
  batch_size: 64
  max_length: 512
  normalize: true

vector_store:
  top_k: 20
  dimension: 1024
  index_type: "IVFFlat"
  similarity_metric: "cosine"
  batch_size: 32
  similarity_threshold: 0.5
  retriever_enhancement:
    topk_multiplier: 3.0
    entity_boost_factor: 1.2
    predicate_boost_factor: 1.15
    must_have_terms_penalty: 0.6
    enable_filter_logging: true

retrieval:
  candidate_pool: 50
  hybrid:
    enable_hybrid_search: true
    fusion:
      method: "linear"
      linear:
        vector_weight: 0.7
        bm25_weight: 0.3
        path_weight: 0.3
    bm25:
      corpus_field: "title_raw_span"
      k1: 1.2
      b: 0.75
    path_aware:
      enabled: true
      min_path_score: 0.30
      max_paths_per_candidate: 5
      min_path_quality: 0.40
  multi_hop:
    enabled: true
    strategy: "hybrid"
    entity_extraction:
      enabled: true
      max_entities: 10
    hybrid_mode:
      primary_strategy: "entity_extraction"
      fallback_strategy: "top_k_seed"
      switch_threshold: 3

path_aware_ranker:
  k_hop: 2
  path_weight: 0.3
  semantic_weight: 0.5
  sparse_weight: 0.2
  max_paths_per_candidate: 5
  min_path_score: 0.1

dispatcher:
  final_semantic_count: 7
  final_graph_count: 0
  bridge_policy: "keepalive"
  bridge_boost_epsilon: 0.02
  debug_log: true

# -------------------------
# 答案生成：LM Studio（唯一通道）
# -------------------------
llm:
  provider: "lmstudio"
  base_url: "http://localhost:1234/v1"
  model: "openai/gpt-oss-20b"   # 或 qwen2.5-7b-instruct（二选一，保持与你本地LM Studio一致）
  timeout: 60
  instances: 1
  # 若你的 LM Studio 支持 OpenAI response_format，可在代码层开启 JSON 模式（答案生成通常不需要）
  
  # LM Studio 并发配置
  lmstudio:
    multiple_instances:
      enabled: false  # 临时禁用多实例并发
      target_instance_count: 2
    concurrent:
      enabled: false  # 临时禁用并发处理
      max_workers: 4
  
  # Ollama 并发配置  
  ollama:
    concurrent:
      enabled: false  # 临时禁用并发处理
      max_workers: 4

# -------------------------
# 原子笔记生成：Ollama + LM Studio 并行
# -------------------------
atomic_note_generation:
  parallel_enabled: true
  parallel_strategy: "task_division"
  task_division:
    enabled: true
    allocation_method: "round_robin"
    enable_fallback: true
    fallback_timeout: 120

  # 后端一：Ollama
  ollama:
    enabled: true
    base_url: "http://localhost:11434"
    model: "qwen2.5:latest"
    temperature: 0.05
    num_ctx: 32768

  # 后端二：LM Studio
  lmstudio:
    enabled: true
    base_url: "http://localhost:1234/v1"
    model: "qwen2.5-7b-instruct"
    temperature: 0.05
    timeout: 120
    top_p: 0.9
    json_mode: false   # 临时关闭JSON模式进行测试

  monitoring:
    enabled: true
    log_stats: true
    export_metrics: false
    performance_threshold: 30.0
    error_rate_threshold: 0.1

# -------------------------
# 显式禁用不需要的 LLM 步骤
# -------------------------
rewriter:
  enabled: false

evaluation:
  enabled: false
