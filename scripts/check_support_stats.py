#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ”¯æŒæ®µè½ç»Ÿè®¡æ£€æŸ¥è„šæœ¬

ä½œç”¨ï¼šè·‘å®Œä¸€æ‰¹é¢„æµ‹åï¼Œå¿«é€ŸéªŒè¯æœ¬æ¬¡æ”¹é€ æ˜¯å¦ç”Ÿæ•ˆã€‚
æ£€æŸ¥é¡¹ï¼š
1. len(predicted_support_idxs)çš„åˆ†å¸ƒï¼ˆåº”é›†ä¸­åœ¨2â€“4ï¼‰
2. support_idxs[0]æ‰€æŒ‡æ®µè½æ˜¯å¦åŒ…å«ç­”æ¡ˆå­ä¸²
3. æŒ‰é—®é¢˜ä¼°è®¡çš„Kä¸å®é™…len(support_idxs)çš„ä¸€è‡´ç‡
"""

import json
import argparse
from pathlib import Path
from typing import Dict, List, Any, Optional
from collections import Counter, defaultdict
from loguru import logger
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from utils.k_estimator import estimate_required_k
from utils.text_utils import TextUtils


class SupportStatsChecker:
    """æ”¯æŒæ®µè½ç»Ÿè®¡æ£€æŸ¥å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ£€æŸ¥å™¨"""
        self.stats = {
            'total_samples': 0,
            'support_length_distribution': Counter(),
            'answer_in_first_support': 0,
            'k_estimation_matches': 0,
            'k_estimation_attempts': 0,
            'ghost_id_count': 0,
            'ghost_id_items': [],
            'unique_ghost_ids': set(),
            'errors': []
        }
    
    def load_results_file(self, file_path: str) -> List[Dict[str, Any]]:
        """åŠ è½½é¢„æµ‹ç»“æœæ–‡ä»¶
        
        Args:
            file_path: ç»“æœæ–‡ä»¶è·¯å¾„ï¼ˆæ”¯æŒ.jsonå’Œ.jsonlæ ¼å¼ï¼‰
            
        Returns:
            é¢„æµ‹ç»“æœåˆ—è¡¨
        """
        results = []
        file_path = Path(file_path)
        
        if not file_path.exists():
            raise FileNotFoundError(f"Results file not found: {file_path}")
        
        try:
            if file_path.suffix == '.jsonl':
                with open(file_path, 'r', encoding='utf-8') as f:
                    for line_no, line in enumerate(f, 1):
                        line = line.strip()
                        if line:
                            try:
                                result = json.loads(line)
                                results.append(result)
                            except json.JSONDecodeError as e:
                                logger.warning(f"Failed to parse line {line_no}: {e}")
            else:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if isinstance(data, list):
                        results = data
                    else:
                        results = [data]
        except Exception as e:
            logger.error(f"Failed to load results file: {e}")
            raise
        
        logger.info(f"Loaded {len(results)} results from {file_path}")
        return results
    
    def extract_passages_from_result(self, result: Dict[str, Any]) -> Optional[Dict[int, str]]:
        """ä»ç»“æœä¸­æå–æ®µè½ä¿¡æ¯
        
        Args:
            result: å•ä¸ªé¢„æµ‹ç»“æœ
            
        Returns:
            æ®µè½ç´¢å¼•åˆ°å†…å®¹çš„æ˜ å°„ï¼Œå¦‚æœæ— æ³•æå–è¿”å›None
        """
        passages_by_idx = {}
        
        # å°è¯•ä»noteså­—æ®µæå–
        notes = result.get('notes', [])
        if notes:
            for note in notes:
                paragraph_idxs = note.get('paragraph_idxs', [])
                content = note.get('content', '')
                if paragraph_idxs and content:
                    primary_idx = paragraph_idxs[0]
                    passages_by_idx[primary_idx] = content
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•å…¶ä»–å¯èƒ½çš„å­—æ®µ
        if not passages_by_idx:
            context = result.get('context', '')
            if context:
                # å°è¯•è§£æ[P{idx}]æ ¼å¼çš„ä¸Šä¸‹æ–‡
                import re
                pattern = r'\[P(\d+)\]\s*([^\[]*?)(?=\[P\d+\]|$)'
                matches = re.findall(pattern, context, re.DOTALL)
                for idx_str, content in matches:
                    try:
                        idx = int(idx_str)
                        passages_by_idx[idx] = content.strip()
                    except ValueError:
                        continue
        
        return passages_by_idx if passages_by_idx else None
    
    def check_answer_in_first_support(self, answer: str, support_idxs: List[int], 
                                     passages_by_idx: Dict[int, str]) -> bool:
        """æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦åœ¨ç¬¬ä¸€ä¸ªæ”¯æŒæ®µè½ä¸­
        
        Args:
            answer: é¢„æµ‹ç­”æ¡ˆ
            support_idxs: æ”¯æŒæ®µè½ç´¢å¼•åˆ—è¡¨
            passages_by_idx: æ®µè½ç´¢å¼•åˆ°å†…å®¹çš„æ˜ å°„
            
        Returns:
            Trueå¦‚æœç­”æ¡ˆåœ¨ç¬¬ä¸€ä¸ªæ”¯æŒæ®µè½ä¸­
        """
        if not support_idxs or not answer:
            return False
        
        first_idx = support_idxs[0]
        first_passage = passages_by_idx.get(first_idx, '')
        
        if not first_passage:
            return False
        
        # æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦åœ¨æ®µè½ä¸­ï¼ˆå¿½ç•¥å¤§å°å†™ï¼‰
        return answer.lower().strip() in first_passage.lower()
    
    def estimate_k_for_sample(self, result: Dict[str, Any], 
                            passages_by_idx: Dict[int, str]) -> Optional[int]:
        """ä¸ºå•ä¸ªæ ·æœ¬ä¼°è®¡Kå€¼
        
        Args:
            result: é¢„æµ‹ç»“æœ
            passages_by_idx: æ®µè½ç´¢å¼•åˆ°å†…å®¹çš„æ˜ å°„
            
        Returns:
            ä¼°è®¡çš„Kå€¼ï¼Œå¦‚æœä¼°è®¡å¤±è´¥è¿”å›None
        """
        try:
            question = result.get('query', result.get('question', ''))
            answer = result.get('predicted_answer', result.get('answer', ''))
            
            if not question or not answer:
                return None
            
            # æ„å»ºpacked_orderï¼ˆæŒ‰ç´¢å¼•æ’åºï¼‰
            packed_order = sorted(passages_by_idx.keys())
            
            estimated_k = estimate_required_k(
                question=question,
                answer=answer,
                passages_by_idx=passages_by_idx,
                packed_order=packed_order
            )
            
            return estimated_k
            
        except Exception as e:
            logger.debug(f"K estimation failed: {e}")
            return None
    
    def check_ghost_ids(self, support_idxs: List[int], passages_by_idx: Dict[int, str]) -> List[int]:
        """æ£€æŸ¥å¹½çµidï¼ˆä¸å­˜åœ¨äºpassages_by_idxä¸­çš„idï¼‰
        
        Args:
            support_idxs: æ”¯æŒæ®µè½ç´¢å¼•åˆ—è¡¨
            passages_by_idx: æ®µè½ç´¢å¼•åˆ°å†…å®¹çš„æ˜ å°„
            
        Returns:
            å¹½çµidåˆ—è¡¨
        """
        ghost_ids = []
        for idx in support_idxs:
            try:
                idx_int = int(idx)
                if idx_int not in passages_by_idx:
                    ghost_ids.append(idx_int)
            except (ValueError, TypeError):
                # éæ•´æ•°idä¹Ÿç®—ä½œå¹½çµid
                ghost_ids.append(idx)
        return ghost_ids
    
    def check_single_result(self, result: Dict[str, Any]) -> Dict[str, Any]:
        """æ£€æŸ¥å•ä¸ªé¢„æµ‹ç»“æœ
        
        Args:
            result: é¢„æµ‹ç»“æœ
            
        Returns:
            æ£€æŸ¥ç»“æœç»Ÿè®¡
        """
        check_result = {
            'support_length': 0,
            'answer_in_first_support': False,
            'estimated_k': None,
            'actual_k': 0,
            'k_match': False,
            'ghost_ids': [],
            'has_ghost_ids': False,
            'error': None
        }
        
        try:
            # æå–åŸºæœ¬ä¿¡æ¯
            support_idxs = result.get('predicted_support_idxs', [])
            answer = result.get('predicted_answer', result.get('answer', ''))
            
            check_result['support_length'] = len(support_idxs)
            check_result['actual_k'] = len(support_idxs)
            
            # æå–æ®µè½ä¿¡æ¯
            passages_by_idx = self.extract_passages_from_result(result)
            
            if passages_by_idx:
                # æ£€æŸ¥å¹½çµid
                ghost_ids = self.check_ghost_ids(support_idxs, passages_by_idx)
                check_result['ghost_ids'] = ghost_ids
                check_result['has_ghost_ids'] = len(ghost_ids) > 0
                
                # æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦åœ¨ç¬¬ä¸€ä¸ªæ”¯æŒæ®µè½ä¸­
                check_result['answer_in_first_support'] = self.check_answer_in_first_support(
                    answer, support_idxs, passages_by_idx
                )
                
                # ä¼°è®¡Kå€¼
                estimated_k = self.estimate_k_for_sample(result, passages_by_idx)
                if estimated_k is not None:
                    check_result['estimated_k'] = estimated_k
                    check_result['k_match'] = (estimated_k == len(support_idxs))
            
        except Exception as e:
            check_result['error'] = str(e)
            logger.debug(f"Error checking result: {e}")
        
        return check_result
    
    def analyze_results(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ†æé¢„æµ‹ç»“æœ
        
        Args:
            results: é¢„æµ‹ç»“æœåˆ—è¡¨
            
        Returns:
            åˆ†æç»Ÿè®¡ç»“æœ
        """
        self.stats['total_samples'] = len(results)
        
        for i, result in enumerate(results):
            try:
                check_result = self.check_single_result(result)
                
                # æ›´æ–°ç»Ÿè®¡
                support_length = check_result['support_length']
                self.stats['support_length_distribution'][support_length] += 1
                
                if check_result['answer_in_first_support']:
                    self.stats['answer_in_first_support'] += 1
                
                if check_result['estimated_k'] is not None:
                    self.stats['k_estimation_attempts'] += 1
                    if check_result['k_match']:
                        self.stats['k_estimation_matches'] += 1
                
                # ç»Ÿè®¡å¹½çµid
                if check_result['has_ghost_ids']:
                    self.stats['ghost_id_count'] += 1
                    self.stats['ghost_id_items'].append({
                        'sample_index': i,
                        'id': result.get('id', f'sample_{i}'),
                        'ghost_ids': check_result['ghost_ids'],
                        'all_support_idxs': result.get('predicted_support_idxs', [])
                    })
                    # è®°å½•å”¯ä¸€çš„å¹½çµid
                    for ghost_id in check_result['ghost_ids']:
                        self.stats['unique_ghost_ids'].add(ghost_id)
                
                if check_result['error']:
                    self.stats['errors'].append({
                        'sample_index': i,
                        'error': check_result['error']
                    })
                    
            except Exception as e:
                logger.error(f"Failed to analyze sample {i}: {e}")
                self.stats['errors'].append({
                    'sample_index': i,
                    'error': str(e)
                })
        
        return self.generate_report()
    
    def generate_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š
        
        Returns:
            åˆ†ææŠ¥å‘Š
        """
        total = self.stats['total_samples']
        
        # è®¡ç®—ç™¾åˆ†æ¯”
        support_length_percentages = {}
        for length, count in self.stats['support_length_distribution'].items():
            support_length_percentages[length] = (count / total * 100) if total > 0 else 0
        
        answer_in_first_percentage = (self.stats['answer_in_first_support'] / total * 100) if total > 0 else 0
        
        k_match_percentage = 0
        if self.stats['k_estimation_attempts'] > 0:
            k_match_percentage = (self.stats['k_estimation_matches'] / self.stats['k_estimation_attempts'] * 100)
        
        # æ£€æŸ¥æ˜¯å¦æ»¡è¶³éªŒæ”¶æ ‡å‡†
        target_lengths = {2, 3, 4}
        target_samples = sum(self.stats['support_length_distribution'][length] for length in target_lengths)
        target_percentage = (target_samples / total * 100) if total > 0 else 0
        
        # å¹½çµidç»Ÿè®¡
        ghost_id_percentage = (self.stats['ghost_id_count'] / total * 100) if total > 0 else 0
        
        report = {
            'summary': {
                'total_samples': total,
                'target_length_samples': target_samples,
                'target_length_percentage': target_percentage,
                'answer_in_first_support_percentage': answer_in_first_percentage,
                'k_estimation_success_rate': k_match_percentage,
                'ghost_id_count': self.stats['ghost_id_count'],
                'ghost_id_percentage': ghost_id_percentage,
                'unique_ghost_ids_count': len(self.stats['unique_ghost_ids'])
            },
            'support_length_distribution': dict(self.stats['support_length_distribution']),
            'support_length_percentages': support_length_percentages,
            'k_estimation_stats': {
                'attempts': self.stats['k_estimation_attempts'],
                'matches': self.stats['k_estimation_matches'],
                'success_rate': k_match_percentage
            },
            'ghost_id_stats': {
                'ghost_id_items': self.stats['ghost_id_items'][:10],  # å‰10ä¸ªå¹½çµidæ ·æœ¬
                'unique_ghost_ids': sorted(list(self.stats['unique_ghost_ids']))
            },
            'validation_results': {
                'structure_check': target_percentage >= 95.0,  # â‰¥95% æ ·æœ¬æ»¡è¶³ len(support_idxs) âˆˆ {2,3,4}
                'verifiability_check': answer_in_first_percentage >= 95.0,  # â‰¥95% æ ·æœ¬æ»¡è¶³ answer âˆˆ paragraph[support_idxs[0]]
                'k_estimation_reasonable': k_match_percentage >= 70.0,  # Kä¼°è®¡åˆç†æ€§æ£€æŸ¥
                'ghost_id_check': self.stats['ghost_id_count'] == 0  # å¹½çµidæ•°é‡å¿…é¡»ä¸º0
            },
            'errors': self.stats['errors'][:10]  # åªæ˜¾ç¤ºå‰10ä¸ªé”™è¯¯
        }
        
        return report
    
    def print_report(self, report: Dict[str, Any]):
        """æ‰“å°åˆ†ææŠ¥å‘Š
        
        Args:
            report: åˆ†ææŠ¥å‘Š
        """
        print("\n" + "="*60)
        print("æ”¯æŒæ®µè½ç»Ÿè®¡æ£€æŸ¥æŠ¥å‘Š")
        print("="*60)
        
        summary = report['summary']
        print(f"\nğŸ“Š æ€»ä½“ç»Ÿè®¡:")
        print(f"  æ€»æ ·æœ¬æ•°: {summary['total_samples']}")
        print(f"  ç›®æ ‡é•¿åº¦æ ·æœ¬æ•° (2-4): {summary['target_length_samples']}")
        print(f"  ç›®æ ‡é•¿åº¦å æ¯”: {summary['target_length_percentage']:.2f}%")
        print(f"  ç­”æ¡ˆåœ¨é¦–æ®µå æ¯”: {summary['answer_in_first_support_percentage']:.2f}%")
        print(f"  Kä¼°è®¡æˆåŠŸç‡: {summary['k_estimation_success_rate']:.2f}%")
        print(f"  å¹½çµIDæ ·æœ¬æ•°: {summary['ghost_id_count']}")
        print(f"  å¹½çµIDå æ¯”: {summary['ghost_id_percentage']:.2f}%")
        print(f"  å”¯ä¸€å¹½çµIDæ•°: {summary['unique_ghost_ids_count']}")
        
        print(f"\nğŸ“ˆ æ”¯æŒæ®µè½é•¿åº¦åˆ†å¸ƒ:")
        for length in sorted(report['support_length_distribution'].keys()):
            count = report['support_length_distribution'][length]
            percentage = report['support_length_percentages'][length]
            print(f"  é•¿åº¦ {length}: {count} æ ·æœ¬ ({percentage:.2f}%)")
        
        print(f"\nğŸ” Kä¼°è®¡ç»Ÿè®¡:")
        k_stats = report['k_estimation_stats']
        print(f"  å°è¯•ä¼°è®¡: {k_stats['attempts']} æ ·æœ¬")
        print(f"  ä¼°è®¡åŒ¹é…: {k_stats['matches']} æ ·æœ¬")
        print(f"  æˆåŠŸç‡: {k_stats['success_rate']:.2f}%")
        
        print(f"\nâœ… éªŒæ”¶æ ‡å‡†æ£€æŸ¥:")
        validation = report['validation_results']
        structure_status = "âœ… é€šè¿‡" if validation['structure_check'] else "âŒ æœªé€šè¿‡"
        verifiability_status = "âœ… é€šè¿‡" if validation['verifiability_check'] else "âŒ æœªé€šè¿‡"
        k_estimation_status = "âœ… åˆç†" if validation['k_estimation_reasonable'] else "âš ï¸ éœ€ä¼˜åŒ–"
        ghost_id_status = "âœ… é€šè¿‡" if validation['ghost_id_check'] else "âŒ å‘ç°å¹½çµID"
        
        print(f"  ç»“æ„æ£€æŸ¥ (â‰¥95% é•¿åº¦2-4): {structure_status}")
        print(f"  å¯éªŒè¯æ€§æ£€æŸ¥ (â‰¥95% ç­”æ¡ˆåœ¨é¦–æ®µ): {verifiability_status}")
        print(f"  Kä¼°è®¡åˆç†æ€§ (â‰¥70% åŒ¹é…): {k_estimation_status}")
        print(f"  å¹½çµIDæ£€æŸ¥ (æ•°é‡=0): {ghost_id_status}")
        
        # æ˜¾ç¤ºå¹½çµIDè¯¦ç»†ä¿¡æ¯
        ghost_stats = report['ghost_id_stats']
        if ghost_stats['unique_ghost_ids']:
            print(f"\nğŸ‘» å¹½çµIDè¯¦ç»†ä¿¡æ¯:")
            print(f"  å‘ç°çš„å”¯ä¸€å¹½çµID: {ghost_stats['unique_ghost_ids']}")
            print(f"  å‰10ä¸ªå¼‚å¸¸æ ·æœ¬:")
            for item in ghost_stats['ghost_id_items']:
                print(f"    ID: {item['id']} - å¹½çµID: {item['ghost_ids']} - æ‰€æœ‰æ”¯æŒID: {item['all_support_idxs']}")
        
        if report['errors']:
            print(f"\nâš ï¸ é”™è¯¯æ ·æœ¬ (æ˜¾ç¤ºå‰10ä¸ª):")
            for error in report['errors']:
                print(f"  æ ·æœ¬ {error['sample_index']}: {error['error']}")
        
        print("\n" + "="*60)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='æ£€æŸ¥æ”¯æŒæ®µè½ç»Ÿè®¡')
    parser.add_argument('results_file', help='é¢„æµ‹ç»“æœæ–‡ä»¶è·¯å¾„ (.json æˆ– .jsonl)')
    parser.add_argument('--output', '-o', help='è¾“å‡ºæŠ¥å‘Šæ–‡ä»¶è·¯å¾„ (å¯é€‰)')
    parser.add_argument('--verbose', '-v', action='store_true', help='è¯¦ç»†è¾“å‡º')
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    if args.verbose:
        logger.remove()
        logger.add(sys.stderr, level="DEBUG")
    else:
        logger.remove()
        logger.add(sys.stderr, level="INFO")
    
    try:
        # åˆ›å»ºæ£€æŸ¥å™¨
        checker = SupportStatsChecker()
        
        # åŠ è½½ç»“æœ
        logger.info(f"Loading results from {args.results_file}")
        results = checker.load_results_file(args.results_file)
        
        # åˆ†æç»“æœ
        logger.info("Analyzing results...")
        report = checker.analyze_results(results)
        
        # æ‰“å°æŠ¥å‘Š
        checker.print_report(report)
        
        # ä¿å­˜æŠ¥å‘Š
        if args.output:
            with open(args.output, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            logger.info(f"Report saved to {args.output}")
        
        # è¿”å›é€€å‡ºç 
        validation = report['validation_results']
        if all(validation.values()):
            logger.info("All validation checks passed!")
            sys.exit(0)
        else:
            logger.warning("Some validation checks failed.")
            sys.exit(1)
            
    except Exception as e:
        logger.error(f"Script failed: {e}")
        sys.exit(1)


if __name__ == '__main__':
    main()