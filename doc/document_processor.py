import os
import numpy as np
from typing import List, Dict, Any, Optional, Tuple
from loguru import logger
from tqdm import tqdm
import networkx as nx
from networkx.readwrite import json_graph
from collections import defaultdict
import hashlib
import json
from .chunker import DocumentChunker
from .clustering import TopicClustering
from .incremental_processor import IncrementalProcessor
from llm import AtomicNoteGenerator, LocalLLM
from llm.parallel_task_atomic_note_generator import ParallelTaskAtomicNoteGenerator
from utils import BatchProcessor, FileUtils, JSONLProgressTracker
from utils.enhanced_ner import EnhancedNER
from utils.consistency_checker import ConsistencyChecker
from config import config
from vector_store import EmbeddingManager
from graph import GraphBuilder

class DocumentProcessor:
    """æ–‡æ¡£å¤„ç†å™¨ä¸»ç±»ï¼Œæ•´åˆæ‰€æœ‰æ–‡æ¡£å¤„ç†åŠŸèƒ½"""

    def __init__(self, output_dir: Optional[str] = None, llm: Optional[LocalLLM] = None):
        # åˆå§‹åŒ–ç»„ä»¶
        self.chunker = DocumentChunker()
        self.clustering = TopicClustering()
        # å¢é‡å¤„ç†ç¼“å­˜ç›®å½•æ”¾åœ¨å·¥ä½œç›®å½•ä¸‹
        cache_dir = None
        if output_dir:
            cache_dir = os.path.join(output_dir, 'cache')
        else:
            work_dir = config.get('storage.work_dir')
            if work_dir:
                cache_dir = os.path.join(work_dir, 'cache')
            else:
                # é»˜è®¤ç¼“å­˜ç›®å½•
                cache_dir = './cache'
        self.incremental_processor = IncrementalProcessor(cache_dir=cache_dir)
        if llm is None:
            raise ValueError("DocumentProcessor requires a LocalLLM instance to be passed")
        self.llm = llm
        
        # é€‰æ‹©åŸå­ç¬”è®°ç”Ÿæˆå™¨ï¼šä¼˜å…ˆä½¿ç”¨å¹¶è¡Œä»»åŠ¡åˆ†é…ç”Ÿæˆå™¨
        parallel_config = config.get('atomic_note_generation', {})
        if (parallel_config.get('parallel_enabled', False) and 
            parallel_config.get('parallel_strategy') == 'task_division'):
            logger.info("Using ParallelTaskAtomicNoteGenerator for atomic note generation")
            self.atomic_note_generator = ParallelTaskAtomicNoteGenerator(self.llm)
        else:
            logger.info("Using standard AtomicNoteGenerator for atomic note generation")
            self.atomic_note_generator = AtomicNoteGenerator(self.llm)
        self.batch_processor = BatchProcessor(
            batch_size=config.get('document.batch_size', 32),
            use_gpu=config.get('performance.use_gpu', True)
        )
        self.embedding_manager = EmbeddingManager()
        self.graph_builder = GraphBuilder(llm=self.llm)
        
        # å­˜å‚¨è·¯å¾„ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„å·¥ä½œç›®å½•
        self.processed_docs_path = output_dir or config.get('storage.work_dir') or config.get('storage.processed_docs_path') or './data/processed'
        FileUtils.ensure_dir(self.processed_docs_path)
        
    def process_documents(self, file_paths: List[str], force_reprocess: bool = False, output_dir: Optional[str] = None) -> Dict[str, Any]:
        """å¤„ç†æ–‡æ¡£çš„ä¸»è¦å…¥å£ç‚¹"""
        logger.info(f"Starting document processing for {len(file_paths)} files")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰JSONLæ–‡ä»¶ï¼Œå¦‚æœæœ‰åˆ™åˆ›å»ºè¿›åº¦è·Ÿè¸ªå™¨
        jsonl_files = [f for f in file_paths if f.endswith('.jsonl')]
        progress_tracker = None
        if jsonl_files:
            # ä½¿ç”¨ç¬¬ä¸€ä¸ªJSONLæ–‡ä»¶åˆ›å»ºè¿›åº¦è·Ÿè¸ªå™¨
            progress_tracker = JSONLProgressTracker(jsonl_files[0], "Processing JSONL data")
            progress_tracker.start()

        if output_dir:
            self.processed_docs_path = output_dir
            FileUtils.ensure_dir(self.processed_docs_path)
        
        # è·å–å¤„ç†è®¡åˆ’
        processing_plan = self.incremental_processor.get_processing_plan(file_paths)

        if not force_reprocess and processing_plan['can_skip_processing']:
            logger.info("No files need processing, loading cached results")
            return self._load_cached_results(file_paths)

        files_to_process = processing_plan['files_to_process']
        unchanged_files = processing_plan.get('unchanged_files', [])
        files_to_clean = processing_plan.get('files_to_clean', [])

        if force_reprocess:
            files_to_process = file_paths
            unchanged_files = []
            files_to_clean = []

        logger.info(f"Processing {len(files_to_process)} files")

        chunk_file = os.path.join(self.processed_docs_path, "chunks.jsonl")
        existing_chunks = []
        if os.path.exists(chunk_file):
            try:
                existing_chunks = FileUtils.read_jsonl(chunk_file)
            except Exception as e:
                logger.warning(f"Failed to load existing chunks: {e}")

        # è¿‡æ»¤å‡ºæœªå˜æ›´æ–‡ä»¶çš„æ—§åˆ†å—
        cached_chunks = [
            c for c in existing_chunks
            if c.get('source_info', {}).get('file_path') in unchanged_files
        ]

        # ç§»é™¤éœ€è¦æ¸…ç†çš„æ–‡ä»¶å¯¹åº”çš„åˆ†å—
        cached_chunks = [
            c for c in cached_chunks
            if c.get('source_info', {}).get('file_path') not in files_to_clean
        ]

        chunks_updated = False
        new_chunks = []
        if files_to_process:
            logger.info("Step 1: Document chunking")
            new_chunks = self._chunk_documents(files_to_process)
            chunks_updated = True
        if files_to_clean:
            chunks_updated = True

        all_chunks = cached_chunks + new_chunks
        chunk_count = len(all_chunks)

        if chunks_updated:
            FileUtils.write_jsonl(all_chunks, chunk_file)
        
        if not all_chunks:
            logger.warning("No chunks created from documents")
            return {'atomic_notes': [], 'topic_pools': [], 'processing_stats': {}}
        
        atomic_file = os.path.join(self.processed_docs_path, "atomic_notes.json")
        if (not force_reprocess and not chunks_updated
                and os.path.exists(atomic_file)):
            logger.info(f"Loading atomic notes from {atomic_file}")
            atomic_notes = FileUtils.read_json(atomic_file)
        else:
            logger.info("Step 2: Generating atomic notes")
            atomic_notes = self._generate_atomic_notes(all_chunks, progress_tracker)
            FileUtils.write_json(atomic_notes, atomic_file)
            
            # ä¿å­˜è¢«æ ‡è®°ä¸ºéœ€è¦é‡å†™çš„æ‘˜è¦ï¼ˆå¦‚æœå¯ç”¨äº†æ‘˜è¦æ ¡éªŒå™¨ï¼‰
            summary_auditor_enabled = config.get('summary_auditor.enabled', False)
            if summary_auditor_enabled:
                try:
                    from utils.summary_auditor import SummaryAuditor
                    auditor = SummaryAuditor(llm=self.llm)
                    auditor.save_flagged_summaries(atomic_notes, self.processed_docs_path)
                except ImportError as e:
                    logger.warning(f"Failed to import SummaryAuditor: {e}")
                except Exception as e:
                    logger.error(f"Failed to save flagged summaries: {e}")
        note_count = len(atomic_notes)
        
        if not atomic_notes:
            logger.warning("No atomic notes generated")
            return {'atomic_notes': [], 'topic_pools': [], 'processing_stats': {}}
        
        embed_file = os.path.join(self.processed_docs_path, "embeddings.npy")
        if (not force_reprocess and not chunks_updated
                and os.path.exists(embed_file)):
            logger.info(f"Loading embeddings from {embed_file}")
            embeddings = np.load(embed_file)
        else:
            logger.info("Step 3: Creating embeddings")
            embeddings = self.embedding_manager.encode_atomic_notes(atomic_notes)
            np.save(embed_file, embeddings)
        
        cluster_file = os.path.join(self.processed_docs_path, "clustering.json")
        if (not force_reprocess and not chunks_updated
                and os.path.exists(cluster_file)):
            logger.info(f"Loading clustering from {cluster_file}")
            clustering_result = FileUtils.read_json(cluster_file)
        else:
            logger.info("Step 4: Topic clustering")
            clustering_result = self.clustering.cluster_notes(atomic_notes, embeddings)
            FileUtils.write_json(clustering_result, cluster_file)
        
        graph_file = os.path.join(self.processed_docs_path, "graph.json")
        graphml_file = os.path.join(self.processed_docs_path, "graph.graphml")
        if (not force_reprocess and not chunks_updated
                and os.path.exists(graph_file)):
            logger.info(f"Loading graph from {graph_file}")
            graph_data = FileUtils.read_json(graph_file)
        else:
            logger.info("Step 5: Building graph relationships")
            graph = self.graph_builder.build_graph(
                clustering_result['clustered_notes'], embeddings
            )
            graph_data = nx.node_link_data(graph)
            FileUtils.write_json(graph_data, graph_file)
            
            # å¯¼å‡ºGraphMLæ ¼å¼
            try:
                from graph.graphml_exporter import GraphMLExporter
                exporter = GraphMLExporter()
                exporter.export_graph(graph, graphml_file)
                logger.info(f"Graph exported to GraphML format: {graphml_file}")
            except Exception as e:
                logger.warning(f"Failed to export GraphML: {e}")
        
        # ä¸€è‡´æ€§æ£€æŸ¥ï¼ˆåœ¨æ•°æ®å†™å…¥å‰ï¼‰
        consistency_result = None
        if config.get('consistency_check.enabled', True):
            logger.info("Performing consistency check before data persistence")
            checker = ConsistencyChecker()
            consistency_result = checker.check_consistency(
                clustering_result['clustered_notes'], 
                graph_data
            )
            
            # å¦‚æœæœ‰ä¸¥é‡é”™è¯¯ä¸”å¯ç”¨äº†ä¸¥æ ¼æ¨¡å¼ï¼Œåœæ­¢å¤„ç†
            if not consistency_result['is_consistent'] and config.get('consistency_check.strict_mode', False):
                error_msg = f"Consistency check failed with {len(consistency_result['errors'])} errors"
                logger.error(error_msg)
                
                # å¯¼å‡ºé”™è¯¯æŠ¥å‘Š
                error_report_file = os.path.join(self.processed_docs_path, "consistency_errors.json")
                checker.export_report(error_report_file)
                
                raise RuntimeError(f"{error_msg}. See report: {error_report_file}")
            
            # å¯¼å‡ºä¸€è‡´æ€§æŠ¥å‘Š
            consistency_report_file = os.path.join(self.processed_docs_path, "consistency_report.json")
            checker.export_report(consistency_report_file)
            
            if consistency_result['errors']:
                logger.warning(f"Consistency check found {len(consistency_result['errors'])} errors and {len(consistency_result['warnings'])} warnings")
            else:
                logger.info(f"Consistency check passed with {len(consistency_result['warnings'])} warnings")
        
        # ä¿å­˜å¤„ç†ç»“æœ
        result = {
            'atomic_notes': clustering_result['clustered_notes'],
            'topic_pools': clustering_result['topic_pools'],
            'cluster_info': clustering_result['cluster_info'],
            'graph_data': graph_data,
            'consistency_check': consistency_result,
            'processing_stats': self._calculate_processing_stats(
                files_to_process,
                atomic_notes,
                clustering_result,
                chunk_count,
                note_count,
            )
        }
        
        # æ›´æ–°ç¼“å­˜
        self._update_processing_cache(files_to_process, result)

        result_file = os.path.join(self.processed_docs_path, "result.json")
        FileUtils.write_json(result, result_file)

        # å…³é—­è¿›åº¦è·Ÿè¸ªå™¨
        if progress_tracker:
            progress_tracker.close()
            
        logger.info("Document processing completed successfully")
        return result
    
    def _chunk_documents(self, file_paths: List[str]) -> List[Dict[str, Any]]:
        """æ–‡æ¡£åˆ†å—å¤„ç†"""
        all_chunks = []
        for file_path in tqdm(file_paths, desc="Chunking documents"):
            try:
                # åˆ›å»ºsource_info
                source_info = {
                    'file_path': file_path,
                    'file_name': os.path.basename(file_path),
                    'file_hash': FileUtils.get_file_hash(file_path) if hasattr(FileUtils, 'get_file_hash') else 'unknown'
                }
                chunks = self.chunker.chunk_document(file_path, source_info)
                all_chunks.extend(chunks)
                logger.debug(f"Created {len(chunks)} chunks from {file_path}")
            except Exception as e:
                logger.error(f"Failed to chunk document {file_path}: {e}")
        
        # éªŒè¯åˆ†å—ç»“æœ
        valid_chunks = self.chunker.validate_chunks(all_chunks)
        logger.info(f"Created {len(valid_chunks)} valid chunks from {len(file_paths)} documents")
        
        return valid_chunks
    
    def _generate_atomic_notes(self, chunks: List[Dict[str, Any]], progress_tracker: Optional[JSONLProgressTracker] = None) -> List[Dict[str, Any]]:
        """ç”ŸæˆåŸå­ç¬”è®°ï¼Œæ”¯æŒå¤šæ¡ç¬”è®°æ‰¹é‡å¤„ç†å’Œå…¨å±€å»é‡"""
        try:
            logger.info(f"å¼€å§‹å¤„ç† {len(chunks)} ä¸ªæ–‡æ¡£å—ï¼Œç”ŸæˆåŸå­ç¬”è®°")
            
            # ä½¿ç”¨æ‰¹å¤„ç†ç”ŸæˆåŸå­ç¬”è®°ï¼Œæ”¯æŒå¤šæ¡ç¬”è®°è¾“å‡º
            raw_atomic_notes = self.atomic_note_generator.generate_atomic_notes(chunks, progress_tracker)
            logger.info(f"åŸå§‹ç”Ÿæˆ {len(raw_atomic_notes)} æ¡åŸå­ç¬”è®°")
            
            # éªŒè¯åŸå­ç¬”è®°è´¨é‡ï¼Œè¿‡æ»¤ä½è´¨é‡ç¬”è®°
            valid_notes = self.atomic_note_generator.validate_atomic_notes(raw_atomic_notes)
            logger.info(f"è´¨é‡éªŒè¯åä¿ç•™ {len(valid_notes)} æ¡ç¬”è®°")

            # å®ä½“å½’ä¸€åŒ–å’Œè¿½è¸ªå¢å¼º
            valid_notes = EnhancedNER().enhance_entity_tracing(valid_notes)
            logger.info(f"å®ä½“å¢å¼ºå®Œæˆï¼Œå¤„ç† {len(valid_notes)} æ¡ç¬”è®°")

            # å¢å¼ºç¬”è®°å…³ç³»
            enhanced_notes = self.atomic_note_generator.enhance_notes_with_relations(valid_notes)
            logger.info(f"å…³ç³»å¢å¼ºå®Œæˆï¼Œå¤„ç† {len(enhanced_notes)} æ¡ç¬”è®°")
            
            # æ‰å¹³åŒ–å¤šæ¡ç¬”è®°å¹¶è¿›è¡Œå…¨å±€å»é‡
            flattened_notes = self._flatten_and_deduplicate_notes(enhanced_notes)
            logger.info(f"æ‰å¹³åŒ–å’Œå»é‡å®Œæˆï¼Œæœ€ç»ˆä¿ç•™ {len(flattened_notes)} æ¡ç¬”è®°")
            
            # è®¡ç®—å’Œè®°å½•è¯¦ç»†ç»Ÿè®¡æŒ‡æ ‡
            stats = self._calculate_note_generation_stats(chunks, raw_atomic_notes, valid_notes, enhanced_notes, flattened_notes)
            self._log_processing_stats(stats)
            
            logger.info(f"åŸå­ç¬”è®°ç”Ÿæˆå®Œæˆï¼šåŸå§‹ {len(raw_atomic_notes)} â†’ æœ€ç»ˆ {len(flattened_notes)} æ¡ï¼ˆå»é‡ç‡: {((len(raw_atomic_notes) - len(flattened_notes)) / max(len(raw_atomic_notes), 1) * 100):.1f}%ï¼‰")
            return flattened_notes
            
        except Exception as e:
            logger.error(f"åŸå­ç¬”è®°ç”Ÿæˆå¤±è´¥: {e}")
            return []
    
    def _flatten_and_deduplicate_notes(self, notes: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æ‰å¹³åŒ–å¤šæ¡ç¬”è®°å¹¶è¿›è¡Œå…¨å±€å»é‡ï¼Œç¡®ä¿æ•°æ®å±‚é¢å¹²å‡€å’Œå¯ç®¡æ§"""
        if not notes:
            return []
            
        logger.info(f"å¼€å§‹æ‰å¹³åŒ–å’Œå»é‡å¤„ç†ï¼Œè¾“å…¥ {len(notes)} æ¡ç¬”è®°")
        flattened_notes = []
        multi_fact_count = 0
        single_fact_count = 0
        
        # æ‰å¹³åŒ–ï¼šç»Ÿä¸€å¤„ç†å¤šæ¡ç¬”è®°ï¼Œå°†å¤šäº‹å®ç¬”è®°æ‹†åˆ†ä¸ºå•ç‹¬çš„ç¬”è®°
        for note_idx, note in enumerate(notes):
            if isinstance(note.get('content'), list) and len(note['content']) > 1:
                # å¤šäº‹å®ç¬”è®°ï¼Œæ‹†åˆ†ä¸ºå•ä¸ªäº‹å®
                multi_fact_count += 1
                for fact_idx, fact in enumerate(note['content']):
                    single_note = note.copy()
                    single_note['content'] = fact
                    single_note['fact_index'] = fact_idx
                    single_note['original_note_id'] = note.get('id', f'note_{note_idx}')
                    single_note['is_multi_fact_split'] = True
                    single_note['total_facts_in_original'] = len(note['content'])
                    flattened_notes.append(single_note)
            else:
                # å•äº‹å®ç¬”è®°ï¼Œç›´æ¥æ·»åŠ 
                single_fact_count += 1
                if isinstance(note.get('content'), list) and len(note['content']) == 1:
                    note['content'] = note['content'][0]  # ç»Ÿä¸€æ ¼å¼
                note['is_multi_fact_split'] = False
                flattened_notes.append(note)
        
        logger.info(f"æ‰å¹³åŒ–å®Œæˆï¼šå¤šäº‹å®ç¬”è®° {multi_fact_count} æ¡ï¼Œå•äº‹å®ç¬”è®° {single_fact_count} æ¡ï¼Œæ€»è®¡ {len(flattened_notes)} æ¡")
        
        # å…¨å±€å»é‡ï¼šåŸºäºæ–‡æ¡£ã€è·¨åº¦å’Œæ–‡æœ¬å†…å®¹è¿›è¡Œç²¾ç¡®å»é‡
        deduplicated_notes = self._deduplicate_notes_by_content(flattened_notes)
        
        duplicate_count = len(flattened_notes) - len(deduplicated_notes)
        logger.info(f"å»é‡å®Œæˆï¼šç§»é™¤ {duplicate_count} æ¡é‡å¤ç¬”è®°ï¼Œä¿ç•™ {len(deduplicated_notes)} æ¡å”¯ä¸€ç¬”è®°")
        
        return deduplicated_notes
    
    def _deduplicate_notes_by_content(self, notes: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """åŸºäºå†…å®¹è¿›è¡Œå»é‡ï¼ŒåŒæ–‡æ¡£åŒè·¨åº¦/åŒæ–‡æœ¬çš„äº‹å®åˆå¹¶ä¸ºä¸€æ¡ï¼Œä¿ç•™æœ€é«˜é‡è¦æ€§åˆ†æˆ–åˆå¹¶å…ƒæ•°æ®"""
        if not notes:
            return []
            
        # åˆ›å»ºå»é‡é”®åˆ°ç¬”è®°åˆ—è¡¨çš„æ˜ å°„
        dedup_groups = defaultdict(list)
        content_only_groups = defaultdict(list)  # çº¯å†…å®¹å»é‡ç»„
        
        for note in notes:
            # ç”Ÿæˆå»é‡é”®ï¼šæ–‡æ¡£è·¯å¾„ + è·¨åº¦ + æ–‡æœ¬å†…å®¹çš„å“ˆå¸Œ
            source_info = note.get('source_info', {})
            file_path = source_info.get('file_path', '')
            span_start = source_info.get('span_start', 0)
            span_end = source_info.get('span_end', 0)
            content = str(note.get('content', '')).strip()
            
            # åˆ›å»ºç²¾ç¡®å»é‡é”®ï¼ˆæ–‡æ¡£+ä½ç½®+å†…å®¹ï¼‰
            content_hash = hashlib.md5(content.encode('utf-8')).hexdigest()
            dedup_key = f"{file_path}:{span_start}-{span_end}:{content_hash}"
            dedup_groups[dedup_key].append(note)
            
            # åˆ›å»ºå†…å®¹å»é‡é”®ï¼ˆä»…å†…å®¹ï¼Œç”¨äºè·¨æ–‡æ¡£å»é‡ï¼‰
            content_only_groups[content_hash].append(note)
        
        # ç»Ÿè®¡å»é‡ä¿¡æ¯
        exact_duplicates = sum(1 for group in dedup_groups.values() if len(group) > 1)
        content_duplicates = sum(1 for group in content_only_groups.values() if len(group) > 1)
        
        logger.info(f"å»é‡åˆ†æï¼šç²¾ç¡®é‡å¤ç»„ {exact_duplicates} ä¸ªï¼Œå†…å®¹é‡å¤ç»„ {content_duplicates} ä¸ª")
        
        # å¯¹æ¯ä¸ªå»é‡ç»„è¿›è¡Œåˆå¹¶
        deduplicated_notes = []
        merged_count = 0
        
        for dedup_key, group_notes in dedup_groups.items():
            if len(group_notes) == 1:
                deduplicated_notes.append(group_notes[0])
            else:
                # åˆå¹¶é‡å¤ç¬”è®°ï¼Œä¿ç•™æœ€é«˜é‡è¦æ€§åˆ†æˆ–åˆå¹¶å…ƒæ•°æ®
                merged_note = self._merge_duplicate_notes(group_notes)
                deduplicated_notes.append(merged_note)
                merged_count += 1
        
        logger.info(f"å»é‡åˆå¹¶ï¼šå¤„ç† {merged_count} ä¸ªé‡å¤ç»„ï¼Œæœ€ç»ˆä¿ç•™ {len(deduplicated_notes)} æ¡å”¯ä¸€ç¬”è®°")
        return deduplicated_notes
    
    def _merge_duplicate_notes(self, notes: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆå¹¶é‡å¤çš„ç¬”è®°ï¼Œä¿ç•™æœ€é«˜é‡è¦æ€§åˆ†æˆ–åˆå¹¶å…ƒæ•°æ®"""
        if not notes:
            return {}
        
        # æŒ‰é‡è¦æ€§åˆ†æ•°æ’åºï¼Œé€‰æ‹©æœ€é«˜åˆ†çš„ä½œä¸ºåŸºç¡€
        sorted_notes = sorted(notes, key=lambda x: x.get('importance_score', 0), reverse=True)
        base_note = sorted_notes[0].copy()
        
        # åˆå¹¶å…ƒæ•°æ®
        all_ids = []
        all_sources = []
        total_confidence = 0
        count = 0
        
        for note in notes:
            if note.get('id'):
                all_ids.append(note['id'])
            if note.get('source_info'):
                all_sources.append(note['source_info'])
            if 'confidence' in note:
                total_confidence += note['confidence']
                count += 1
        
        # æ›´æ–°åˆå¹¶åçš„å…ƒæ•°æ®
        base_note['merged_from_ids'] = all_ids
        base_note['merged_sources'] = all_sources
        base_note['duplicate_count'] = len(notes)
        
        if count > 0:
            base_note['average_confidence'] = total_confidence / count
        
        return base_note
    
    def _calculate_note_generation_stats(self, chunks: List[Dict[str, Any]], 
                                       raw_notes: List[Dict[str, Any]], 
                                       valid_notes: List[Dict[str, Any]],
                                       enhanced_notes: List[Dict[str, Any]],
                                       final_notes: List[Dict[str, Any]]) -> Dict[str, Any]:
        """è®¡ç®—ç¬”è®°ç”Ÿæˆçš„è¯¦ç»†ç»Ÿè®¡æŒ‡æ ‡ï¼Œè®°å½•æ¯åƒtokenäº§å‡ºç¬”è®°æ•°ã€å¹³å‡æ¯å¥äº‹å®æ•°ã€è¢«è¿‡æ»¤æ¯”ä¾‹"""
        # è®¡ç®—æ€»tokenæ•°å’Œå¥å­æ•°
        total_tokens = sum(len(chunk.get('content', '').split()) for chunk in chunks)
        total_chars = sum(len(chunk.get('content', '')) for chunk in chunks)
        total_sentences = sum(chunk.get('content', '').count('.') + chunk.get('content', '').count('!') + chunk.get('content', '').count('?') for chunk in chunks)
        
        # è®¡ç®—æ¯åƒtokenäº§å‡ºç¬”è®°æ•°
        notes_per_1k_tokens = (len(final_notes) / max(total_tokens, 1)) * 1000
        
        # è®¡ç®—å¹³å‡æ¯å¥äº‹å®æ•°
        avg_facts_per_sentence = len(final_notes) / max(total_sentences, 1)
        
        # è®¡ç®—å„é˜¶æ®µè¿‡æ»¤ç»Ÿè®¡
        quality_filtered = len(raw_notes) - len(valid_notes)  # è´¨é‡éªŒè¯è¿‡æ»¤
        enhancement_change = len(enhanced_notes) - len(valid_notes)  # å¢å¼ºé˜¶æ®µå˜åŒ–
        duplicate_filtered = len(enhanced_notes) - len(final_notes)  # å»é‡è¿‡æ»¤
        
        # è®¡ç®—è¿‡æ»¤æ¯”ä¾‹
        total_raw = max(len(raw_notes), 1)
        quality_filter_ratio = quality_filtered / total_raw
        duplicate_filter_ratio = duplicate_filtered / max(len(enhanced_notes), 1)
        total_retention_ratio = len(final_notes) / total_raw
        
        # è®¡ç®—å†…å®¹è´¨é‡ç»Ÿè®¡
        length_filtered = sum(1 for note in raw_notes if len(str(note.get('content', ''))) < 10)
        score_filtered = sum(1 for note in raw_notes if note.get('importance_score', 0) < 0.3)
        
        # è®¡ç®—å¤šäº‹å®ç¬”è®°ç»Ÿè®¡
        multi_fact_notes = sum(1 for note in enhanced_notes if isinstance(note.get('content'), list) and len(note['content']) > 1)
        single_fact_notes = len(enhanced_notes) - multi_fact_notes
        
        # è®¡ç®—å¹³å‡ç¬”è®°é•¿åº¦
        avg_note_length = sum(len(str(note.get('content', ''))) for note in final_notes) / max(len(final_notes), 1)
        
        # è®¡ç®—å®ä½“å’Œå…³é”®è¯ç»Ÿè®¡
        total_entities = sum(len(note.get('entities', [])) for note in final_notes)
        total_keywords = sum(len(note.get('keywords', [])) for note in final_notes)
        unique_entities = len(set(entity for note in final_notes for entity in note.get('entities', [])))
        unique_keywords = len(set(keyword for note in final_notes for keyword in note.get('keywords', [])))
        
        return {
            # åŸºç¡€ç»Ÿè®¡
            'total_tokens': total_tokens,
            'total_chars': total_chars,
            'total_sentences': total_sentences,
            'chunk_count': len(chunks),
            
            # å„é˜¶æ®µç¬”è®°æ•°é‡
            'raw_notes_count': len(raw_notes),
            'valid_notes_count': len(valid_notes),
            'enhanced_notes_count': len(enhanced_notes),
            'final_notes_count': len(final_notes),
            
            # æ ¸å¿ƒæŒ‡æ ‡
            'notes_per_1k_tokens': round(notes_per_1k_tokens, 2),
            'avg_facts_per_sentence': round(avg_facts_per_sentence, 3),
            'avg_note_length': round(avg_note_length, 1),
            
            # è¿‡æ»¤ç»Ÿè®¡
            'quality_filtered': quality_filtered,
            'duplicate_filtered': duplicate_filtered,
            'length_filtered': length_filtered,
            'score_filtered': score_filtered,
            
            # è¿‡æ»¤æ¯”ä¾‹
            'quality_filter_ratio': round(quality_filter_ratio, 3),
            'duplicate_filter_ratio': round(duplicate_filter_ratio, 3),
            'total_retention_ratio': round(total_retention_ratio, 3),
            
            # å¤šäº‹å®ç»Ÿè®¡
            'multi_fact_notes': multi_fact_notes,
            'single_fact_notes': single_fact_notes,
            'multi_fact_ratio': round(multi_fact_notes / max(len(enhanced_notes), 1), 3),
            
            # å®ä½“å’Œå…³é”®è¯ç»Ÿè®¡
            'total_entities': total_entities,
            'total_keywords': total_keywords,
            'unique_entities': unique_entities,
            'unique_keywords': unique_keywords,
            'avg_entities_per_note': round(total_entities / max(len(final_notes), 1), 2),
            'avg_keywords_per_note': round(total_keywords / max(len(final_notes), 1), 2),
            
            # å¤„ç†æ•ˆç‡
            'processing_efficiency': round(len(final_notes) / max(total_chars / 1000, 1), 2),  # æ¯åƒå­—ç¬¦äº§å‡ºç¬”è®°æ•°
            'timestamp': self._get_timestamp()
        }
    
    def _log_processing_stats(self, stats: Dict[str, Any]):
        """å°†ç»Ÿè®¡æŒ‡æ ‡å†™å…¥æ—¥å¿—ï¼Œä¾¿äºA/Bæµ‹è¯•åˆ†æï¼Œæä¾›è¯¦ç»†çš„å¤šäº‹å®å¤„ç†ç»Ÿè®¡"""
        logger.info("=== åŸå­ç¬”è®°ç”Ÿæˆç»Ÿè®¡æŒ‡æ ‡ ===")
        
        # åŸºç¡€å¤„ç†ç»Ÿè®¡
        logger.info(f"ğŸ“Š åŸºç¡€ç»Ÿè®¡:")
        logger.info(f"  - å¤„ç†æ–‡æ¡£å—: {stats['chunk_count']} ä¸ª")
        logger.info(f"  - æ€»tokenæ•°: {stats['total_tokens']:,}")
        logger.info(f"  - æ€»å­—ç¬¦æ•°: {stats['total_chars']:,}")
        logger.info(f"  - æ€»å¥å­æ•°: {stats['total_sentences']:,}")
        
        # å„é˜¶æ®µç¬”è®°æ•°é‡
        logger.info(f"ğŸ“ ç¬”è®°å¤„ç†æµç¨‹:")
        logger.info(f"  - åŸå§‹ç”Ÿæˆ: {stats['raw_notes_count']} æ¡")
        logger.info(f"  - è´¨é‡éªŒè¯: {stats['valid_notes_count']} æ¡ (è¿‡æ»¤ {stats['quality_filtered']} æ¡)")
        logger.info(f"  - å…³ç³»å¢å¼º: {stats['enhanced_notes_count']} æ¡")
        logger.info(f"  - å»é‡å: {stats['final_notes_count']} æ¡ (å»é‡ {stats['duplicate_filtered']} æ¡)")
        
        # æ ¸å¿ƒæ•ˆç‡æŒ‡æ ‡
        logger.info(f"âš¡ æ ¸å¿ƒæŒ‡æ ‡:")
        logger.info(f"  - æ¯åƒtokenäº§å‡ºç¬”è®°æ•°: {stats['notes_per_1k_tokens']}")
        logger.info(f"  - å¹³å‡æ¯å¥äº‹å®æ•°: {stats['avg_facts_per_sentence']}")
        logger.info(f"  - å¤„ç†æ•ˆç‡: {stats['processing_efficiency']} ç¬”è®°/åƒå­—ç¬¦")
        logger.info(f"  - å¹³å‡ç¬”è®°é•¿åº¦: {stats['avg_note_length']} å­—ç¬¦")
        
        # è¿‡æ»¤å’Œè´¨é‡æ§åˆ¶
        logger.info(f"ğŸ” è´¨é‡æ§åˆ¶:")
        logger.info(f"  - è´¨é‡è¿‡æ»¤ç‡: {stats['quality_filter_ratio']*100:.1f}% ({stats['quality_filtered']} æ¡)")
        logger.info(f"  - å»é‡è¿‡æ»¤ç‡: {stats['duplicate_filter_ratio']*100:.1f}% ({stats['duplicate_filtered']} æ¡)")
        logger.info(f"  - æ€»ä¿ç•™ç‡: {stats['total_retention_ratio']*100:.1f}%")
        logger.info(f"  - é•¿åº¦è¿‡æ»¤: {stats['length_filtered']} æ¡")
        logger.info(f"  - åˆ†æ•°è¿‡æ»¤: {stats['score_filtered']} æ¡")
        
        # å¤šäº‹å®å¤„ç†ç»Ÿè®¡
        logger.info(f"ğŸ“‹ å¤šäº‹å®å¤„ç†:")
        logger.info(f"  - å¤šäº‹å®ç¬”è®°: {stats['multi_fact_notes']} æ¡ ({stats['multi_fact_ratio']*100:.1f}%)")
        logger.info(f"  - å•äº‹å®ç¬”è®°: {stats['single_fact_notes']} æ¡")
        
        # å®ä½“å’Œå…³é”®è¯ç»Ÿè®¡
        logger.info(f"ğŸ·ï¸ å®ä½“å’Œå…³é”®è¯:")
        logger.info(f"  - æ€»å®ä½“æ•°: {stats['total_entities']} (å”¯ä¸€: {stats['unique_entities']})")
        logger.info(f"  - æ€»å…³é”®è¯æ•°: {stats['total_keywords']} (å”¯ä¸€: {stats['unique_keywords']})")
        logger.info(f"  - å¹³å‡æ¯ç¬”è®°å®ä½“æ•°: {stats['avg_entities_per_note']}")
        logger.info(f"  - å¹³å‡æ¯ç¬”è®°å…³é”®è¯æ•°: {stats['avg_keywords_per_note']}")
        
        logger.info(f"â° å¤„ç†æ—¶é—´: {stats['timestamp']}")
        logger.info("=== ç»Ÿè®¡æŒ‡æ ‡ç»“æŸ ===")
        
        # ç»“æ„åŒ–æ ¼å¼è®°å½•ï¼Œä¾¿äºA/Bæµ‹è¯•å’Œè‡ªåŠ¨åŒ–åˆ†æ
        logger.info(f"ATOMIC_NOTE_STATS_JSON: {json.dumps(stats, ensure_ascii=False)}")
        
        # å…³é”®æŒ‡æ ‡æ‘˜è¦ï¼Œä¾¿äºå¿«é€Ÿå¯¹æ¯”
        summary_stats = {
            'notes_per_1k_tokens': stats['notes_per_1k_tokens'],
            'avg_facts_per_sentence': stats['avg_facts_per_sentence'],
            'total_retention_ratio': stats['total_retention_ratio'],
            'duplicate_filter_ratio': stats['duplicate_filter_ratio'],
            'multi_fact_ratio': stats['multi_fact_ratio'],
            'processing_efficiency': stats['processing_efficiency'],
            'timestamp': stats['timestamp']
        }
        logger.info(f"ATOMIC_NOTE_SUMMARY: {json.dumps(summary_stats, ensure_ascii=False)}")
    
    
    def _calculate_processing_stats(self, file_paths: List[str],
                                   atomic_notes: List[Dict[str, Any]],
                                   clustering_result: Dict[str, Any],
                                   chunk_count: int,
                                   note_count: int) -> Dict[str, Any]:
        """è®¡ç®—å¤„ç†ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'files_processed': len(file_paths),
            'chunks_created': chunk_count,
            'atomic_notes_created': note_count,
            'topic_pools_created': len(clustering_result.get('topic_pools', [])),
            'clusters_found': clustering_result.get('cluster_info', {}).get('n_clusters', 0),
            'noise_notes': clustering_result.get('cluster_info', {}).get('n_noise', 0),
            'silhouette_score': clustering_result.get('cluster_info', {}).get('silhouette_score', 0.0),
            'processing_time': self._get_timestamp()
        }
        
        # è®¡ç®—å¹³å‡ç¬”è®°é•¿åº¦
        if atomic_notes:
            avg_note_length = sum(len(note.get('content', '')) for note in atomic_notes) / len(atomic_notes)
            stats['avg_note_length'] = avg_note_length
        
        # è®¡ç®—å…³é”®è¯å’Œå®ä½“ç»Ÿè®¡
        all_keywords = []
        all_entities = []
        for note in atomic_notes:
            all_keywords.extend(note.get('keywords', []))
            all_entities.extend(note.get('entities', []))
        
        stats['unique_keywords'] = len(set(all_keywords))
        stats['unique_entities'] = len(set(all_entities))
        
        return stats
    
    def _update_processing_cache(self, file_paths: List[str], result: Dict[str, Any]):
        """æ›´æ–°å¤„ç†ç¼“å­˜"""
        for file_path in tqdm(file_paths, desc="Updating cache"):
            self.incremental_processor.update_file_cache(file_path, {
                'atomic_notes_count': len([n for n in result['atomic_notes']
                                         if n.get('source_info', {}).get('file_path') == file_path]),
                'processing_stats': result['processing_stats']
            })
    
    def _load_cached_results(self, file_paths: List[str]) -> Dict[str, Any]:
        """åŠ è½½ç¼“å­˜çš„å¤„ç†ç»“æœ"""
        logger.info("Loading cached results from disk")

        atomic_file = os.path.join(self.processed_docs_path, "atomic_notes.json")
        cluster_file = os.path.join(self.processed_docs_path, "clustering.json")
        embed_file = os.path.join(self.processed_docs_path, "embeddings.npy")
        graph_file = os.path.join(self.processed_docs_path, "graph.json")
        result_file = os.path.join(self.processed_docs_path, "result.json")

        atomic_notes = []
        topic_pools = []
        cluster_info = {}
        graph_data = {}
        embeddings = None
        processing_stats = None

        if os.path.exists(atomic_file):
            try:
                atomic_notes = FileUtils.read_json(atomic_file)
            except Exception as e:
                logger.warning(f"Failed to load atomic notes: {e}")

        if os.path.exists(cluster_file):
            try:
                cluster_data = FileUtils.read_json(cluster_file)
                topic_pools = cluster_data.get('topic_pools', [])
                cluster_info = cluster_data.get('cluster_info', {})
                atomic_notes = cluster_data.get('clustered_notes', atomic_notes)
            except Exception as e:
                logger.warning(f"Failed to load clustering result: {e}")

        if os.path.exists(embed_file):
            try:
                embeddings = np.load(embed_file)
            except Exception as e:
                logger.warning(f"Failed to load embeddings: {e}")

        if os.path.exists(graph_file):
            try:
                graph_data = FileUtils.read_json(graph_file)
            except Exception as e:
                logger.warning(f"Failed to load graph data: {e}")

        if os.path.exists(result_file):
            try:
                processing_stats = FileUtils.read_json(result_file).get('processing_stats')
            except Exception as e:
                logger.warning(f"Failed to load processing stats: {e}")

        if not processing_stats:
            processing_stats = {
                'files_processed': len(file_paths),
                'loaded_from_cache': True
            }

        return {
            'atomic_notes': atomic_notes,
            'topic_pools': topic_pools,
            'cluster_info': cluster_info,
            'graph_data': graph_data,
            'embeddings': embeddings,
            'processing_stats': processing_stats
        }
    
    def process_single_document(self, file_path: str, force_reprocess: bool = False) -> Dict[str, Any]:
        """å¤„ç†å•ä¸ªæ–‡æ¡£"""
        return self.process_documents([file_path], force_reprocess)
    
    def get_processing_status(self, file_paths: List[str]) -> Dict[str, Any]:
        """è·å–æ–‡æ¡£å¤„ç†çŠ¶æ€"""
        processing_plan = self.incremental_processor.get_processing_plan(file_paths)
        cache_stats = self.incremental_processor.get_cache_statistics()
        
        return {
            'processing_plan': processing_plan,
            'cache_statistics': cache_stats,
            'can_skip_processing': processing_plan['can_skip_processing']
        }
    
    def clear_processing_cache(self, file_patterns: List[str] = None):
        """æ¸…ç†å¤„ç†ç¼“å­˜"""
        self.incremental_processor.clear_cache(file_patterns)
        logger.info("Processing cache cleared")
    
    def validate_and_repair_cache(self):
        """éªŒè¯å’Œä¿®å¤ç¼“å­˜"""
        validation_result = self.incremental_processor.validate_cache_integrity()
        
        issues_found = any(validation_result.values())
        if issues_found:
            logger.warning("Cache integrity issues found, repairing...")
            self.incremental_processor.repair_cache(validation_result)
            logger.info("Cache repair completed")
        else:
            logger.info("Cache integrity check passed")
        
        return validation_result
    
    def _get_timestamp(self) -> str:
        """è·å–å½“å‰æ—¶é—´æˆ³"""
        from datetime import datetime
        return datetime.now().isoformat()
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if hasattr(self.llm, 'cleanup'):
            self.llm.cleanup()
        if hasattr(self.embedding_manager, 'cleanup'):
            self.embedding_manager.cleanup()
        logger.info("Document processor cleanup completed")