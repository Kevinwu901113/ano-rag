import json
import random
import time
import threading
from typing import Any, Dict, Iterator, List, Union, Optional, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass, field
from enum import Enum
import subprocess
import psutil
import GPUtil

import openai
import requests
from loguru import logger

from config import config
from .prompts import (
    FINAL_ANSWER_SYSTEM_PROMPT,
    FINAL_ANSWER_PROMPT,
    EVALUATE_ANSWER_SYSTEM_PROMPT,
    EVALUATE_ANSWER_PROMPT,
)


class LoadBalancingStrategy(Enum):
    """è´Ÿè½½å‡è¡¡ç­–ç•¥æšä¸¾"""
    ROUND_ROBIN = "round_robin"
    RANDOM = "random"
    LEAST_BUSY = "least_busy"
    GPU_OPTIMAL = "gpu_optimal"  # åŸºäºGPUä½¿ç”¨ç‡çš„ä¼˜åŒ–ç­–ç•¥
    PERFORMANCE_BASED = "performance_based"  # åŸºäºæ€§èƒ½æŒ‡æ ‡çš„ç­–ç•¥


@dataclass
class ModelInstance:
    """æ¨¡å‹å®ä¾‹é…ç½®"""
    model_name: str
    port: int
    base_url: str
    gpu_id: Optional[int] = None
    process: Optional[subprocess.Popen] = None
    client: Optional[openai.OpenAI] = None
    is_healthy: bool = False
    is_loading: bool = False
    last_health_check: float = 0
    active_requests: int = 0
    total_requests: int = 0
    error_count: int = 0
    avg_response_time: float = 0.0
    gpu_memory_usage: float = 0.0
    
    def __post_init__(self):
        if self.client is None and not self.is_loading:
            self._init_client()
    
    def _init_client(self):
        """åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯"""
        try:
            client_kwargs = {
                "api_key": "lm-studio",
                "base_url": self.base_url,
                "timeout": config.get("llm.multi_model.timeout", 60),
                "max_retries": config.get("llm.multi_model.max_retries", 3),
            }
            self.client = openai.OpenAI(**client_kwargs)
            logger.info(f"Initialized client for model {self.model_name} on port {self.port}")
        except Exception as e:
            logger.error(f"Failed to initialize client for {self.model_name}: {e}")
            self.is_healthy = False


class GPUResourceManager:
    """GPUèµ„æºç®¡ç†å™¨"""
    
    def __init__(self):
        self.gpu_info = self._get_gpu_info()
        self.allocated_memory = {gpu_id: 0 for gpu_id in range(len(self.gpu_info))}
        self._lock = threading.Lock()
    
    def _get_gpu_info(self) -> List[Dict[str, Any]]:
        """è·å–GPUä¿¡æ¯"""
        try:
            gpus = GPUtil.getGPUs()
            return [
                {
                    "id": gpu.id,
                    "name": gpu.name,
                    "memory_total": gpu.memoryTotal,
                    "memory_free": gpu.memoryFree,
                    "memory_used": gpu.memoryUsed,
                    "load": gpu.load,
                }
                for gpu in gpus
            ]
        except Exception as e:
            logger.warning(f"Failed to get GPU info: {e}")
            return []
    
    def allocate_gpu(self, memory_required: float = 4096) -> Optional[int]:
        """åˆ†é…GPUèµ„æº"""
        with self._lock:
            for gpu in self.gpu_info:
                gpu_id = gpu["id"]
                available_memory = gpu["memory_total"] - self.allocated_memory[gpu_id]
                if available_memory >= memory_required:
                    self.allocated_memory[gpu_id] += memory_required
                    logger.info(f"Allocated {memory_required}MB on GPU {gpu_id}")
                    return gpu_id
            return None
    
    def release_gpu(self, gpu_id: int, memory_amount: float):
        """é‡Šæ”¾GPUèµ„æº"""
        with self._lock:
            if gpu_id in self.allocated_memory:
                self.allocated_memory[gpu_id] = max(0, self.allocated_memory[gpu_id] - memory_amount)
                logger.info(f"Released {memory_amount}MB from GPU {gpu_id}")
    
    def get_gpu_usage(self) -> Dict[int, Dict[str, float]]:
        """è·å–GPUä½¿ç”¨æƒ…å†µ"""
        try:
            gpus = GPUtil.getGPUs()
            return {
                gpu.id: {
                    "memory_used": gpu.memoryUsed,
                    "memory_total": gpu.memoryTotal,
                    "load": gpu.load,
                    "allocated": self.allocated_memory.get(gpu.id, 0)
                }
                for gpu in gpus
            }
        except Exception as e:
            logger.warning(f"Failed to get GPU usage: {e}")
            return {}


class MultiModelClient:
    """å¤šæ¨¡å‹å¹¶è¡Œå®¢æˆ·ç«¯
    
    æ”¯æŒåŒæ—¶åŠ è½½å’Œè¿è¡Œå¤šä¸ªç‹¬ç«‹çš„æ¨¡å‹å®ä¾‹ï¼š
    - å¤šæ¨¡å‹å®ä¾‹ï¼šæ¯ä¸ªæ¨¡å‹è¿è¡Œåœ¨ç‹¬ç«‹çš„ç«¯å£å’Œè¿›ç¨‹ä¸­
    - GPUèµ„æºç®¡ç†ï¼šæ™ºèƒ½åˆ†é…GPUèµ„æºç»™ä¸åŒæ¨¡å‹
    - è´Ÿè½½å‡è¡¡ï¼šæ ¹æ®æ¨¡å‹æ€§èƒ½å’ŒGPUä½¿ç”¨æƒ…å†µåˆ†å‘è¯·æ±‚
    - å¥åº·ç›‘æ§ï¼šç›‘æ§æ¯ä¸ªæ¨¡å‹å®ä¾‹çš„å¥åº·çŠ¶æ€å’Œæ€§èƒ½
    
    æä¾›ç»Ÿä¸€çš„APIæ¥å£ï¼Œæ”¯æŒé«˜æ•ˆçš„å¹¶å‘å¤„ç†ã€‚
    """

    def __init__(self):
        # åŸºç¡€é…ç½®
        self.temperature = config.get("llm.multi_model.temperature", 0.7)
        self.max_tokens = config.get("llm.multi_model.max_tokens", 4096)
        self.timeout = config.get("llm.multi_model.timeout", 60)
        self.max_retries = config.get("llm.multi_model.max_retries", 3)
        
        # å¤šæ¨¡å‹é…ç½®
        self.model_instances: List[ModelInstance] = []
        self.gpu_manager = GPUResourceManager()
        
        # è´Ÿè½½å‡è¡¡é…ç½®
        strategy_str = config.get("llm.multi_model.load_balancing", "gpu_optimal")
        self.load_balancing_strategy = LoadBalancingStrategy(strategy_str)
        self._round_robin_index = 0
        self._lock = threading.Lock()
        
        # å¥åº·æ£€æŸ¥é…ç½®
        self.health_check_interval = config.get("llm.multi_model.health_check_interval", 30)
        self.health_check_thread = None
        
        # é»˜è®¤é€‰é¡¹
        self.default_options = {
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
        }
        
        # åˆå§‹åŒ–æ¨¡å‹å®ä¾‹
        self._init_model_instances()
        
        # å¯åŠ¨å¥åº·æ£€æŸ¥
        self._start_health_check_thread()
        
        logger.info(f"Initialized MultiModelClient with {len(self.model_instances)} model instances")
    
    def _init_model_instances(self):
        """åˆå§‹åŒ–å¤šä¸ªæ¨¡å‹å®ä¾‹
        
        åˆ©ç”¨LM Studioçš„è‡ªåŠ¨å‘½åæœºåˆ¶ï¼š
        - é‡å¤åŠ è½½ç›¸åŒæ¨¡å‹æ—¶ï¼ŒLM Studioä¼šè‡ªåŠ¨ä¸ºæ¨¡å‹åˆ†é…ä¸åŒåç§°
        - ä¾‹å¦‚ï¼šgpt-oss-20b, gpt-oss-20b:2, gpt-oss-20b:3 ç­‰
        - æ‰€æœ‰å®ä¾‹å…±äº«åŒä¸€ä¸ªç«¯å£ï¼ˆ1234ï¼‰ï¼Œé€šè¿‡æ¨¡å‹åç§°åŒºåˆ†
        """
        models_config = config.get("llm.multi_model.instances", [])
        base_port = config.get("llm.multi_model.base_port", 1234)
        base_url = f"http://localhost:{base_port}/v1"
        
        if not models_config:
            # å¦‚æœæ²¡æœ‰é…ç½®ï¼Œåˆ›å»ºé»˜è®¤çš„å¤šå®ä¾‹é…ç½®
            model_count = config.get("llm.multi_model.instance_count", 2)
            base_model_name = config.get("llm.multi_model.model_name", "gpt-oss-20b")
            
            for i in range(model_count):
                # ç¬¬ä¸€ä¸ªå®ä¾‹ä½¿ç”¨åŸå§‹æ¨¡å‹åï¼Œåç»­å®ä¾‹ä½¿ç”¨LM Studioçš„è‡ªåŠ¨å‘½å
                if i == 0:
                    model_name = base_model_name
                else:
                    model_name = f"{base_model_name}:{i+1}"
                
                instance = ModelInstance(
                    model_name=model_name,
                    port=base_port,  # æ‰€æœ‰å®ä¾‹ä½¿ç”¨åŒä¸€ä¸ªç«¯å£
                    base_url=base_url,
                    gpu_id=self.gpu_manager.allocate_gpu()
                )
                self.model_instances.append(instance)
        else:
            # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å®ä¾‹é…ç½®
            for model_config in models_config:
                port = model_config.get("port", base_port)
                instance = ModelInstance(
                    model_name=model_config["model_name"],
                    port=port,
                    base_url=model_config.get("base_url", f"http://localhost:{port}/v1"),
                    gpu_id=model_config.get("gpu_id") or self.gpu_manager.allocate_gpu()
                )
                self.model_instances.append(instance)
        
        # æ£€æŸ¥æ¨¡å‹å®ä¾‹æ˜¯å¦å·²åœ¨LM Studioä¸­åŠ è½½
        self._check_and_load_model_instances()
    
    def _check_and_load_model_instances(self):
        """æ£€æŸ¥å¹¶åŠ è½½æ¨¡å‹å®ä¾‹
        
        æ£€æŸ¥LM Studioä¸­æ˜¯å¦å·²åŠ è½½æ‰€éœ€çš„æ¨¡å‹å®ä¾‹ï¼š
        1. ç›´æ¥æµ‹è¯•æ¯ä¸ªé…ç½®çš„æ¨¡å‹åç§°æ˜¯å¦å¯ç”¨
        2. å¦‚æœæ¨¡å‹å¯ç”¨ï¼Œç›´æ¥åˆå§‹åŒ–å®¢æˆ·ç«¯
        3. å¦‚æœæ¨¡å‹ä¸å¯ç”¨ï¼Œæ ‡è®°ä¸ºæœªåŠ è½½
        """
        try:
            # è·å–å½“å‰LM Studioä¸­å·²åŠ è½½çš„æ¨¡å‹åˆ—è¡¨ï¼ˆä»…ç”¨äºæ—¥å¿—ï¼‰
            available_models = self._get_available_models()
            logger.info(f"Available models in LM Studio: {available_models}")
            
            for instance in self.model_instances:
                # ç›´æ¥æµ‹è¯•æ¨¡å‹æ˜¯å¦å¯ç”¨ï¼Œè€Œä¸ä¾èµ–äº/v1/modelsæ¥å£çš„è¿”å›
                if self._test_model_availability(instance.model_name):
                    logger.info(f"Model {instance.model_name} is available and responding")
                    instance._init_client()
                    instance.is_healthy = True
                else:
                    logger.warning(f"Model {instance.model_name} is not available or not responding")
                    instance.is_healthy = False
            
            # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„å¥åº·å®ä¾‹
            healthy_count = sum(1 for inst in self.model_instances if inst.is_healthy)
            if healthy_count == 0:
                logger.error("No healthy model instances available. Please load models in LM Studio manually.")
                self._print_loading_instructions()
            elif healthy_count < len(self.model_instances):
                logger.warning(f"Only {healthy_count}/{len(self.model_instances)} model instances are healthy")
                self._print_loading_instructions()
            else:
                logger.info(f"All {healthy_count} model instances are ready")
                
        except Exception as e:
            logger.error(f"Failed to check model instances: {e}")
            # å¦‚æœæ— æ³•è¿æ¥åˆ°LM Studioï¼Œå°è¯•åˆå§‹åŒ–æ‰€æœ‰å®ä¾‹
            for instance in self.model_instances:
                instance._init_client()
    
    def _get_available_models(self) -> List[str]:
        """è·å–LM Studioä¸­å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨"""
        try:
            base_port = config.get("llm.multi_model.base_port", 1234)
            response = requests.get(f"http://localhost:{base_port}/v1/models", timeout=10)
            if response.status_code == 200:
                models_data = response.json()
                models = [model["id"] for model in models_data.get("data", [])]
                logger.info(f"åœ¨ç«¯å£ {base_port} æ‰¾åˆ° {len(models)} ä¸ªæ¨¡å‹: {models}")
                return models
            else:
                logger.warning(f"Failed to get models list: HTTP {response.status_code}")
                return []
        except Exception as e:
            logger.warning(f"Failed to connect to LM Studio: {e}")
            return []
    
    def _test_model_availability(self, model_name: str) -> bool:
        """ç›´æ¥æµ‹è¯•æ¨¡å‹æ˜¯å¦å¯ç”¨
        
        é€šè¿‡å‘é€ä¸€ä¸ªç®€å•çš„è¯·æ±‚æ¥æµ‹è¯•æ¨¡å‹æ˜¯å¦çœŸæ­£å¯ç”¨
        """
        try:
            base_port = config.get("llm.multi_model.base_port", 1234)
            url = f"http://localhost:{base_port}/v1/chat/completions"
            
            payload = {
                "model": model_name,
                "messages": [{"role": "user", "content": "test"}],
                "max_tokens": 1,
                "temperature": 0
            }
            
            response = requests.post(url, json=payload, timeout=10)
            
            if response.status_code == 200:
                logger.debug(f"Model {model_name} test successful")
                return True
            else:
                logger.debug(f"Model {model_name} test failed with status {response.status_code}")
                return False
                
        except Exception as e:
            logger.debug(f"Model {model_name} test failed with exception: {e}")
            return False
    
    def _print_loading_instructions(self):
        """æ‰“å°æ¨¡å‹åŠ è½½æŒ‡å¯¼"""
        base_model_name = config.get("llm.multi_model.model_name", "gpt-oss-20b")
        model_count = len(self.model_instances)
        
        logger.info("\n" + "="*60)
        logger.info("å¤šæ¨¡å‹åŠ è½½æŒ‡å¯¼:")
        logger.info(f"éœ€è¦å¯åŠ¨ {model_count} ä¸ªç‹¬ç«‹çš„LM Studioå®ä¾‹")
        logger.info("æ¯ä¸ªå®ä¾‹è¿è¡Œåœ¨ä¸åŒçš„ç«¯å£ä¸Š:")
        for i, instance in enumerate(self.model_instances):
            status = "âœ“" if instance.is_healthy else "âœ—"
            logger.info(f"  {status} å®ä¾‹ {i+1}: ç«¯å£ {instance.port} - æ¨¡å‹ {instance.model_name}")
        logger.info("\nåŠ è½½æ­¥éª¤:")
        logger.info("1. å¯åŠ¨ç¬¬ä¸€ä¸ªLM Studioå®ä¾‹ (é»˜è®¤ç«¯å£1234)")
        logger.info(f"   - åŠ è½½æ¨¡å‹: {base_model_name}")
        for i in range(1, model_count):
            port = self.model_instances[i].port
            logger.info(f"2. å¯åŠ¨ç¬¬{i+1}ä¸ªLM Studioå®ä¾‹ (ç«¯å£{port})")
            logger.info(f"   - åœ¨LM Studioè®¾ç½®ä¸­ä¿®æ”¹æœåŠ¡å™¨ç«¯å£ä¸º {port}")
            logger.info(f"   - åŠ è½½æ¨¡å‹: {base_model_name}")
        logger.info(f"{model_count+1}. ç¡®ä¿æ‰€æœ‰æ¨¡å‹å®ä¾‹éƒ½å·²å¯åŠ¨å¹¶å¯è®¿é—®")
        logger.info(f"{model_count+2}. é‡æ–°è¿è¡Œç¨‹åº")
        logger.info("="*60 + "\n")
    

    
    def _start_health_check_thread(self):
        """å¯åŠ¨å¥åº·æ£€æŸ¥çº¿ç¨‹"""
        if self.health_check_thread is None or not self.health_check_thread.is_alive():
            self.health_check_thread = threading.Thread(
                target=self._health_check_loop,
                daemon=True
            )
            self.health_check_thread.start()
            logger.info("Health check thread started")
    
    def _health_check_loop(self):
        """å¥åº·æ£€æŸ¥å¾ªç¯"""
        while True:
            try:
                self._perform_health_checks()
                time.sleep(self.health_check_interval)
            except Exception as e:
                logger.error(f"Health check error: {e}")
                time.sleep(5)
    
    def _perform_health_checks(self):
        """æ‰§è¡Œå¥åº·æ£€æŸ¥"""
        current_time = time.time()
        
        for instance in self.model_instances:
            if instance.is_loading:
                continue
                
            try:
                # æ£€æŸ¥æ¨¡å‹å®ä¾‹æ˜¯å¦å“åº”
                start_time = time.time()
                response = requests.get(f"{instance.base_url}/models", timeout=10)
                response_time = time.time() - start_time
                
                if response.status_code == 200:
                    instance.is_healthy = True
                    # æ›´æ–°å¹³å‡å“åº”æ—¶é—´
                    if instance.avg_response_time == 0:
                        instance.avg_response_time = response_time
                    else:
                        instance.avg_response_time = (instance.avg_response_time * 0.8 + response_time * 0.2)
                else:
                    instance.is_healthy = False
                    
            except Exception as e:
                instance.is_healthy = False
                logger.warning(f"Health check failed for {instance.model_name}: {e}")
            
            instance.last_health_check = current_time
            
            # æ›´æ–°GPUä½¿ç”¨æƒ…å†µ
            if instance.gpu_id is not None:
                gpu_usage = self.gpu_manager.get_gpu_usage()
                if instance.gpu_id in gpu_usage:
                    instance.gpu_memory_usage = gpu_usage[instance.gpu_id]["memory_used"]
    
    def _select_instance(self) -> Optional[ModelInstance]:
        """æ ¹æ®è´Ÿè½½å‡è¡¡ç­–ç•¥é€‰æ‹©å®ä¾‹"""
        healthy_instances = [inst for inst in self.model_instances if inst.is_healthy and not inst.is_loading]
        
        if not healthy_instances:
            logger.warning("No healthy model instances available")
            return None
        
        if self.load_balancing_strategy == LoadBalancingStrategy.ROUND_ROBIN:
            with self._lock:
                instance = healthy_instances[self._round_robin_index % len(healthy_instances)]
                self._round_robin_index += 1
                return instance
        
        elif self.load_balancing_strategy == LoadBalancingStrategy.RANDOM:
            return random.choice(healthy_instances)
        
        elif self.load_balancing_strategy == LoadBalancingStrategy.LEAST_BUSY:
            return min(healthy_instances, key=lambda x: x.active_requests)
        
        elif self.load_balancing_strategy == LoadBalancingStrategy.GPU_OPTIMAL:
            # é€‰æ‹©GPUä½¿ç”¨ç‡æœ€ä½çš„å®ä¾‹ï¼Œå¦‚æœç›¸åŒåˆ™é€‰æ‹©æ´»è·ƒè¯·æ±‚æœ€å°‘çš„
            def gpu_score(instance):
                return (instance.gpu_memory_usage, instance.active_requests, instance.total_requests)
            return min(healthy_instances, key=gpu_score)
        
        elif self.load_balancing_strategy == LoadBalancingStrategy.PERFORMANCE_BASED:
            # åŸºäºå“åº”æ—¶é—´å’Œæ´»è·ƒè¯·æ±‚æ•°çš„ç»¼åˆè¯„åˆ†
            def calculate_score(instance):
                response_penalty = instance.avg_response_time * 10
                load_penalty = instance.active_requests * 5
                return response_penalty + load_penalty
            
            return min(healthy_instances, key=calculate_score)
        
        return healthy_instances[0]
    
    def _execute_with_retry(self, func, *args, **kwargs):
        """å¸¦é‡è¯•çš„æ‰§è¡Œå‡½æ•°"""
        last_exception = None
        
        for attempt in range(self.max_retries + 1):
            instance = self._select_instance()
            
            if instance is None:
                raise Exception("No healthy model instances available")
            
            try:
                # æ›´æ–°å®ä¾‹çŠ¶æ€
                instance.active_requests += 1
                instance.total_requests += 1
                
                start_time = time.time()
                result = func(instance, *args, **kwargs)
                response_time = time.time() - start_time
                
                # æ›´æ–°å¹³å‡å“åº”æ—¶é—´
                if instance.avg_response_time == 0:
                    instance.avg_response_time = response_time
                else:
                    instance.avg_response_time = (instance.avg_response_time * 0.9 + response_time * 0.1)
                
                return result
                
            except Exception as e:
                last_exception = e
                instance.error_count += 1
                instance.is_healthy = False  # æ ‡è®°ä¸ºä¸å¥åº·
                logger.warning(f"Request failed on model {instance.model_name} (attempt {attempt + 1}): {e}")
                
                if attempt < self.max_retries:
                    time.sleep(0.5 * (attempt + 1))  # æŒ‡æ•°é€€é¿
            
            finally:
                instance.active_requests = max(0, instance.active_requests - 1)
        
        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
        raise last_exception or Exception("All model instances failed")
    
    def generate(
        self,
        prompt: str,
        system_prompt: str | None = None,
        *,
        stream: bool = False,
        **kwargs: Any,
    ) -> str:
        """ç”Ÿæˆæ–‡æœ¬"""
        def _generate(instance: ModelInstance, prompt: str, system_prompt: str | None, stream: bool, **kwargs):
            messages = []
            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})
            messages.append({"role": "user", "content": prompt})
            
            options = {**self.default_options, **kwargs}
            
            response = instance.client.chat.completions.create(
                model=instance.model_name,
                messages=messages,
                stream=stream,
                **options
            )
            
            if stream:
                return response
            else:
                return response.choices[0].message.content
        
        return self._execute_with_retry(_generate, prompt, system_prompt, stream, **kwargs)
    
    def chat(
        self,
        messages: list[dict[str, str]],
        *,
        stream: bool = False,
        **kwargs: Any,
    ) -> Union[str, Iterator[str]]:
        """å¯¹è¯æ¥å£"""
        def _chat(instance: ModelInstance, messages: list[dict[str, str]], stream: bool, **kwargs):
            options = {**self.default_options, **kwargs}
            
            response = instance.client.chat.completions.create(
                model=instance.model_name,
                messages=messages,
                stream=stream,
                **options
            )
            
            if stream:
                return response
            else:
                return response.choices[0].message.content
        
        return self._execute_with_retry(_chat, messages, stream, **kwargs)
    
    def generate_concurrent(
        self,
        prompts: List[str],
        system_prompt: str | None = None,
        **kwargs: Any,
    ) -> List[str]:
        """å¹¶å‘ç”Ÿæˆå¤šä¸ªæ–‡æœ¬ï¼ˆä½¿ç”¨è´Ÿè½½å‡è¡¡ï¼‰"""
        if not prompts:
            return []
        
        # ä½¿ç”¨æ‰€æœ‰å¥åº·çš„å®ä¾‹è¿›è¡Œå¹¶å‘å¤„ç†
        healthy_instances = [inst for inst in self.model_instances if inst.is_healthy and not inst.is_loading]
        max_workers = min(len(healthy_instances), len(prompts))
        
        if max_workers == 0:
            raise Exception("No healthy model instances available for concurrent processing")
        
        results = [None] * len(prompts)
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤ä»»åŠ¡
            future_to_index = {}
            for i, prompt in enumerate(prompts):
                future = executor.submit(self.generate, prompt, system_prompt, **kwargs)
                future_to_index[future] = i
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_index):
                index = future_to_index[future]
                try:
                    results[index] = future.result()
                except Exception as e:
                    logger.error(f"Concurrent generation failed for prompt {index}: {e}")
                    results[index] = f"Error: {str(e)}"
        
        return results
    
    def generate_parallel(
        self,
        prompts: List[str],
        system_prompt: str | None = None,
        **kwargs: Any,
    ) -> List[str]:
        """çœŸæ­£çš„å¹¶è¡Œç”Ÿæˆå¤šä¸ªæ–‡æœ¬ï¼ˆç›´æ¥åˆ†é…åˆ°å®ä¾‹ï¼‰
        
        ä¸generate_concurrentä¸åŒï¼Œè¿™ä¸ªæ–¹æ³•ç¡®ä¿å¤šä¸ªå®ä¾‹çœŸæ­£åŒæ—¶å¤„ç†è¯·æ±‚ï¼Œ
        è€Œä¸æ˜¯é€šè¿‡è´Ÿè½½å‡è¡¡ç­–ç•¥é€‰æ‹©å•ä¸ªå®ä¾‹ã€‚
        
        Args:
            prompts: æç¤ºåˆ—è¡¨
            system_prompt: ç³»ç»Ÿæç¤º
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            å“åº”åˆ—è¡¨
        """
        if not prompts:
            return []
        
        # è·å–å¥åº·å®ä¾‹
        healthy_instances = [inst for inst in self.model_instances if inst.is_healthy and not inst.is_loading]
        
        if not healthy_instances:
            raise Exception("No healthy model instances available for parallel processing")
        
        logger.info(f"ğŸš€ å¼€å§‹å¹¶è¡Œå¤„ç† {len(prompts)} ä¸ªè¯·æ±‚ï¼Œä½¿ç”¨ {len(healthy_instances)} ä¸ªå®ä¾‹")
        
        # å¦‚æœæç¤ºæ•°é‡å°‘äºæˆ–ç­‰äºå®ä¾‹æ•°é‡ï¼Œæ¯ä¸ªå®ä¾‹å¤„ç†ä¸€ä¸ª
        if len(prompts) <= len(healthy_instances):
            return self._execute_direct_parallel(prompts, healthy_instances, system_prompt, **kwargs)
        else:
            # å¦‚æœæç¤ºæ•°é‡å¤šäºå®ä¾‹æ•°é‡ï¼Œéœ€è¦åˆ†æ‰¹å¤„ç†
            return self._execute_batch_parallel(prompts, healthy_instances, system_prompt, **kwargs)
    
    def _execute_direct_parallel(self, prompts: List[str], instances: List, system_prompt: str = None, **kwargs) -> List[str]:
        """ç›´æ¥å¹¶è¡Œæ‰§è¡Œï¼ˆæç¤ºæ•° <= å®ä¾‹æ•°ï¼‰"""
        results = [None] * len(prompts)
        
        with ThreadPoolExecutor(max_workers=len(instances)) as executor:
            # ä¸ºæ¯ä¸ªæç¤ºåˆ†é…ä¸€ä¸ªå®ä¾‹
            future_to_index = {}
            for i, (prompt, instance) in enumerate(zip(prompts, instances)):
                future = executor.submit(self._execute_single_request, instance, prompt, system_prompt, **kwargs)
                future_to_index[future] = i
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_index):
                index = future_to_index[future]
                try:
                    results[index] = future.result()
                except Exception as e:
                    logger.error(f"Parallel request {index} failed: {e}")
                    results[index] = f"Error: {str(e)}"
        
        return results
    
    def _execute_batch_parallel(self, prompts: List[str], instances: List, system_prompt: str = None, **kwargs) -> List[str]:
        """æ‰¹é‡å¹¶è¡Œæ‰§è¡Œï¼ˆæç¤ºæ•° > å®ä¾‹æ•°ï¼‰"""
        results = [None] * len(prompts)
        num_instances = len(instances)
        
        # å°†æç¤ºåˆ†é…ç»™å®ä¾‹
        instance_tasks = [[] for _ in range(num_instances)]
        for i, prompt in enumerate(prompts):
            instance_idx = i % num_instances
            instance_tasks[instance_idx].append((i, prompt))
        
        with ThreadPoolExecutor(max_workers=num_instances) as executor:
            # ä¸ºæ¯ä¸ªå®ä¾‹æäº¤ä¸€ä¸ªæ‰¹é‡ä»»åŠ¡
            futures = []
            for instance_idx, tasks in enumerate(instance_tasks):
                if tasks:  # åªå¤„ç†æœ‰ä»»åŠ¡çš„å®ä¾‹
                    future = executor.submit(
                        self._execute_instance_batch, 
                        instances[instance_idx], 
                        tasks, 
                        system_prompt, 
                        **kwargs
                    )
                    futures.append((future, instance_idx))
            
            # æ”¶é›†ç»“æœ
            for future, instance_idx in futures:
                try:
                    batch_results = future.result()
                    for original_index, result in batch_results:
                        results[original_index] = result
                except Exception as e:
                    logger.error(f"Batch execution failed for instance {instance_idx}: {e}")
                    # ä¸ºè¯¥å®ä¾‹çš„æ‰€æœ‰ä»»åŠ¡è®¾ç½®é”™è¯¯ç»“æœ
                    for original_index, _ in instance_tasks[instance_idx]:
                        results[original_index] = f"Error: {str(e)}"
        
        return results
    
    def _execute_instance_batch(self, instance, tasks: List[tuple], system_prompt: str = None, **kwargs) -> List[tuple]:
        """å•ä¸ªå®ä¾‹æ‰§è¡Œæ‰¹é‡ä»»åŠ¡"""
        results = []
        for original_index, prompt in tasks:
            try:
                result = self._execute_single_request(instance, prompt, system_prompt, **kwargs)
                results.append((original_index, result))
            except Exception as e:
                logger.error(f"Single request failed in batch: {e}")
                results.append((original_index, f"Error: {str(e)}"))
        return results
    
    def _execute_single_request(self, instance, prompt: str, system_prompt: str = None, **kwargs) -> str:
        """æ‰§è¡Œå•ä¸ªè¯·æ±‚"""
        try:
            # æ›´æ–°å®ä¾‹çŠ¶æ€
            instance.active_requests += 1
            
            # å‡†å¤‡æ¶ˆæ¯
            messages = []
            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})
            messages.append({"role": "user", "content": prompt})
            
            # åˆå¹¶é€‰é¡¹
            options = {**self.default_options, **kwargs}
            
            # æ‰§è¡Œè¯·æ±‚
            start_time = time.time()
            response = instance.client.chat.completions.create(
                model=instance.model_name,
                messages=messages,
                **options
            )
            end_time = time.time()
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            instance.total_requests += 1
            response_time = end_time - start_time
            instance.avg_response_time = (
                (instance.avg_response_time * (instance.total_requests - 1) + response_time) / 
                instance.total_requests
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            instance.error_count += 1
            raise e
        finally:
            instance.active_requests -= 1
    
    def get_status(self) -> Dict[str, Any]:
        """è·å–å¤šæ¨¡å‹ç³»ç»ŸçŠ¶æ€"""
        healthy_count = sum(1 for inst in self.model_instances if inst.is_healthy)
        loading_count = sum(1 for inst in self.model_instances if inst.is_loading)
        total_requests = sum(inst.total_requests for inst in self.model_instances)
        total_errors = sum(inst.error_count for inst in self.model_instances)
        
        return {
            "total_instances": len(self.model_instances),
            "healthy_instances": healthy_count,
            "loading_instances": loading_count,
            "total_requests": total_requests,
            "total_errors": total_errors,
            "error_rate": total_errors / max(total_requests, 1),
            "load_balancing_strategy": self.load_balancing_strategy.value,
            "gpu_usage": self.gpu_manager.get_gpu_usage(),
            "instances": [
                {
                    "model_name": inst.model_name,
                    "port": inst.port,
                    "gpu_id": inst.gpu_id,
                    "is_healthy": inst.is_healthy,
                    "is_loading": inst.is_loading,
                    "active_requests": inst.active_requests,
                    "total_requests": inst.total_requests,
                    "error_count": inst.error_count,
                    "avg_response_time": inst.avg_response_time,
                    "gpu_memory_usage": inst.gpu_memory_usage,
                }
                for inst in self.model_instances
            ]
        }
    
    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return {
            'provider': 'multi_model',
            'instance_count': len(self.model_instances),
            'healthy_instances': sum(1 for inst in self.model_instances if inst.is_healthy),
            'load_balancing_strategy': self.load_balancing_strategy.value,
            'temperature': self.temperature,
            'max_tokens': self.max_tokens,
            'gpu_info': self.gpu_manager.gpu_info,
            'instances': [{
                'model_name': inst.model_name,
                'port': inst.port,
                'gpu_id': inst.gpu_id,
                'is_healthy': inst.is_healthy,
                'is_loading': inst.is_loading,
            } for inst in self.model_instances]
        }
    
    def shutdown(self):
        """å…³é—­å¤šæ¨¡å‹ç³»ç»Ÿ"""
        logger.info("Shutting down MultiModelClient...")
        
        # åœæ­¢å¥åº·æ£€æŸ¥çº¿ç¨‹
        if self.health_check_thread and self.health_check_thread.is_alive():
            # è¿™é‡Œéœ€è¦å®ç°çº¿ç¨‹åœæ­¢æœºåˆ¶
            pass
        
        # åœæ­¢æ‰€æœ‰æ¨¡å‹å®ä¾‹
        for instance in self.model_instances:
            try:
                if instance.process and instance.process.poll() is None:
                    instance.process.terminate()
                    instance.process.wait(timeout=10)
                
                # é‡Šæ”¾GPUèµ„æº
                if instance.gpu_id is not None:
                    self.gpu_manager.release_gpu(instance.gpu_id, 4096)  # å‡è®¾æ¯ä¸ªæ¨¡å‹å ç”¨4GB
                    
            except Exception as e:
                logger.error(f"Error shutting down instance {instance.model_name}: {e}")
        
        logger.info("MultiModelClient shutdown complete")


# ä¸ºäº†å‘åå…¼å®¹ï¼Œä¿ç•™åŸæœ‰çš„ç±»ååˆ«å
MultiLMStudioClient = MultiModelClient