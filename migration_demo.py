#!/usr/bin/env python3
"""
Anorag LLM è¿ç§»æ¼”ç¤ºè„šæœ¬
å±•ç¤ºä» Ollama åˆ° vLLM çš„å®Œæ•´è¿ç§»æµç¨‹
"""

import sys
import os
import yaml
from pathlib import Path
from typing import Dict, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

def load_config() -> Dict[str, Any]:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    config_path = project_root / "config.yaml"
    if not config_path.exists():
        print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        return {}
    
    with open(config_path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)

def show_config_migration():
    """å±•ç¤ºé…ç½®è¿ç§»"""
    print("=== é…ç½®è¿ç§»æ¼”ç¤º ===")
    print("\n1. åŸå§‹ Ollama é…ç½®:")
    print("```yaml")
    print("llm:")
    print("  provider: ollama")
    print("  ollama:")
    print("    base_url: http://localhost:11434")
    print("    model: qwen2.5:0.5b")
    print("```")
    
    print("\n2. æ–°å¢ vLLM é…ç½®:")
    print("```yaml")
    print("llm:")
    print("  provider: vllm_openai  # åˆ‡æ¢åˆ° vLLM")
    print("  vllm_openai:")
    print("    routes:")
    print("      tiny_qwen:")
    print("        base_url: http://127.0.0.1:8001/v1")
    print("        model: qwen2_5_0_5b")
    print("      large_qwen:")
    print("        base_url: http://127.0.0.1:8002/v1")
    print("        model: qwen2_5_7b")
    print("    default_route: tiny_qwen")
    print("    params:")
    print("      temperature: 0.0")
    print("      max_tokens: 2048")
    print("```")
    
    # æ˜¾ç¤ºå½“å‰é…ç½®
    config = load_config()
    if config:
        print("\n3. å½“å‰é¡¹ç›®é…ç½®:")
        llm_config = config.get('llm', {})
        provider = llm_config.get('provider', 'unknown')
        print(f"   å½“å‰æä¾›å•†: {provider}")
        
        if provider == 'vllm_openai':
            vllm_config = llm_config.get('vllm_openai', {})
            routes = vllm_config.get('routes', {})
            default_route = vllm_config.get('default_route', 'unknown')
            print(f"   é»˜è®¤è·¯ç”±: {default_route}")
            print(f"   å¯ç”¨è·¯ç”±: {list(routes.keys())}")
            print("   âœ… vLLM é…ç½®å·²å°±ç»ª")
        else:
            print("   âš ï¸  å½“å‰ä½¿ç”¨ä¼ ç»Ÿé…ç½®")

def show_provider_architecture():
    """å±•ç¤ºæä¾›å•†æ¶æ„"""
    print("\n=== æä¾›å•†æ¶æ„æ¼”ç¤º ===")
    print("\n1. å·¥å‚æ¨¡å¼è®¾è®¡:")
    print("   ğŸ“ llm/factory.py - LLM å·¥å‚ç±»")
    print("   ğŸ“ llm/providers/ - æä¾›å•†å®ç°")
    print("   â”œâ”€â”€ __init__.py")
    print("   â””â”€â”€ vllm_openai.py - vLLM OpenAI å…¼å®¹æä¾›å•†")
    
    print("\n2. æä¾›å•†æ³¨å†Œ:")
    print("   - OllamaClient (ä¼ ç»Ÿ)")
    print("   - MultiOllamaClient (å¤šå®ä¾‹)")
    print("   - OpenAIClient (OpenAI API)")
    print("   - VLLMOpenAIProvider (æ–°å¢ vLLM)")
    
    print("\n3. è‡ªåŠ¨å›é€€æœºåˆ¶:")
    print("   vLLM ä¸å¯ç”¨ â†’ è‡ªåŠ¨å›é€€åˆ° Ollama")
    print("   ç¡®ä¿æœåŠ¡è¿ç»­æ€§å’Œå…¼å®¹æ€§")

def show_usage_examples():
    """å±•ç¤ºä½¿ç”¨ç¤ºä¾‹"""
    print("\n=== ä½¿ç”¨ç¤ºä¾‹æ¼”ç¤º ===")
    
    print("\n1. ç›´æ¥ä½¿ç”¨ vLLM Provider:")
    print("```python")
    print("from llm.providers.vllm_openai import VLLMOpenAIProvider")
    print("")
    print("provider = VLLMOpenAIProvider(")
    print("    base_url='http://127.0.0.1:8001/v1',")
    print("    model='qwen2_5_0_5b'")
    print(")")
    print("")
    print("response = provider.chat([")
    print("    {'role': 'user', 'content': 'ä½ å¥½'}")
    print("])")
    print("```")
    
    print("\n2. é€šè¿‡å·¥å‚æ¨¡å¼ä½¿ç”¨:")
    print("```python")
    print("from llm.factory import LLMFactory")
    print("")
    print("provider = LLMFactory.create_provider('vllm_openai', config)")
    print("response = provider.chat(messages)")
    print("```")
    
    print("\n3. é›†æˆåˆ° LocalLLM:")
    print("```python")
    print("from llm import LocalLLM")
    print("")
    print("# é…ç½®æ–‡ä»¶ä¸­è®¾ç½® provider: vllm_openai")
    print("llm = LocalLLM(provider='vllm_openai')")
    print("response = llm.generate('ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±')")
    print("```")

def show_performance_benefits():
    """å±•ç¤ºæ€§èƒ½ä¼˜åŠ¿"""
    print("\n=== æ€§èƒ½ä¼˜åŠ¿æ¼”ç¤º ===")
    
    print("\n1. vLLM ä¼˜åŠ¿:")
    print("   ğŸš€ æ›´é«˜çš„æ¨ç†ååé‡")
    print("   âš¡ æ›´ä½çš„å»¶è¿Ÿ")
    print("   ğŸ”§ æ›´å¥½çš„ GPU åˆ©ç”¨ç‡")
    print("   ğŸ“Š æ”¯æŒæ‰¹å¤„ç†æ¨ç†")
    print("   ğŸ”„ åŠ¨æ€æ‰¹å¤„ç†")
    
    print("\n2. å…¼å®¹æ€§:")
    print("   âœ… OpenAI API å®Œå…¨å…¼å®¹")
    print("   âœ… æ”¯æŒæµå¼è¾“å‡º")
    print("   âœ… æ”¯æŒå¼‚æ­¥è°ƒç”¨")
    print("   âœ… æ”¯æŒå¤šæ¨¡å‹è·¯ç”±")
    
    print("\n3. å‹æµ‹è„šæœ¬:")
    print("   ğŸ“„ benchmark_vllm.py - æ€§èƒ½æµ‹è¯•")
    print("   ğŸ“Š æ”¯æŒå¹¶å‘æµ‹è¯•")
    print("   ğŸ“ˆ å»¶è¿Ÿå’Œååé‡ç»Ÿè®¡")
    print("   ğŸ” GPU åˆ©ç”¨ç‡ç›‘æ§")

def show_deployment_guide():
    """å±•ç¤ºéƒ¨ç½²æŒ‡å—"""
    print("\n=== éƒ¨ç½²æŒ‡å—æ¼”ç¤º ===")
    
    print("\n1. å¯åŠ¨ vLLM æœåŠ¡:")
    print("```bash")
    print("# ä½¿ç”¨å¯åŠ¨è„šæœ¬")
    print("./scripts/start_vllm.sh start-tiny    # å¯åŠ¨å°æ¨¡å‹")
    print("./scripts/start_vllm.sh start-medium  # å¯åŠ¨ä¸­ç­‰æ¨¡å‹")
    print("./scripts/start_vllm.sh status        # æ£€æŸ¥çŠ¶æ€")
    print("```")
    
    print("\n2. æ‰‹åŠ¨å¯åŠ¨:")
    print("```bash")
    print("python -m vllm.entrypoints.openai.api_server \\")
    print("  --model Qwen/Qwen2.5-0.5B-Instruct \\")
    print("  --served-model-name qwen2_5_0_5b \\")
    print("  --dtype float16 \\")
    print("  --max-model-len 4096 \\")
    print("  --gpu-memory-utilization 0.80 \\")
    print("  --port 8001")
    print("```")
    
    print("\n3. å¤š GPU éƒ¨ç½²:")
    print("```bash")
    print("./scripts/start_vllm.sh start-large \\")
    print("  --tensor-parallel 2 \\")
    print("  --gpu-memory 0.90")
    print("```")

def show_testing_guide():
    """å±•ç¤ºæµ‹è¯•æŒ‡å—"""
    print("\n=== æµ‹è¯•æŒ‡å—æ¼”ç¤º ===")
    
    print("\n1. ç‹¬ç«‹æµ‹è¯•:")
    print("```bash")
    print("python test_vllm_standalone.py  # ç‹¬ç«‹ API æµ‹è¯•")
    print("```")
    
    print("\n2. é›†æˆæµ‹è¯•:")
    print("```bash")
    print("python examples/vllm_example.py  # å®Œæ•´é›†æˆç¤ºä¾‹")
    print("```")
    
    print("\n3. æ€§èƒ½æµ‹è¯•:")
    print("```bash")
    print("python benchmark_vllm.py \\")
    print("  --base-url http://127.0.0.1:8001/v1 \\")
    print("  --model qwen2_5_0_5b \\")
    print("  --concurrency 10 \\")
    print("  --requests 100")
    print("```")

def show_migration_checklist():
    """å±•ç¤ºè¿ç§»æ£€æŸ¥æ¸…å•"""
    print("\n=== è¿ç§»æ£€æŸ¥æ¸…å• ===")
    
    checklist = [
        ("âœ…", "åˆ›å»º vLLM Provider", "llm/providers/vllm_openai.py"),
        ("âœ…", "æ³¨å†Œåˆ°å·¥å‚æ¨¡å¼", "llm/factory.py"),
        ("âœ…", "æ›´æ–°é…ç½®æ–‡ä»¶", "config.yaml"),
        ("âœ…", "é›†æˆåˆ° LocalLLM", "llm/local_llm.py"),
        ("âœ…", "åˆ›å»ºå¯åŠ¨è„šæœ¬", "scripts/start_vllm.sh"),
        ("âœ…", "ç¼–å†™æµ‹è¯•è„šæœ¬", "test_vllm_*.py"),
        ("âœ…", "åˆ›å»ºå‹æµ‹å·¥å…·", "benchmark_vllm.py"),
        ("âœ…", "ç¼–å†™ä½¿ç”¨ç¤ºä¾‹", "examples/vllm_example.py"),
        ("âš ï¸", "å¯åŠ¨ vLLM æœåŠ¡", "éœ€è¦ç½‘ç»œè¿æ¥ä¸‹è½½æ¨¡å‹"),
        ("âš ï¸", "æ€§èƒ½éªŒè¯", "éœ€è¦ vLLM æœåŠ¡è¿è¡Œ")
    ]
    
    for status, task, note in checklist:
        print(f"   {status} {task:<20} - {note}")
    
    print("\nğŸ“ æ³¨æ„äº‹é¡¹:")
    print("   - ç¡®ä¿æœ‰è¶³å¤Ÿçš„ GPU å†…å­˜")
    print("   - ç½‘ç»œè¿æ¥ç”¨äºä¸‹è½½æ¨¡å‹")
    print("   - é…ç½®æ–‡ä»¶å¤‡ä»½")
    print("   - æ¸è¿›å¼è¿ç§»æµ‹è¯•")

def main():
    """ä¸»æ¼”ç¤ºå‡½æ•°"""
    print("ğŸš€ Anorag LLM è¿ç§»æ¼”ç¤º")
    print("ä» Ollama åˆ° vLLM çš„å®Œæ•´è¿ç§»æµç¨‹\n")
    
    # å±•ç¤ºå„ä¸ªéƒ¨åˆ†
    show_config_migration()
    show_provider_architecture()
    show_usage_examples()
    show_performance_benefits()
    show_deployment_guide()
    show_testing_guide()
    show_migration_checklist()
    
    print("\nğŸ‰ è¿ç§»æ¼”ç¤ºå®Œæˆï¼")
    print("\nğŸ“š ç›¸å…³æ–‡ä»¶:")
    print("   - llm/providers/vllm_openai.py - vLLM æä¾›å•†å®ç°")
    print("   - llm/factory.py - LLM å·¥å‚æ¨¡å¼")
    print("   - config.yaml - é…ç½®æ–‡ä»¶")
    print("   - scripts/start_vllm.sh - å¯åŠ¨è„šæœ¬")
    print("   - benchmark_vllm.py - æ€§èƒ½æµ‹è¯•")
    print("   - examples/vllm_example.py - ä½¿ç”¨ç¤ºä¾‹")
    
    print("\nğŸ”§ ä¸‹ä¸€æ­¥:")
    print("   1. ç¡®ä¿ç½‘ç»œè¿æ¥")
    print("   2. å¯åŠ¨ vLLM æœåŠ¡")
    print("   3. è¿è¡Œæµ‹è¯•è„šæœ¬")
    print("   4. æ‰§è¡Œæ€§èƒ½å¯¹æ¯”")
    print("   5. ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²")

if __name__ == "__main__":
    main()