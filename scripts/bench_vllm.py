#!/usr/bin/env python3
"""
vLLM å‹åŠ›æµ‹è¯•è„šæœ¬

æ”¯æŒç¯å¢ƒå˜é‡é…ç½®ï¼Œç»Ÿè®¡å»¶è¿Ÿå¹¶è¾“å‡º JSON ç»“æœ
"""

import os
import sys
import json
import time
import asyncio
import statistics
from pathlib import Path
from typing import List, Dict, Any
from concurrent.futures import ThreadPoolExecutor, as_completed

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from openai import OpenAI
from loguru import logger


class VLLMBenchmark:
    """vLLM å‹åŠ›æµ‹è¯•ç±»"""
    
    def __init__(self, base_url: str, model: str, concurrency: int = 64, num_requests: int = 256):
        self.base_url = base_url
        self.model = model
        self.concurrency = concurrency
        self.num_requests = num_requests
        
        # åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
        self.client = OpenAI(
            base_url=base_url,
            api_key="EMPTY",  # vLLM ä¸éœ€è¦çœŸå®çš„ API key
            timeout=60
        )
        
        # æµ‹è¯•æ¶ˆæ¯
        self.test_messages = [
            [{"role": "user", "content": "è¯·ç®€è¦ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½çš„å‘å±•å†å²ã€‚"}],
            [{"role": "user", "content": "è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"}],
            [{"role": "user", "content": "Python å’Œ Java æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ"}],
            [{"role": "user", "content": "å¦‚ä½•ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢æ€§èƒ½ï¼Ÿ"}],
            [{"role": "user", "content": "è¯·ä»‹ç»ä¸€ä¸‹äº‘è®¡ç®—çš„ä¸»è¦ç‰¹ç‚¹ã€‚"}],
        ]
    
    def single_request(self, request_id: int) -> Dict[str, Any]:
        """æ‰§è¡Œå•ä¸ªè¯·æ±‚
        
        Args:
            request_id: è¯·æ±‚ID
            
        Returns:
            dict: è¯·æ±‚ç»“æœ
        """
        result = {
            'request_id': request_id,
            'success': False,
            'latency': None,
            'tokens': 0,
            'error': None
        }
        
        try:
            # éšæœºé€‰æ‹©æµ‹è¯•æ¶ˆæ¯
            messages = self.test_messages[request_id % len(self.test_messages)]
            
            start_time = time.time()
            
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=0.0,
                max_tokens=256,
                top_p=1.0
            )
            
            end_time = time.time()
            
            result['success'] = True
            result['latency'] = (end_time - start_time) * 1000  # ms
            
            if response.usage:
                result['tokens'] = response.usage.completion_tokens or 0
            
            # è®°å½•å“åº”å†…å®¹é•¿åº¦ï¼ˆç”¨äºéªŒè¯ï¼‰
            if response.choices and response.choices[0].message.content:
                result['content_length'] = len(response.choices[0].message.content)
            
        except Exception as e:
            result['error'] = str(e)
            result['latency'] = (time.time() - start_time) * 1000 if 'start_time' in locals() else 0
        
        return result
    
    def run_benchmark(self) -> Dict[str, Any]:
        """è¿è¡Œå‹åŠ›æµ‹è¯•
        
        Returns:
            dict: æµ‹è¯•ç»“æœ
        """
        print(f"å¼€å§‹å‹åŠ›æµ‹è¯•...")
        print(f"ç›®æ ‡: {self.base_url}")
        print(f"æ¨¡å‹: {self.model}")
        print(f"å¹¶å‘æ•°: {self.concurrency}")
        print(f"è¯·æ±‚æ•°: {self.num_requests}")
        print("=" * 50)
        
        start_time = time.time()
        results = []
        
        # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œå¹¶å‘è¯·æ±‚
        with ThreadPoolExecutor(max_workers=self.concurrency) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_id = {
                executor.submit(self.single_request, i): i 
                for i in range(self.num_requests)
            }
            
            # æ”¶é›†ç»“æœ
            completed = 0
            for future in as_completed(future_to_id):
                result = future.result()
                results.append(result)
                completed += 1
                
                # è¿›åº¦æ˜¾ç¤º
                if completed % 10 == 0 or completed == self.num_requests:
                    progress = (completed / self.num_requests) * 100
                    print(f"è¿›åº¦: {completed}/{self.num_requests} ({progress:.1f}%)")
        
        end_time = time.time()
        total_time = end_time - start_time
        
        # ç»Ÿè®¡ç»“æœ
        return self._analyze_results(results, total_time)
    
    def _analyze_results(self, results: List[Dict[str, Any]], total_time: float) -> Dict[str, Any]:
        """åˆ†ææµ‹è¯•ç»“æœ
        
        Args:
            results: æ‰€æœ‰è¯·æ±‚çš„ç»“æœ
            total_time: æ€»è€—æ—¶
            
        Returns:
            dict: åˆ†æç»“æœ
        """
        successful_results = [r for r in results if r['success']]
        failed_results = [r for r in results if not r['success']]
        
        success_count = len(successful_results)
        failure_count = len(failed_results)
        
        # å»¶è¿Ÿç»Ÿè®¡ï¼ˆåªç»Ÿè®¡æˆåŠŸçš„è¯·æ±‚ï¼‰
        latencies = [r['latency'] for r in successful_results if r['latency'] is not None]
        
        latency_stats = {}
        if latencies:
            latencies.sort()
            latency_stats = {
                'min': round(min(latencies), 2),
                'max': round(max(latencies), 2),
                'mean': round(statistics.mean(latencies), 2),
                'median': round(statistics.median(latencies), 2),
                'p95': round(latencies[int(len(latencies) * 0.95)], 2),
                'p99': round(latencies[int(len(latencies) * 0.99)], 2),
            }
        
        # Token ç»Ÿè®¡
        total_tokens = sum(r.get('tokens', 0) for r in successful_results)
        
        # é”™è¯¯ç»Ÿè®¡
        error_types = {}
        for result in failed_results:
            error = result.get('error', 'Unknown')
            error_types[error] = error_types.get(error, 0) + 1
        
        analysis = {
            'benchmark_config': {
                'base_url': self.base_url,
                'model': self.model,
                'concurrency': self.concurrency,
                'num_requests': self.num_requests
            },
            'summary': {
                'total_requests': len(results),
                'successful_requests': success_count,
                'failed_requests': failure_count,
                'success_rate': round((success_count / len(results)) * 100, 2),
                'total_time_seconds': round(total_time, 2),
                'requests_per_second': round(len(results) / total_time, 2),
                'successful_rps': round(success_count / total_time, 2)
            },
            'latency_ms': latency_stats,
            'tokens': {
                'total_completion_tokens': total_tokens,
                'avg_tokens_per_request': round(total_tokens / success_count, 2) if success_count > 0 else 0
            },
            'errors': error_types
        }
        
        return analysis


def main():
    """ä¸»å‡½æ•°"""
    # ä»ç¯å¢ƒå˜é‡è·å–é…ç½®
    base_url = os.getenv('VLLM_BASE', 'http://127.0.0.1:8002/v1')
    model = os.getenv('VLLM_MODEL', 'gpt_oss_20b')
    concurrency = int(os.getenv('CONC', '64'))
    num_requests = int(os.getenv('NREQ', '256'))
    
    # åˆ›å»ºå¹¶è¿è¡ŒåŸºå‡†æµ‹è¯•
    benchmark = VLLMBenchmark(
        base_url=base_url,
        model=model,
        concurrency=concurrency,
        num_requests=num_requests
    )
    
    try:
        results = benchmark.run_benchmark()
        
        # è¾“å‡ºç»“æœ
        print("\n" + "=" * 50)
        print("æµ‹è¯•å®Œæˆï¼")
        print("\nğŸ“Š ç»“æœæ‘˜è¦:")
        print(f"æˆåŠŸç‡: {results['summary']['success_rate']}%")
        print(f"æ€» RPS: {results['summary']['requests_per_second']}")
        print(f"æˆåŠŸ RPS: {results['summary']['successful_rps']}")
        
        if results['latency_ms']:
            print(f"\nâ±ï¸  å»¶è¿Ÿç»Ÿè®¡ (ms):")
            print(f"å¹³å‡: {results['latency_ms']['mean']}")
            print(f"ä¸­ä½æ•°: {results['latency_ms']['median']}")
            print(f"P95: {results['latency_ms']['p95']}")
            print(f"P99: {results['latency_ms']['p99']}")
        
        if results['errors']:
            print(f"\nâŒ é”™è¯¯ç»Ÿè®¡:")
            for error, count in results['errors'].items():
                print(f"  {error}: {count}")
        
        # è¾“å‡ºå®Œæ•´ JSON ç»“æœ
        print("\n" + "=" * 50)
        print("å®Œæ•´ JSON ç»“æœ:")
        print(json.dumps(results, indent=2, ensure_ascii=False))
        
    except KeyboardInterrupt:
        print("\næµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        print(f"\næµ‹è¯•å¤±è´¥: {e}")
        sys.exit(1)


if __name__ == '__main__':
    main()