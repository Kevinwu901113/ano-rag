# RAG System Configuration

# Document Processing
document:
  chunk_size: 128
  overlap: 50
  batch_size: 32
  supported_formats: ["json", "jsonl", "docx"]
  
# Embedding Model
embedding:
  model_name: "BAAI/bge-m3"  # or "nomic-ai/nomic-embed-text-v1"
  batch_size: 64
  max_length: 512
  device: "cuda"
  normalize: true
  
# Clustering
clustering:
  algorithm: "hdbscan"  # or "kmeans", "dbscan"
  min_cluster_size: 5
  min_samples: 3
  metric: "euclidean"
  use_gpu: true
  
# Graph Construction
graph:
  k_hop: 3
  edge_types:
    - "reference"
    - "entity_coexistence"
    - "context_relation"
    - "topic_link"
  centrality_weight: 0.3
  path_value_weight: 0.2
  similarity_threshold: 0.4
  entity_cooccurrence_threshold: 0.5
  context_window: 3
  batch_size: 64
  max_relations_per_note: 20
  weights:
    reference: 1.0
    entity_coexistence: 0.9
    context_relation: 0.8
    topic_relation: 0.7
    semantic_similarity: 0.7
    personal_relation: 0.9

# Multi-hop Reasoning
multi_hop:
  enabled: true
  max_reasoning_hops: 3
  max_paths: 10
  min_path_score: 0.3
  min_path_score_floor: 0.1
  min_path_score_step: 0.05
  path_diversity_threshold: 0.7
  relation_types:
    causal:
      weight: 1.2
      confidence_threshold: 0.7
    temporal:
      weight: 1.1
      confidence_threshold: 0.6
    definition:
      weight: 1.3
      confidence_threshold: 0.8
  llm_relation_extraction:
    enabled: true
    use_fast_model: true
    batch_size: 32
    max_pairs_per_batch: 100
    candidate_selection_threshold: 0.2
    fast_model_name: ./models/distiluse-base-multilingual-cased
  topic_group_llm:
    enabled: true
    min_group_size: 3
    max_notes: 5
  
# Query Processing
query:
  rewrite_enabled: false
  split_multi_queries: true
  placeholder_split: false
  add_prior_knowledge: false  # switch for LLM hallucination prevention
  parallel_processing: true
  
# Context Scheduler Weights (Legacy)
context_scheduler:
  semantic_weight: 0.15      # t1
  graph_weight: 0.35        # t2  
  topic_weight: 0.2         # t3
  feedback_weight: 0.15     # t4
  redundancy_penalty: 0.15   # t5
  top_n_notes: 10

# Context Dispatcher (New Structure-Enhanced Approach)
context_dispatcher:
  enabled: true              # Enable new structure-enhanced context dispatcher
  
  # Stage 1: Semantic Recall Parameters
  semantic_top_n: 10         # n: top-n semantic recall
  semantic_threshold: 0.25    # minimum similarity threshold
  
  # Stage 2: Graph Expansion Parameters  
  graph_expand_top_p: 5     # p: top-p notes for graph expansion (p < n)
  k_hop: 3                   # k: k-hop graph expansion
  graph_threshold: 0.25       # minimum graph score threshold
  
  # Stage 3: Context Scheduling Parameters
  final_semantic_count: 3    # x: final semantic results to select
  final_graph_count: 5      # y: final graph results to select

# LLM Configuration
llm:
  # Provider selection: "ollama", "openai"
  provider: lmstudio
  
  # For atomic note generation and query rewriting (legacy support)
  local_model:
    temperature: 0.1
    max_tokens: 4096
    
  # Ollama Configuration
  ollama:
    base_url: "http://localhost:11434"
    model: "gpt-oss:latest"
    temperature: 0.7
    max_tokens: 4096
    # LightRAG-inspired configuration
    num_ctx: 32768          # Context window size
    max_async: 32            # Max concurrent requests
    timeout: 60             # Request timeout in seconds
    
  # OpenAI API Configuration
  openai:
    api_key: "sk-a19bf5fe08374888877667458bfa0f59"
    base_url: "https://api.deepseek.com/v1"          # Optional: for OpenAI-compatible APIs
    model: "deepseek-chat"
    temperature: 0.7
    max_tokens: 4096
    timeout: 60
    max_retries: 3
    
  # LM Studio Configuration
  lmstudio:
    # 单实例配置（默认配置）
    port: 1234                                        # LM Studio默认端口
    base_url: "http://localhost:1234/v1"             # LM Studio API地址
    model: "gpt-oss-20b"                           # 模型名称，需要与LM Studio中加载的模型匹配
    temperature: 0.7
    max_tokens: 8192
    timeout: 60
    
    # 多实例配置（优化版：自动检测LM Studio中的模型实例）
    multiple_instances:
      enabled: true           # 是否启用多实例负载均衡（支持自动检测）
      # 注意：启用后会自动检测LM Studio中加载的所有模型，
      # 相同模型会自动添加序号后缀（如 model:2, model:3）
      # 所有实例使用相同端口1234，通过模型名称区分
      instances:
        # 以下配置仅在自动检测失败时作为备用配置
        - port: 1234
          base_url: "http://localhost:1234/v1"
          model: "default-model"
        # 备用实例配置（可选）
        # - port: 1234
        #   base_url: "http://localhost:1234/v1"
        #   model: "another-model"
      load_balancing: "round_robin"  # 负载均衡策略: round_robin, random, least_busy
      health_check_interval: 30      # 健康检查间隔（秒）
      max_retries_per_instance: 2    # 每个实例的最大重试次数
    
# Vector Store
vector_store:
  index_type: "Flat"  # faiss index types: Flat, IVFFlat, IVFPQ, HNSW, LSH
  dimension: 1024
  similarity_metric: "cosine"
  top_k: 50
  similarity_threshold: 0.1  # 降低相似度阈值以增加召回
  recall_optimization:
    enabled: true
  
# Storage Paths
storage:
  vector_db_path: null  # 将在运行时设置为工作目录下的子目录
  graph_db_path: null   # 将在运行时设置为工作目录下的子目录
  processed_docs_path: null  # 将在运行时设置为工作目录下的子目录
  cache_path: null      # 将在运行时设置为工作目录下的子目录
  source_docs_dir: "./data"  # 源文档目录保持不变
  result_root: "./result"
  work_dir: null
  
# Evaluation
eval:
  datasets_path: null  # 将在运行时设置为工作目录下的子目录
  metrics: ["precision", "recall", "f1", "bleu", "rouge"]
  batch_size: 16
  

# Summary Auditor Configuration
summary_auditor:
  enabled: false                    # 是否启用摘要校验器
  llm_check_ratio: 0.2            # 控制最多多少比例的数据可触发LLM校验
  ner_model: "spacy"              # 设置使用的NER工具，可选值为spacy或ltp
  entity_similarity_threshold: 0.8 # 实体相似度阈值
  missing_entity_threshold: 0.3    # 缺失实体比例阈值
  
# Performance
performance:
  use_gpu: true
  use_cudf: true
  num_workers: 8
  cache_enabled: true
