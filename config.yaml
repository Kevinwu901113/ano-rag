# RAG System Configuration

# Document Processing
document:
  chunk_size: 128
  overlap: 50
  batch_size: 32
  supported_formats: ["json", "jsonl", "docx"]
  
# Embedding Model
embedding:
  model_name: "BAAI/bge-m3"  # or "nomic-ai/nomic-embed-text-v1"
  batch_size: 64
  max_length: 512
  device: "cuda"
  normalize: true
  
# Clustering
clustering:
  algorithm: "hdbscan"  # or "kmeans", "dbscan"
  min_cluster_size: 5
  min_samples: 3
  metric: "euclidean"
  use_gpu: true
  
# Graph Construction
graph:
  k_hop: 3
  edge_types:
    - "reference"
    - "entity_coexistence"
    - "context_relation"
    - "topic_link"
  centrality_weight: 0.3
  path_value_weight: 0.2
  similarity_threshold: 0.4
  entity_cooccurrence_threshold: 0.5
  context_window: 3
  batch_size: 64
  max_relations_per_note: 20
  weights:
    reference: 1.0
    entity_coexistence: 0.9
    context_relation: 0.8
    topic_relation: 0.7
    semantic_similarity: 0.7
    personal_relation: 0.9

# Multi-hop Reasoning
multi_hop:
  enabled: true
  max_reasoning_hops: 3
  max_paths: 10
  min_path_score: 0.3
  min_path_score_floor: 0.1
  min_path_score_step: 0.05
  path_diversity_threshold: 0.7
  relation_types:
    causal:
      weight: 1.2
      confidence_threshold: 0.7
    temporal:
      weight: 1.1
      confidence_threshold: 0.6
    definition:
      weight: 1.3
      confidence_threshold: 0.8
  llm_relation_extraction:
    enabled: true
    use_fast_model: true
    batch_size: 32
    max_pairs_per_batch: 100
    candidate_selection_threshold: 0.2
    fast_model_name: ./models/distiluse-base-multilingual-cased
  topic_group_llm:
    enabled: true
    min_group_size: 3
    max_notes: 5
  
# Query Processing
query:
  rewrite_enabled: true
  split_multi_queries: true
  placeholder_split: false
  add_prior_knowledge: false  # switch for LLM hallucination prevention
  parallel_processing: true
  
# Context Scheduler Weights (Legacy)
context_scheduler:
  semantic_weight: 0.15      # t1
  graph_weight: 0.35        # t2  
  topic_weight: 0.2         # t3
  feedback_weight: 0.15     # t4
  redundancy_penalty: 0.15   # t5
  top_n_notes: 10

# Context Dispatcher (New Structure-Enhanced Approach)
context_dispatcher:
  enabled: true              # Enable new structure-enhanced context dispatcher
  
  # Stage 1: Semantic Recall Parameters
  semantic_top_n: 10         # n: top-n semantic recall
  semantic_threshold: 0.15    # minimum similarity threshold
  
  # Stage 2: Graph Expansion Parameters  
  graph_expand_top_p: 5     # p: top-p notes for graph expansion (p < n)
  k_hop: 4                   # k: k-hop graph expansion
  graph_threshold: 0.15       # minimum graph score threshold
  
  # Stage 3: Context Scheduling Parameters
  final_semantic_count: 5    # x: final semantic results to select
  final_graph_count: 10       # y: final graph results to select

# LLM Configuration
llm:
  # Provider selection: "vllm_openai", "ollama", "openai"
  provider: vllm_openai
  
  # vLLM Routes Configuration
  routes:
    gpt20_a:
      base_url: http://127.0.0.1:8002/v1
      model: gpt_oss_20b
      timeout: 60
    # 可选的第二个实例用于负载均衡
    # gpt20_b:
    #   base_url: http://127.0.0.1:8202/v1
    #   model: gpt_oss_20b
    #   timeout: 60
    # 备用较小模型路由
    tiny_qwen:
      base_url: http://127.0.0.1:8001/v1
      model: qwen2_5_0_5b
      timeout: 60
  
  # Default route to use
  default_route: gpt20_a
  
  # Fallback route when default fails
  fallback_route: null
  
  # Load balancing policy: round_robin | least_latency
  lb_policy: round_robin
  
  # Global parameters for all routes
  params:
    temperature: 0.0
    max_tokens: 1024
    top_p: 1.0
    timeout: 60
    max_retries: 3
  
  # For atomic note generation and query rewriting (legacy support)
  local_model:
    provider: "ollama"  # Switch between "ollama" and "openai"
    temperature: 0.1
    max_tokens: 4096
    
  # Ollama Configuration
  ollama:
    # 单实例配置（向后兼容）
    base_url: "http://localhost:11434"
    model: "gpt-oss:latest"
    temperature: 0.7
    max_tokens: 4096
    # LightRAG-inspired configuration
    num_ctx: 32768          # Context window size
    max_async: 8            # Max concurrent requests
    timeout: 60             # Request timeout in seconds
    
    # 多实例配置（用于提升并行效率）
    # 注意：当前使用GPU优化端口11440-11441，避免与系统ollama服务冲突
    multiple_instances:
      enabled: false          # 是否启用多实例负载均衡
      instances:
        - base_url: "http://localhost:11440"
          model: "gpt-oss:latest"
        - base_url: "http://localhost:11441"
          model: "gpt-oss:latest"
        # 备用实例（如需要更多实例时启用）
        # - base_url: "http://localhost:11442"
        #   model: "gpt-oss:latest"
        # - base_url: "http://localhost:11443"
        #   model: "gpt-oss:latest"
      load_balancing: "round_robin"  # 负载均衡策略: round_robin, random, least_busy
      health_check_interval: 30      # 健康检查间隔（秒）
      max_retries_per_instance: 2    # 每个实例的最大重试次数
    
  # OpenAI API Configuration
  openai:
    api_key: "sk-a19bf5fe08374888877667458bfa0f59"
    base_url: "https://api.deepseek.com/v1"          # Optional: for OpenAI-compatible APIs
    model: "deepseek-chat"
    temperature: 0.7
    max_tokens: 4096
    timeout: 60
    max_retries: 3
    
# Vector Store
vector_store:
  index_type: "Flat"  # faiss index types: Flat, IVFFlat, IVFPQ, HNSW, LSH
  dimension: 1024
  similarity_metric: "cosine"
  top_k: 50
  similarity_threshold: 0.1  # 降低相似度阈值以增加召回
  recall_optimization:
    enabled: true
  
# Storage Paths
storage:
  vector_db_path: null  # 将在运行时设置为工作目录下的子目录
  graph_db_path: null   # 将在运行时设置为工作目录下的子目录
  processed_docs_path: null  # 将在运行时设置为工作目录下的子目录
  cache_path: null      # 将在运行时设置为工作目录下的子目录
  source_docs_dir: "./data"  # 源文档目录保持不变
  result_root: "./result"
  work_dir: null
  
# Evaluation
eval:
  datasets_path: null  # 将在运行时设置为工作目录下的子目录
  metrics: ["precision", "recall", "f1", "bleu", "rouge"]
  batch_size: 16
  

# Summary Auditor Configuration
summary_auditor:
  enabled: false                    # 是否启用摘要校验器
  llm_check_ratio: 0.2            # 控制最多多少比例的数据可触发LLM校验
  ner_model: "spacy"              # 设置使用的NER工具，可选值为spacy或ltp
  entity_similarity_threshold: 0.8 # 实体相似度阈值
  missing_entity_threshold: 0.3    # 缺失实体比例阈值
  
# Performance
performance:
  use_gpu: true
  use_cudf: true
  num_workers: 8
  cache_enabled: true
