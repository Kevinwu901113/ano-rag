#!/usr/bin/env python3
"""
vLLM Provider ä½¿ç”¨ç¤ºä¾‹

å±•ç¤ºå¦‚ä½•åœ¨ anorag ä¸­ä½¿ç”¨ vLLM provider
"""

import asyncio
import time
from loguru import logger

# å¯¼å…¥ anorag ç»„ä»¶
from llm.factory import LLMFactory
from llm.providers.vllm_openai import VLLMOpenAIProvider
from config import config


def example_direct_usage():
    """ç›´æ¥ä½¿ç”¨ vLLM Provider çš„ç¤ºä¾‹"""
    print("\n=== ç›´æ¥ä½¿ç”¨ vLLM Provider ===")
    
    # ç›´æ¥åˆ›å»º vLLM provider
    provider = VLLMOpenAIProvider(
        base_url="http://127.0.0.1:8001/v1",
        model="qwen2_5_0_5b",
        api_key="EMPTY",
        temperature=0.1,
        max_tokens=512
    )
    
    # æ£€æŸ¥å¯ç”¨æ€§
    if not provider.is_available():
        print("âŒ vLLM æœåŠ¡ä¸å¯ç”¨ï¼Œè¯·å…ˆå¯åŠ¨æœåŠ¡")
        return False
    
    print("âœ… vLLM æœåŠ¡å¯ç”¨")
    
    # 1. ç®€å•æ–‡æœ¬ç”Ÿæˆ
    print("\n1. ç®€å•æ–‡æœ¬ç”Ÿæˆ:")
    response = provider.generate("è¯·ç®€å•ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½çš„å‘å±•å†ç¨‹ã€‚")
    print(f"å›å¤: {response}")
    
    # 2. èŠå¤©å¯¹è¯
    print("\n2. èŠå¤©å¯¹è¯:")
    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ã€‚"},
        {"role": "user", "content": "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"}
    ]
    response = provider.chat(messages)
    print(f"å›å¤: {response}")
    
    # 3. æµå¼è¾“å‡º
    print("\n3. æµå¼è¾“å‡º:")
    messages = [
        {"role": "user", "content": "è¯·å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„çŸ­è¯—ã€‚"}
    ]
    print("æµå¼å›å¤: ", end="")
    for chunk in provider.stream(messages):
        print(chunk, end="", flush=True)
    print("\n")
    
    return True


def example_factory_usage():
    """ä½¿ç”¨å·¥å‚æ¨¡å¼çš„ç¤ºä¾‹"""
    print("\n=== ä½¿ç”¨å·¥å‚æ¨¡å¼ ===")
    
    try:
        # é€šè¿‡å·¥å‚åˆ›å»º provider
        provider = LLMFactory.create_provider('vllm_openai')
        
        # ä½¿ç”¨é…ç½®ä¸­çš„è·¯ç”±
        provider_with_route = LLMFactory.create_provider('vllm_openai', route_name='tiny_qwen')
        
        print(f"âœ… æˆåŠŸåˆ›å»º vLLM provider")
        print(f"æ¨¡å‹ä¿¡æ¯: {provider.get_model_info()}")
        
        # æµ‹è¯•ç”Ÿæˆ
        response = provider.generate("è¯·ç”¨ä¸€å¥è¯æ€»ç»“æ·±åº¦å­¦ä¹ çš„æ ¸å¿ƒæ€æƒ³ã€‚")
        print(f"å›å¤: {response}")
        
        return True
        
    except Exception as e:
        print(f"âŒ å·¥å‚æ¨¡å¼åˆ›å»ºå¤±è´¥: {e}")
        return False


async def example_async_usage():
    """å¼‚æ­¥ä½¿ç”¨ç¤ºä¾‹"""
    print("\n=== å¼‚æ­¥ä½¿ç”¨ç¤ºä¾‹ ===")
    
    provider = VLLMOpenAIProvider(
        base_url="http://127.0.0.1:8001/v1",
        model="qwen2_5_0_5b",
        api_key="EMPTY"
    )
    
    if not provider.is_available():
        print("âŒ vLLM æœåŠ¡ä¸å¯ç”¨")
        return False
    
    # å¹¶å‘è¯·æ±‚æµ‹è¯•
    questions = [
        "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
        "æœºå™¨å­¦ä¹ æœ‰å“ªäº›ç±»å‹ï¼Ÿ",
        "æ·±åº¦å­¦ä¹ çš„ä¼˜åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ",
        "è‡ªç„¶è¯­è¨€å¤„ç†çš„åº”ç”¨æœ‰å“ªäº›ï¼Ÿ",
        "è®¡ç®—æœºè§†è§‰æŠ€æœ¯å¦‚ä½•å·¥ä½œï¼Ÿ"
    ]
    
    print(f"å¹¶å‘å¤„ç† {len(questions)} ä¸ªé—®é¢˜...")
    start_time = time.time()
    
    # å¹¶å‘æ‰§è¡Œ
    tasks = [provider.async_generate(q) for q in questions]
    responses = await asyncio.gather(*tasks)
    
    end_time = time.time()
    
    print(f"æ€»è€—æ—¶: {end_time - start_time:.2f}ç§’")
    print(f"å¹³å‡è€—æ—¶: {(end_time - start_time) / len(questions):.2f}ç§’/é—®é¢˜")
    
    for i, (question, response) in enumerate(zip(questions, responses), 1):
        print(f"\né—®é¢˜ {i}: {question}")
        print(f"å›ç­” {i}: {response[:100]}..." if len(response) > 100 else f"å›ç­” {i}: {response}")
    
    return True


def example_integration_with_anorag():
    """ä¸ anorag ç³»ç»Ÿé›†æˆçš„ç¤ºä¾‹"""
    print("\n=== ä¸ anorag ç³»ç»Ÿé›†æˆ ===")
    
    try:
        # æ¨¡æ‹ŸåŸå­ç¬”è®°ç”Ÿæˆ
        from llm.atomic_note_generator import AtomicNoteGenerator
        
        # åˆ›å»ºåŸå­ç¬”è®°ç”Ÿæˆå™¨ï¼Œä½¿ç”¨ vLLM
        # æ³¨æ„ï¼šè¿™éœ€è¦ä¿®æ”¹ AtomicNoteGenerator ä»¥æ”¯æŒå·¥å‚æ¨¡å¼
        print("æ¨¡æ‹ŸåŸå­ç¬”è®°ç”Ÿæˆ...")
        
        provider = LLMFactory.create_provider('vllm_openai')
        
        # ç¤ºä¾‹æ–‡æ¡£
        document = """
        äººå·¥æ™ºèƒ½ï¼ˆArtificial Intelligenceï¼ŒAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œ
        å®ƒä¼å›¾äº†è§£æ™ºèƒ½çš„å®è´¨ï¼Œå¹¶ç”Ÿäº§å‡ºä¸€ç§æ–°çš„èƒ½ä»¥äººç±»æ™ºèƒ½ç›¸ä¼¼çš„æ–¹å¼åšå‡ºååº”çš„æ™ºèƒ½æœºå™¨ã€‚
        è¯¥é¢†åŸŸçš„ç ”ç©¶åŒ…æ‹¬æœºå™¨äººã€è¯­è¨€è¯†åˆ«ã€å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œä¸“å®¶ç³»ç»Ÿç­‰ã€‚
        """
        
        # ç”ŸæˆåŸå­ç¬”è®°
        prompt = f"""è¯·å°†ä»¥ä¸‹æ–‡æ¡£åˆ†è§£ä¸ºåŸå­ç¬”è®°ï¼Œæ¯ä¸ªç¬”è®°åº”è¯¥åŒ…å«ä¸€ä¸ªç‹¬ç«‹çš„æ¦‚å¿µæˆ–äº‹å®ï¼š
        
        æ–‡æ¡£ï¼š{document}
        
        è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
        - title: ç¬”è®°æ ‡é¢˜
        - content: ç¬”è®°å†…å®¹
        - keywords: å…³é”®è¯åˆ—è¡¨
        """
        
        response = provider.generate(prompt, max_tokens=1024)
        print(f"åŸå­ç¬”è®°ç”Ÿæˆç»“æœ:\n{response}")
        
        return True
        
    except Exception as e:
        print(f"âŒ é›†æˆæµ‹è¯•å¤±è´¥: {e}")
        return False


def example_performance_comparison():
    """æ€§èƒ½å¯¹æ¯”ç¤ºä¾‹"""
    print("\n=== æ€§èƒ½å¯¹æ¯”ç¤ºä¾‹ ===")
    
    # è·å–å¯ç”¨çš„ providers
    available_providers = LLMFactory.get_available_providers()
    print(f"å¯ç”¨çš„ providers: {available_providers}")
    
    test_prompt = "è¯·ç®€å•è§£é‡Šä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ã€‚"
    results = {}
    
    for provider_name, is_available in available_providers.items():
        if not is_available:
            print(f"â­ï¸  è·³è¿‡ä¸å¯ç”¨çš„ provider: {provider_name}")
            continue
        
        try:
            print(f"\næµ‹è¯• {provider_name}...")
            provider = LLMFactory.create_provider(provider_name)
            
            start_time = time.time()
            response = provider.generate(test_prompt)
            end_time = time.time()
            
            results[provider_name] = {
                'latency': end_time - start_time,
                'response_length': len(response),
                'response': response[:100] + '...' if len(response) > 100 else response
            }
            
            print(f"âœ… {provider_name} - å»¶è¿Ÿ: {results[provider_name]['latency']:.2f}s")
            
        except Exception as e:
            print(f"âŒ {provider_name} æµ‹è¯•å¤±è´¥: {e}")
            results[provider_name] = {'error': str(e)}
    
    # è¾“å‡ºå¯¹æ¯”ç»“æœ
    print("\nğŸ“Š æ€§èƒ½å¯¹æ¯”ç»“æœ:")
    print("-" * 60)
    for provider_name, result in results.items():
        if 'error' in result:
            print(f"{provider_name:<15} âŒ {result['error']}")
        else:
            print(f"{provider_name:<15} â±ï¸  {result['latency']:.2f}s  ğŸ“ {result['response_length']} chars")
    
    return True


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ vLLM Provider ä½¿ç”¨ç¤ºä¾‹")
    print("=" * 50)
    
    # æ£€æŸ¥é…ç½®
    print(f"å½“å‰é…ç½®çš„ provider: {config.get('llm.provider', 'not set')}")
    print(f"é»˜è®¤è·¯ç”±: {config.get('llm.default_route', 'not set')}")
    
    results = []
    
    # 1. ç›´æ¥ä½¿ç”¨ç¤ºä¾‹
    results.append(("ç›´æ¥ä½¿ç”¨", example_direct_usage()))
    
    # 2. å·¥å‚æ¨¡å¼ç¤ºä¾‹
    results.append(("å·¥å‚æ¨¡å¼", example_factory_usage()))
    
    # 3. å¼‚æ­¥ä½¿ç”¨ç¤ºä¾‹
    results.append(("å¼‚æ­¥ä½¿ç”¨", asyncio.run(example_async_usage())))
    
    # 4. ç³»ç»Ÿé›†æˆç¤ºä¾‹
    results.append(("ç³»ç»Ÿé›†æˆ", example_integration_with_anorag()))
    
    # 5. æ€§èƒ½å¯¹æ¯”ç¤ºä¾‹
    results.append(("æ€§èƒ½å¯¹æ¯”", example_performance_comparison()))
    
    # è¾“å‡ºæ€»ç»“
    print("\n" + "=" * 50)
    print("ğŸ“‹ ç¤ºä¾‹è¿è¡Œç»“æœ:")
    for name, success in results:
        status = "âœ… æˆåŠŸ" if success else "âŒ å¤±è´¥"
        print(f"  {name:<12} {status}")
    
    successful_count = sum(results[i][1] for i in range(len(results)))
    print(f"\næ€»ä½“ç»“æœ: {successful_count}/{len(results)} ä¸ªç¤ºä¾‹æˆåŠŸ")
    
    if successful_count == len(results):
        print("ğŸ‰ æ‰€æœ‰ç¤ºä¾‹è¿è¡ŒæˆåŠŸï¼vLLM é›†æˆå®Œæˆ")
    else:
        print("âš ï¸  éƒ¨åˆ†ç¤ºä¾‹å¤±è´¥ï¼Œè¯·æ£€æŸ¥ vLLM æœåŠ¡çŠ¶æ€")
        print("\nğŸ’¡ å¯åŠ¨ vLLM æœåŠ¡:")
        print("./scripts/start_vllm.sh start-tiny")
        print("\nğŸ’¡ æµ‹è¯•æœåŠ¡:")
        print("./scripts/start_vllm.sh test")


if __name__ == "__main__":
    main()