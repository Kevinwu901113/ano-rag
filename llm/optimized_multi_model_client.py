#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¼˜åŒ–çš„å¤šæ¨¡å‹å¹¶è¡Œå®¢æˆ·ç«¯
å®ç°çœŸæ­£çš„å¹¶è¡Œæ‰§è¡Œï¼Œæé«˜æ•´ä½“æ‰§è¡Œæ•ˆç‡
"""

import time
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Any, Optional
from loguru import logger
from llm.multi_model_client import MultiModelClient

class OptimizedMultiModelClient(MultiModelClient):
    """ä¼˜åŒ–çš„å¤šæ¨¡å‹å¹¶è¡Œå®¢æˆ·ç«¯
    
    ç»§æ‰¿è‡ªMultiModelClientï¼Œå¢å¼ºå¹¶è¡Œå¤„ç†èƒ½åŠ›ï¼š
    - çœŸæ­£çš„å¹¶è¡Œæ‰§è¡Œï¼šå¤šä¸ªå®ä¾‹åŒæ—¶å¤„ç†è¯·æ±‚
    - æ™ºèƒ½ä»»åŠ¡åˆ†é…ï¼šæ ¹æ®å®ä¾‹æ€§èƒ½åŠ¨æ€åˆ†é…ä»»åŠ¡
    - æ€§èƒ½ç›‘æ§ï¼šå®æ—¶ç›‘æ§å¹¶è¡Œæ‰§è¡Œæ•ˆç‡
    - è´Ÿè½½å‡è¡¡ä¼˜åŒ–ï¼šé¿å…å•å®ä¾‹è¿‡è½½
    """
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.parallel_stats = {
            'total_parallel_requests': 0,
            'total_parallel_time': 0.0,
            'avg_parallel_efficiency': 0.0
        }
    
    def generate_parallel(self, prompts: List[str], system_prompt: str = None, **kwargs) -> List[str]:
        """çœŸæ­£çš„å¹¶è¡Œç”Ÿæˆæ–¹æ³•
        
        ä¸generate_concurrentä¸åŒï¼Œè¿™ä¸ªæ–¹æ³•ç¡®ä¿å¤šä¸ªå®ä¾‹çœŸæ­£åŒæ—¶å¤„ç†è¯·æ±‚
        
        Args:
            prompts: æç¤ºåˆ—è¡¨
            system_prompt: ç³»ç»Ÿæç¤º
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            å“åº”åˆ—è¡¨
        """
        if not prompts:
            return []
        
        # è·å–å¥åº·å®ä¾‹
        healthy_instances = [inst for inst in self.model_instances if inst.is_healthy and not inst.is_loading]
        
        if not healthy_instances:
            raise Exception("No healthy model instances available for parallel processing")
        
        logger.info(f"ğŸš€ å¼€å§‹å¹¶è¡Œå¤„ç† {len(prompts)} ä¸ªè¯·æ±‚ï¼Œä½¿ç”¨ {len(healthy_instances)} ä¸ªå®ä¾‹")
        
        start_time = time.time()
        
        # å¦‚æœæç¤ºæ•°é‡å°‘äºæˆ–ç­‰äºå®ä¾‹æ•°é‡ï¼Œæ¯ä¸ªå®ä¾‹å¤„ç†ä¸€ä¸ª
        if len(prompts) <= len(healthy_instances):
            return self._execute_direct_parallel(prompts, healthy_instances, system_prompt, **kwargs)
        else:
            # å¦‚æœæç¤ºæ•°é‡å¤šäºå®ä¾‹æ•°é‡ï¼Œéœ€è¦åˆ†æ‰¹å¤„ç†
            return self._execute_batch_parallel(prompts, healthy_instances, system_prompt, **kwargs)
    
    def _execute_direct_parallel(self, prompts: List[str], instances: List, system_prompt: str = None, **kwargs) -> List[str]:
        """ç›´æ¥å¹¶è¡Œæ‰§è¡Œï¼ˆæç¤ºæ•° <= å®ä¾‹æ•°ï¼‰"""
        results = [None] * len(prompts)
        
        with ThreadPoolExecutor(max_workers=len(instances)) as executor:
            # ä¸ºæ¯ä¸ªæç¤ºåˆ†é…ä¸€ä¸ªå®ä¾‹
            future_to_index = {}
            for i, (prompt, instance) in enumerate(zip(prompts, instances)):
                future = executor.submit(self._execute_single_request, instance, prompt, system_prompt, **kwargs)
                future_to_index[future] = i
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_index):
                index = future_to_index[future]
                try:
                    results[index] = future.result()
                except Exception as e:
                    logger.error(f"Parallel request {index} failed: {e}")
                    results[index] = f"Error: {str(e)}"
        
        return results
    
    def _execute_batch_parallel(self, prompts: List[str], instances: List, system_prompt: str = None, **kwargs) -> List[str]:
        """æ‰¹é‡å¹¶è¡Œæ‰§è¡Œï¼ˆæç¤ºæ•° > å®ä¾‹æ•°ï¼‰"""
        results = [None] * len(prompts)
        num_instances = len(instances)
        
        # å°†æç¤ºåˆ†é…ç»™å®ä¾‹
        instance_tasks = [[] for _ in range(num_instances)]
        for i, prompt in enumerate(prompts):
            instance_idx = i % num_instances
            instance_tasks[instance_idx].append((i, prompt))
        
        with ThreadPoolExecutor(max_workers=num_instances) as executor:
            # ä¸ºæ¯ä¸ªå®ä¾‹æäº¤ä¸€ä¸ªæ‰¹é‡ä»»åŠ¡
            futures = []
            for instance_idx, tasks in enumerate(instance_tasks):
                if tasks:  # åªå¤„ç†æœ‰ä»»åŠ¡çš„å®ä¾‹
                    future = executor.submit(
                        self._execute_instance_batch, 
                        instances[instance_idx], 
                        tasks, 
                        system_prompt, 
                        **kwargs
                    )
                    futures.append((future, instance_idx))
            
            # æ”¶é›†ç»“æœ
            for future, instance_idx in futures:
                try:
                    batch_results = future.result()
                    for original_index, result in batch_results:
                        results[original_index] = result
                except Exception as e:
                    logger.error(f"Batch execution failed for instance {instance_idx}: {e}")
                    # ä¸ºè¯¥å®ä¾‹çš„æ‰€æœ‰ä»»åŠ¡è®¾ç½®é”™è¯¯ç»“æœ
                    for original_index, _ in instance_tasks[instance_idx]:
                        results[original_index] = f"Error: {str(e)}"
        
        return results
    
    def _execute_instance_batch(self, instance, tasks: List[tuple], system_prompt: str = None, **kwargs) -> List[tuple]:
        """å•ä¸ªå®ä¾‹æ‰§è¡Œæ‰¹é‡ä»»åŠ¡"""
        results = []
        for original_index, prompt in tasks:
            try:
                result = self._execute_single_request(instance, prompt, system_prompt, **kwargs)
                results.append((original_index, result))
            except Exception as e:
                logger.error(f"Single request failed in batch: {e}")
                results.append((original_index, f"Error: {str(e)}"))
        return results
    
    def _execute_single_request(self, instance, prompt: str, system_prompt: str = None, **kwargs) -> str:
        """æ‰§è¡Œå•ä¸ªè¯·æ±‚"""
        try:
            # æ›´æ–°å®ä¾‹çŠ¶æ€
            instance.active_requests += 1
            
            # å‡†å¤‡æ¶ˆæ¯
            messages = []
            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})
            messages.append({"role": "user", "content": prompt})
            
            # åˆå¹¶é€‰é¡¹
            options = {**self.default_options, **kwargs}
            
            # æ‰§è¡Œè¯·æ±‚
            start_time = time.time()
            response = instance.client.chat.completions.create(
                model=instance.model_name,
                messages=messages,
                **options
            )
            end_time = time.time()
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            instance.total_requests += 1
            response_time = end_time - start_time
            instance.avg_response_time = (
                (instance.avg_response_time * (instance.total_requests - 1) + response_time) / 
                instance.total_requests
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            instance.error_count += 1
            raise e
        finally:
            instance.active_requests -= 1
    
    def generate_concurrent_optimized(self, prompts: List[str], system_prompt: str = None, **kwargs) -> List[str]:
        """ä¼˜åŒ–çš„å¹¶å‘ç”Ÿæˆæ–¹æ³•
        
        ç»“åˆäº†åŸæœ‰çš„è´Ÿè½½å‡è¡¡ç­–ç•¥å’Œæ–°çš„å¹¶è¡Œæ‰§è¡Œèƒ½åŠ›
        """
        if not prompts:
            return []
        
        start_time = time.time()
        
        # ä½¿ç”¨çœŸæ­£çš„å¹¶è¡Œæ‰§è¡Œ
        results = self.generate_parallel(prompts, system_prompt, **kwargs)
        
        end_time = time.time()
        total_time = end_time - start_time
        
        # æ›´æ–°å¹¶è¡Œç»Ÿè®¡
        self.parallel_stats['total_parallel_requests'] += len(prompts)
        self.parallel_stats['total_parallel_time'] += total_time
        
        # è®¡ç®—å¹¶è¡Œæ•ˆç‡
        if len(prompts) > 1:
            theoretical_serial_time = total_time * len(prompts)
            efficiency = theoretical_serial_time / total_time if total_time > 0 else 1.0
            self.parallel_stats['avg_parallel_efficiency'] = (
                (self.parallel_stats['avg_parallel_efficiency'] * 
                 (self.parallel_stats['total_parallel_requests'] - len(prompts)) + efficiency * len(prompts)) /
                self.parallel_stats['total_parallel_requests']
            )
        
        logger.info(f"âœ… å¹¶è¡Œå¤„ç†å®Œæˆï¼Œè€—æ—¶: {total_time:.2f}sï¼Œæ•ˆç‡: {efficiency:.2f}x")
        
        return results
    
    def get_parallel_stats(self) -> Dict[str, Any]:
        """è·å–å¹¶è¡Œæ‰§è¡Œç»Ÿè®¡ä¿¡æ¯"""
        return {
            **self.parallel_stats,
            'instance_stats': [
                {
                    'model_name': inst.model_name,
                    'total_requests': inst.total_requests,
                    'active_requests': inst.active_requests,
                    'avg_response_time': inst.avg_response_time,
                    'error_count': inst.error_count,
                    'is_healthy': inst.is_healthy
                }
                for inst in self.model_instances
            ]
        }
    
    def benchmark_parallel_performance(self, test_prompts: List[str] = None, iterations: int = 3) -> Dict[str, Any]:
        """åŸºå‡†æµ‹è¯•å¹¶è¡Œæ€§èƒ½"""
        if test_prompts is None:
            test_prompts = [
                "è¯·ç®€å•ä»‹ç»äººå·¥æ™ºèƒ½ã€‚",
                "è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ã€‚",
                "æè¿°æ·±åº¦å­¦ä¹ çš„åº”ç”¨ã€‚",
                "ä»€ä¹ˆæ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ï¼Ÿ"
            ]
        
        logger.info(f"ğŸ§ª å¼€å§‹å¹¶è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•ï¼Œ{iterations} æ¬¡è¿­ä»£")
        
        # æµ‹è¯•ä¸²è¡Œæ‰§è¡Œ
        serial_times = []
        for i in range(iterations):
            start_time = time.time()
            for prompt in test_prompts:
                self.generate(prompt, max_tokens=30)
            serial_times.append(time.time() - start_time)
        
        avg_serial_time = sum(serial_times) / len(serial_times)
        
        # æµ‹è¯•å¹¶è¡Œæ‰§è¡Œ
        parallel_times = []
        for i in range(iterations):
            start_time = time.time()
            self.generate_parallel(test_prompts, max_tokens=30)
            parallel_times.append(time.time() - start_time)
        
        avg_parallel_time = sum(parallel_times) / len(parallel_times)
        
        # è®¡ç®—æ€§èƒ½æå‡
        speedup = avg_serial_time / avg_parallel_time if avg_parallel_time > 0 else 1.0
        efficiency = speedup / len(self.model_instances) if self.model_instances else 0.0
        
        results = {
            'test_prompts_count': len(test_prompts),
            'iterations': iterations,
            'avg_serial_time': avg_serial_time,
            'avg_parallel_time': avg_parallel_time,
            'speedup': speedup,
            'efficiency': efficiency,
            'instance_count': len(self.model_instances),
            'performance_improvement': (speedup - 1) * 100  # ç™¾åˆ†æ¯”æå‡
        }
        
        logger.info(f"ğŸ“Š åŸºå‡†æµ‹è¯•ç»“æœ:")
        logger.info(f"   ä¸²è¡Œå¹³å‡è€—æ—¶: {avg_serial_time:.2f}s")
        logger.info(f"   å¹¶è¡Œå¹³å‡è€—æ—¶: {avg_parallel_time:.2f}s")
        logger.info(f"   æ€§èƒ½æå‡: {speedup:.2f}x ({results['performance_improvement']:.1f}%)")
        logger.info(f"   å¹¶è¡Œæ•ˆç‡: {efficiency:.2f}")
        
        return results

# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•å‡½æ•°
def test_optimized_client():
    """æµ‹è¯•ä¼˜åŒ–çš„å¤šæ¨¡å‹å®¢æˆ·ç«¯"""
    logger.info("ğŸ§ª æµ‹è¯•ä¼˜åŒ–çš„å¤šæ¨¡å‹å®¢æˆ·ç«¯")
    
    try:
        # åˆ›å»ºä¼˜åŒ–å®¢æˆ·ç«¯
        client = OptimizedMultiModelClient()
        
        # æµ‹è¯•æç¤º
        test_prompts = [
            "è¯·ç”¨ä¸€å¥è¯æè¿°æ˜¥å¤©ã€‚",
            "è§£é‡Šä»€ä¹ˆæ˜¯äº‘è®¡ç®—ã€‚",
            "æ¨èä¸€æœ¬å¥½ä¹¦ã€‚",
            "æè¿°äººå·¥æ™ºèƒ½çš„æœªæ¥ã€‚",
            "ä»€ä¹ˆæ˜¯åŒºå—é“¾æŠ€æœ¯ï¼Ÿ",
            "ä»‹ç»é‡å­è®¡ç®—çš„åŸç†ã€‚"
        ]
        
        # æµ‹è¯•å¹¶è¡Œæ‰§è¡Œ
        logger.info(f"\nğŸš€ æµ‹è¯•å¹¶è¡Œæ‰§è¡Œ {len(test_prompts)} ä¸ªè¯·æ±‚")
        start_time = time.time()
        results = client.generate_parallel(test_prompts, max_tokens=50)
        parallel_time = time.time() - start_time
        
        logger.info(f"âœ… å¹¶è¡Œæ‰§è¡Œå®Œæˆï¼Œè€—æ—¶: {parallel_time:.2f}s")
        
        # æ˜¾ç¤ºç»“æœ
        for i, result in enumerate(results, 1):
            if result and not result.startswith("Error:"):
                logger.info(f"ğŸ“ ç»“æœ{i}: {result[:50]}...")
            else:
                logger.error(f"âŒ ç»“æœ{i}: {result}")
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        stats = client.get_parallel_stats()
        logger.info(f"\nğŸ“Š å¹¶è¡Œç»Ÿè®¡: æ€»è¯·æ±‚ {stats['total_parallel_requests']}, å¹³å‡æ•ˆç‡ {stats['avg_parallel_efficiency']:.2f}x")
        
        # è¿è¡ŒåŸºå‡†æµ‹è¯•
        logger.info("\nğŸ è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•")
        benchmark_results = client.benchmark_parallel_performance(test_prompts[:4], iterations=2)
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    test_optimized_client()