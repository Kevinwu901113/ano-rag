import json
import random
import time
import threading
from typing import Any, Dict, Iterator, List, Union, Optional, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass, field
from enum import Enum
import subprocess
import psutil
try:
    import GPUtil  # type: ignore
except ModuleNotFoundError:  # pragma: no cover - fallback when optional dep missing
    class _GPUtilFallback:
        """Minimal fallback GPUtil implementation returning no GPUs."""

        @staticmethod
        def getGPUs():  # noqa: D401 - simple stub method
            """Return an empty list when GPUtil is unavailable."""

            return []

    GPUtil = _GPUtilFallback()  # type: ignore

import openai
import requests
from loguru import logger

from config import config
from .prompts import (
    FINAL_ANSWER_SYSTEM_PROMPT,
    FINAL_ANSWER_PROMPT,
    EVALUATE_ANSWER_SYSTEM_PROMPT,
    EVALUATE_ANSWER_PROMPT,
)


class LoadBalancingStrategy(Enum):
    """è´Ÿè½½å‡è¡¡ç­–ç•¥æšä¸¾"""
    ROUND_ROBIN = "round_robin"
    RANDOM = "random"
    LEAST_BUSY = "least_busy"
    GPU_OPTIMAL = "gpu_optimal"  # åŸºäºGPUä½¿ç”¨ç‡çš„ä¼˜åŒ–ç­–ç•¥
    PERFORMANCE_BASED = "performance_based"  # åŸºäºæ€§èƒ½æŒ‡æ ‡çš„ç­–ç•¥


@dataclass
class ModelInstance:
    """æ¨¡å‹å®ä¾‹é…ç½®"""
    model_name: str
    port: int
    base_url: str
    gpu_id: Optional[int] = None
    process: Optional[subprocess.Popen] = None
    client: Optional[openai.OpenAI] = None
    is_healthy: bool = False
    is_loading: bool = False
    last_health_check: float = 0
    active_requests: int = 0
    total_requests: int = 0
    error_count: int = 0
    avg_response_time: float = 0.0
    gpu_memory_usage: float = 0.0
    
    def __post_init__(self):
        if self.client is None and not self.is_loading:
            self._init_client()
    
    def _init_client(self):
        """åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯"""
        try:
            client_kwargs = {
                "api_key": "lm-studio",
                "base_url": self.base_url,
                "timeout": config.get("llm.multi_model.timeout", 60),
                "max_retries": config.get("llm.multi_model.max_retries", 3),
            }
            self.client = openai.OpenAI(**client_kwargs)
            logger.info(f"Initialized client for model {self.model_name} on port {self.port}")
        except Exception as e:
            logger.error(f"Failed to initialize client for {self.model_name}: {e}")
            self.is_healthy = False


class GPUResourceManager:
    """GPUèµ„æºç®¡ç†å™¨"""
    
    def __init__(self):
        self.gpu_info = self._get_gpu_info()
        self.allocated_memory = {gpu_id: 0 for gpu_id in range(len(self.gpu_info))}
        self._lock = threading.Lock()
    
    def _get_gpu_info(self) -> List[Dict[str, Any]]:
        """è·å–GPUä¿¡æ¯"""
        try:
            gpus = GPUtil.getGPUs()
            return [
                {
                    "id": gpu.id,
                    "name": gpu.name,
                    "memory_total": gpu.memoryTotal,
                    "memory_free": gpu.memoryFree,
                    "memory_used": gpu.memoryUsed,
                    "load": gpu.load,
                }
                for gpu in gpus
            ]
        except Exception as e:
            logger.warning(f"Failed to get GPU info: {e}")
            return []
    
    def allocate_gpu(self, memory_required: float = 4096) -> Optional[int]:
        """åˆ†é…GPUèµ„æº"""
        with self._lock:
            for gpu in self.gpu_info:
                gpu_id = gpu["id"]
                available_memory = gpu["memory_total"] - self.allocated_memory[gpu_id]
                if available_memory >= memory_required:
                    self.allocated_memory[gpu_id] += memory_required
                    logger.info(f"Allocated {memory_required}MB on GPU {gpu_id}")
                    return gpu_id
            return None
    
    def release_gpu(self, gpu_id: int, memory_amount: float):
        """é‡Šæ”¾GPUèµ„æº"""
        with self._lock:
            if gpu_id in self.allocated_memory:
                self.allocated_memory[gpu_id] = max(0, self.allocated_memory[gpu_id] - memory_amount)
                logger.info(f"Released {memory_amount}MB from GPU {gpu_id}")
    
    def get_gpu_usage(self) -> Dict[int, Dict[str, float]]:
        """è·å–GPUä½¿ç”¨æƒ…å†µ"""
        try:
            gpus = GPUtil.getGPUs()
            return {
                gpu.id: {
                    "memory_used": gpu.memoryUsed,
                    "memory_total": gpu.memoryTotal,
                    "load": gpu.load,
                    "allocated": self.allocated_memory.get(gpu.id, 0)
                }
                for gpu in gpus
            }
        except Exception as e:
            logger.warning(f"Failed to get GPU usage: {e}")
            return {}


class MultiModelClient:
    """å¤šæ¨¡å‹å¹¶è¡Œå®¢æˆ·ç«¯
    
    æ”¯æŒåŒæ—¶åŠ è½½å’Œè¿è¡Œå¤šä¸ªç‹¬ç«‹çš„æ¨¡å‹å®ä¾‹ï¼š
    - å¤šæ¨¡å‹å®ä¾‹ï¼šæ¯ä¸ªæ¨¡å‹è¿è¡Œåœ¨ç‹¬ç«‹çš„ç«¯å£å’Œè¿›ç¨‹ä¸­
    - GPUèµ„æºç®¡ç†ï¼šæ™ºèƒ½åˆ†é…GPUèµ„æºç»™ä¸åŒæ¨¡å‹
    - è´Ÿè½½å‡è¡¡ï¼šæ ¹æ®æ¨¡å‹æ€§èƒ½å’ŒGPUä½¿ç”¨æƒ…å†µåˆ†å‘è¯·æ±‚
    - å¥åº·ç›‘æ§ï¼šç›‘æ§æ¯ä¸ªæ¨¡å‹å®ä¾‹çš„å¥åº·çŠ¶æ€å’Œæ€§èƒ½
    
    æä¾›ç»Ÿä¸€çš„APIæ¥å£ï¼Œæ”¯æŒé«˜æ•ˆçš„å¹¶å‘å¤„ç†ã€‚
    """

    def __init__(self):
        # åŸºç¡€é…ç½®
        self.temperature = config.get("llm.multi_model.temperature", 0.7)
        self.max_tokens = config.get("llm.multi_model.max_tokens", 4096)
        self.timeout = config.get("llm.multi_model.timeout", 60)
        self.max_retries = config.get("llm.multi_model.max_retries", 3)
        
        # å¤šæ¨¡å‹é…ç½®
        self.model_instances: List[ModelInstance] = []
        self.gpu_manager = GPUResourceManager()
        
        # è´Ÿè½½å‡è¡¡é…ç½®
        strategy_str = config.get("llm.multi_model.load_balancing", "gpu_optimal")
        self.load_balancing_strategy = LoadBalancingStrategy(strategy_str)
        self._round_robin_index = 0
        self._lock = threading.Lock()
        
        # å¥åº·æ£€æŸ¥é…ç½®
        self.health_check_interval = config.get("llm.multi_model.health_check_interval", 30)
        self.health_check_thread = None
        
        # é»˜è®¤é€‰é¡¹
        self.default_options = {
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
        }
        
        # åˆå§‹åŒ–æ¨¡å‹å®ä¾‹
        self._init_model_instances()
        
        # å¯åŠ¨å¥åº·æ£€æŸ¥
        self._start_health_check_thread()
        
        logger.info(f"Initialized MultiModelClient with {len(self.model_instances)} model instances")
    
    def _init_model_instances(self):
        """åˆå§‹åŒ–å¤šä¸ªæ¨¡å‹å®ä¾‹
        
        åˆ©ç”¨LM Studioçš„è‡ªåŠ¨å‘½åæœºåˆ¶ï¼š
        - é‡å¤åŠ è½½ç›¸åŒæ¨¡å‹æ—¶ï¼ŒLM Studioä¼šè‡ªåŠ¨ä¸ºæ¨¡å‹åˆ†é…ä¸åŒåç§°
        - ä¾‹å¦‚ï¼šgpt-oss-20b, gpt-oss-20b:2, gpt-oss-20b:3 ç­‰
        - æ‰€æœ‰å®ä¾‹å…±äº«åŒä¸€ä¸ªç«¯å£ï¼ˆ1234ï¼‰ï¼Œé€šè¿‡æ¨¡å‹åç§°åŒºåˆ†
        """
        models_config = config.get("llm.multi_model.instances", [])
        base_port = config.get("llm.multi_model.base_port", 1234)
        base_url = f"http://localhost:{base_port}/v1"
        
        if not models_config:
            # å¦‚æœæ²¡æœ‰é…ç½®ï¼Œåˆ›å»ºé»˜è®¤çš„å¤šå®ä¾‹é…ç½®
            model_count = config.get("llm.multi_model.instance_count", 2)
            base_model_name = config.get("llm.multi_model.model_name", "gpt-oss-20b")
            
            for i in range(model_count):
                # ç¬¬ä¸€ä¸ªå®ä¾‹ä½¿ç”¨åŸå§‹æ¨¡å‹åï¼Œåç»­å®ä¾‹ä½¿ç”¨LM Studioçš„è‡ªåŠ¨å‘½å
                if i == 0:
                    model_name = base_model_name
                else:
                    model_name = f"{base_model_name}:{i+1}"
                
                instance = ModelInstance(
                    model_name=model_name,
                    port=base_port,  # æ‰€æœ‰å®ä¾‹ä½¿ç”¨åŒä¸€ä¸ªç«¯å£
                    base_url=base_url,
                    gpu_id=self.gpu_manager.allocate_gpu()
                )
                self.model_instances.append(instance)
        else:
            # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å®ä¾‹é…ç½®
            for model_config in models_config:
                port = model_config.get("port", base_port)
                instance = ModelInstance(
                    model_name=model_config["model_name"],
                    port=port,
                    base_url=model_config.get("base_url", f"http://localhost:{port}/v1"),
                    gpu_id=model_config.get("gpu_id") or self.gpu_manager.allocate_gpu()
                )
                self.model_instances.append(instance)
        
        # æ£€æŸ¥æ¨¡å‹å®ä¾‹æ˜¯å¦å·²åœ¨LM Studioä¸­åŠ è½½
        self._check_and_load_model_instances()
    
    def _check_and_load_model_instances(self):
        """æ£€æŸ¥å¹¶åŠ è½½æ¨¡å‹å®ä¾‹
        
        æ£€æŸ¥LM Studioä¸­æ˜¯å¦å·²åŠ è½½æ‰€éœ€çš„æ¨¡å‹å®ä¾‹ï¼š
        1. ç›´æ¥æµ‹è¯•æ¯ä¸ªé…ç½®çš„æ¨¡å‹åç§°æ˜¯å¦å¯ç”¨
        2. å¦‚æœæ¨¡å‹å¯ç”¨ï¼Œç›´æ¥åˆå§‹åŒ–å®¢æˆ·ç«¯
        3. å¦‚æœæ¨¡å‹ä¸å¯ç”¨ï¼Œæ ‡è®°ä¸ºæœªåŠ è½½
        """
        try:
            # è·å–å½“å‰LM Studioä¸­å·²åŠ è½½çš„æ¨¡å‹åˆ—è¡¨ï¼ˆä»…ç”¨äºæ—¥å¿—ï¼‰
            available_models = self._get_available_models()
            logger.info(f"Available models in LM Studio: {available_models}")
            
            for instance in self.model_instances:
                # ç›´æ¥æµ‹è¯•æ¨¡å‹æ˜¯å¦å¯ç”¨ï¼Œè€Œä¸ä¾èµ–äº/v1/modelsæ¥å£çš„è¿”å›
                if self._test_model_availability(instance.model_name):
                    logger.info(f"Model {instance.model_name} is available and responding")
                    instance._init_client()
                    instance.is_healthy = True
                else:
                    logger.warning(f"Model {instance.model_name} is not available or not responding")
                    instance.is_healthy = False
            
            # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„å¥åº·å®ä¾‹
            healthy_count = sum(1 for inst in self.model_instances if inst.is_healthy)
            if healthy_count == 0:
                logger.error("No healthy model instances available. Please load models in LM Studio manually.")
                self._print_loading_instructions()
            elif healthy_count < len(self.model_instances):
                logger.warning(f"Only {healthy_count}/{len(self.model_instances)} model instances are healthy")
                self._print_loading_instructions()
            else:
                logger.info(f"All {healthy_count} model instances are ready")
                
        except Exception as e:
            logger.error(f"Failed to check model instances: {e}")
            # å¦‚æœæ— æ³•è¿æ¥åˆ°LM Studioï¼Œå°è¯•åˆå§‹åŒ–æ‰€æœ‰å®ä¾‹
            for instance in self.model_instances:
                instance._init_client()
    
    def _get_available_models(self) -> List[str]:
        """è·å–LM Studioä¸­å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨"""
        try:
            base_port = config.get("llm.multi_model.base_port", 1234)
            response = requests.get(f"http://localhost:{base_port}/v1/models", timeout=10)
            if response.status_code == 200:
                models_data = response.json()
                models = [model["id"] for model in models_data.get("data", [])]
                logger.info(f"åœ¨ç«¯å£ {base_port} æ‰¾åˆ° {len(models)} ä¸ªæ¨¡å‹: {models}")
                return models
            else:
                logger.warning(f"Failed to get models list: HTTP {response.status_code}")
                return []
        except Exception as e:
            logger.warning(f"Failed to connect to LM Studio: {e}")
            return []
    
    def _test_model_availability(self, model_name: str) -> bool:
        """ç›´æ¥æµ‹è¯•æ¨¡å‹æ˜¯å¦å¯ç”¨
        
        é€šè¿‡å‘é€ä¸€ä¸ªç®€å•çš„è¯·æ±‚æ¥æµ‹è¯•æ¨¡å‹æ˜¯å¦çœŸæ­£å¯ç”¨
        """
        try:
            base_port = config.get("llm.multi_model.base_port", 1234)
            url = f"http://localhost:{base_port}/v1/chat/completions"
            
            payload = {
                "model": model_name,
                "messages": [{"role": "user", "content": "test"}],
                "max_tokens": 1,
                "temperature": 0
            }
            
            response = requests.post(url, json=payload, timeout=10)
            
            if response.status_code == 200:
                logger.debug(f"Model {model_name} test successful")
                return True
            else:
                logger.debug(f"Model {model_name} test failed with status {response.status_code}")
                return False
                
        except Exception as e:
            logger.debug(f"Model {model_name} test failed with exception: {e}")
            return False
    
    def _print_loading_instructions(self):
        """æ‰“å°æ¨¡å‹åŠ è½½æŒ‡å¯¼"""
        base_model_name = config.get("llm.multi_model.model_name", "gpt-oss-20b")
        model_count = len(self.model_instances)
        
        logger.info("\n" + "="*60)
        logger.info("å¤šæ¨¡å‹åŠ è½½æŒ‡å¯¼:")
        logger.info(f"éœ€è¦å¯åŠ¨ {model_count} ä¸ªç‹¬ç«‹çš„LM Studioå®ä¾‹")
        logger.info("æ¯ä¸ªå®ä¾‹è¿è¡Œåœ¨ä¸åŒçš„ç«¯å£ä¸Š:")
        for i, instance in enumerate(self.model_instances):
            status = "âœ“" if instance.is_healthy else "âœ—"
            logger.info(f"  {status} å®ä¾‹ {i+1}: ç«¯å£ {instance.port} - æ¨¡å‹ {instance.model_name}")
        logger.info("\nåŠ è½½æ­¥éª¤:")
        logger.info("1. å¯åŠ¨ç¬¬ä¸€ä¸ªLM Studioå®ä¾‹ (é»˜è®¤ç«¯å£1234)")
        logger.info(f"   - åŠ è½½æ¨¡å‹: {base_model_name}")
        for i in range(1, model_count):
            port = self.model_instances[i].port
            logger.info(f"2. å¯åŠ¨ç¬¬{i+1}ä¸ªLM Studioå®ä¾‹ (ç«¯å£{port})")
            logger.info(f"   - åœ¨LM Studioè®¾ç½®ä¸­ä¿®æ”¹æœåŠ¡å™¨ç«¯å£ä¸º {port}")
            logger.info(f"   - åŠ è½½æ¨¡å‹: {base_model_name}")
        logger.info(f"{model_count+1}. ç¡®ä¿æ‰€æœ‰æ¨¡å‹å®ä¾‹éƒ½å·²å¯åŠ¨å¹¶å¯è®¿é—®")
        logger.info(f"{model_count+2}. é‡æ–°è¿è¡Œç¨‹åº")
        logger.info("="*60 + "\n")
    

    
    def _start_health_check_thread(self):
        """å¯åŠ¨å¥åº·æ£€æŸ¥çº¿ç¨‹"""
        if self.health_check_thread is None or not self.health_check_thread.is_alive():
            self.health_check_thread = threading.Thread(
                target=self._health_check_loop,
                daemon=True
            )
            self.health_check_thread.start()
            logger.info("Health check thread started")
    
    def _health_check_loop(self):
        """å¥åº·æ£€æŸ¥å¾ªç¯"""
        while True:
            try:
                self._perform_health_checks()
                time.sleep(self.health_check_interval)
            except Exception as e:
                logger.error(f"Health check error: {e}")
                time.sleep(5)
    
    def _perform_health_checks(self):
        """æ‰§è¡Œå¥åº·æ£€æŸ¥"""
        current_time = time.time()
        
        for instance in self.model_instances:
            if instance.is_loading:
                continue
                
            try:
                # æ£€æŸ¥æ¨¡å‹å®ä¾‹æ˜¯å¦å“åº”
                start_time = time.time()
                response = requests.get(f"{instance.base_url}/models", timeout=10)
                response_time = time.time() - start_time
                
                if response.status_code == 200:
                    instance.is_healthy = True
                    # æ›´æ–°å¹³å‡å“åº”æ—¶é—´
                    if instance.avg_response_time == 0:
                        instance.avg_response_time = response_time
                    else:
                        instance.avg_response_time = (instance.avg_response_time * 0.8 + response_time * 0.2)
                else:
                    instance.is_healthy = False
                    
            except Exception as e:
                instance.is_healthy = False
                logger.warning(f"Health check failed for {instance.model_name}: {e}")
            
            instance.last_health_check = current_time
            
            # æ›´æ–°GPUä½¿ç”¨æƒ…å†µ
            if instance.gpu_id is not None:
                gpu_usage = self.gpu_manager.get_gpu_usage()
                if instance.gpu_id in gpu_usage:
                    instance.gpu_memory_usage = gpu_usage[instance.gpu_id]["memory_used"]
    
    def _select_instance(self) -> Optional[ModelInstance]:
        """æ ¹æ®è´Ÿè½½å‡è¡¡ç­–ç•¥é€‰æ‹©å®ä¾‹"""
        healthy_instances = [inst for inst in self.model_instances if inst.is_healthy and not inst.is_loading]
        
        if not healthy_instances:
            logger.warning("No healthy model instances available")
            return None
        
        if self.load_balancing_strategy == LoadBalancingStrategy.ROUND_ROBIN:
            with self._lock:
                instance = healthy_instances[self._round_robin_index % len(healthy_instances)]
                self._round_robin_index += 1
                return instance
        
        elif self.load_balancing_strategy == LoadBalancingStrategy.RANDOM:
            return random.choice(healthy_instances)
        
        elif self.load_balancing_strategy == LoadBalancingStrategy.LEAST_BUSY:
            return min(healthy_instances, key=lambda x: x.active_requests)
        
        elif self.load_balancing_strategy == LoadBalancingStrategy.GPU_OPTIMAL:
            # é€‰æ‹©GPUä½¿ç”¨ç‡æœ€ä½çš„å®ä¾‹ï¼Œå¦‚æœç›¸åŒåˆ™é€‰æ‹©æ´»è·ƒè¯·æ±‚æœ€å°‘çš„
            def gpu_score(instance):
                return (instance.gpu_memory_usage, instance.active_requests, instance.total_requests)
            return min(healthy_instances, key=gpu_score)
        
        elif self.load_balancing_strategy == LoadBalancingStrategy.PERFORMANCE_BASED:
            # åŸºäºå“åº”æ—¶é—´å’Œæ´»è·ƒè¯·æ±‚æ•°çš„ç»¼åˆè¯„åˆ†
            def calculate_score(instance):
                response_penalty = instance.avg_response_time * 10
                load_penalty = instance.active_requests * 5
                return response_penalty + load_penalty
            
            return min(healthy_instances, key=calculate_score)
        
        return healthy_instances[0]
    
    def _execute_with_retry(self, func, *args, **kwargs):
        """å¸¦é‡è¯•çš„æ‰§è¡Œå‡½æ•°"""
        last_exception = None
        
        for attempt in range(self.max_retries + 1):
            instance = self._select_instance()
            
            if instance is None:
                raise Exception("No healthy model instances available")
            
            try:
                # æ›´æ–°å®ä¾‹çŠ¶æ€
                instance.active_requests += 1
                instance.total_requests += 1
                
                start_time = time.time()
                result = func(instance, *args, **kwargs)
                response_time = time.time() - start_time
                
                # æ›´æ–°å¹³å‡å“åº”æ—¶é—´
                if instance.avg_response_time == 0:
                    instance.avg_response_time = response_time
                else:
                    instance.avg_response_time = (instance.avg_response_time * 0.9 + response_time * 0.1)
                
                return result
                
            except Exception as e:
                last_exception = e
                instance.error_count += 1
                instance.is_healthy = False  # æ ‡è®°ä¸ºä¸å¥åº·
                logger.warning(f"Request failed on model {instance.model_name} (attempt {attempt + 1}): {e}")
                
                if attempt < self.max_retries:
                    time.sleep(0.5 * (attempt + 1))  # æŒ‡æ•°é€€é¿
            
            finally:
                instance.active_requests = max(0, instance.active_requests - 1)
        
        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
        raise last_exception or Exception("All model instances failed")
    
    def generate(
        self,
        prompt: str,
        system_prompt: str | None = None,
        *,
        stream: bool = False,
        **kwargs: Any,
    ) -> str:
        """ç”Ÿæˆæ–‡æœ¬"""
        def _generate(instance: ModelInstance, prompt: str, system_prompt: str | None, stream: bool, **kwargs):
            messages = []
            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})
            messages.append({"role": "user", "content": prompt})
            
            options = {**self.default_options, **kwargs}
            
            response = instance.client.chat.completions.create(
                model=instance.model_name,
                messages=messages,
                stream=stream,
                **options
            )
            
            if stream:
                return response
            else:
                return response.choices[0].message.content
        
        return self._execute_with_retry(_generate, prompt, system_prompt, stream, **kwargs)
    
    def generate_final_answer(self, prompt: str, **kwargs) -> str:
        """Generate final answer using the multi-model system"""
        return self.generate(prompt, FINAL_ANSWER_SYSTEM_PROMPT, **kwargs)
    
    def evaluate_answer(self, question: str, answer: str, context: str = "") -> dict:
        """Evaluate answer quality using the multi-model system"""
        try:
            eval_prompt = EVALUATE_ANSWER_PROMPT.format(
                query=question,
                answer=answer,
                context=context
            )
            
            response = self.generate(eval_prompt, EVALUATE_ANSWER_SYSTEM_PROMPT)
            
            # Parse the response (assuming it returns JSON format)
            try:
                import json
                return json.loads(response)
            except json.JSONDecodeError:
                # If not JSON, return a basic structure
                return {
                    "score": 0.7,
                    "reasoning": response,
                    "confidence": 0.5
                }
                
        except Exception as e:
            logger.error(f"Answer evaluation failed: {e}")
            return {
                "score": 0.0,
                "reasoning": f"Evaluation failed: {str(e)}",
                "confidence": 0.0
            }
    
    def chat(
        self,
        messages: list[dict[str, str]],
        *,
        stream: bool = False,
        **kwargs: Any,
    ) -> Union[str, Iterator[str]]:
        """å¯¹è¯æ¥å£"""
        def _chat(instance: ModelInstance, messages: list[dict[str, str]], stream: bool, **kwargs):
            options = {**self.default_options, **kwargs}
            
            response = instance.client.chat.completions.create(
                model=instance.model_name,
                messages=messages,
                stream=stream,
                **options
            )
            
            if stream:
                return response
            else:
                return response.choices[0].message.content
        
        return self._execute_with_retry(_chat, messages, stream, **kwargs)
    
    def generate_concurrent(
        self,
        prompts: List[str],
        system_prompt: str | None = None,
        **kwargs: Any,
    ) -> List[str]:
        """å¹¶å‘ç”Ÿæˆå¤šä¸ªæ–‡æœ¬ï¼ˆä½¿ç”¨è´Ÿè½½å‡è¡¡ï¼‰"""
        if not prompts:
            return []
        
        # ä½¿ç”¨æ‰€æœ‰å¥åº·çš„å®ä¾‹è¿›è¡Œå¹¶å‘å¤„ç†
        healthy_instances = [inst for inst in self.model_instances if inst.is_healthy and not inst.is_loading]
        max_workers = min(len(healthy_instances), len(prompts))
        
        if max_workers == 0:
            raise Exception("No healthy model instances available for concurrent processing")
        
        results = [None] * len(prompts)
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤ä»»åŠ¡
            future_to_index = {}
            for i, prompt in enumerate(prompts):
                future = executor.submit(self.generate, prompt, system_prompt, **kwargs)
                future_to_index[future] = i
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_index):
                index = future_to_index[future]
                try:
                    results[index] = future.result()
                except Exception as e:
                    logger.error(f"Concurrent generation failed for prompt {index}: {e}")
                    results[index] = f"Error: {str(e)}"
        
        return results
    
    def generate_parallel(
        self,
        prompts: List[str],
        system_prompt: str | None = None,
        **kwargs: Any,
    ) -> List[str]:
        """çœŸæ­£çš„å¹¶è¡Œç”Ÿæˆå¤šä¸ªæ–‡æœ¬ï¼ˆç›´æ¥åˆ†é…åˆ°å®ä¾‹ï¼‰
        
        ä¸generate_concurrentä¸åŒï¼Œè¿™ä¸ªæ–¹æ³•ç¡®ä¿å¤šä¸ªå®ä¾‹çœŸæ­£åŒæ—¶å¤„ç†è¯·æ±‚ï¼Œ
        è€Œä¸æ˜¯é€šè¿‡è´Ÿè½½å‡è¡¡ç­–ç•¥é€‰æ‹©å•ä¸ªå®ä¾‹ã€‚
        
        Args:
            prompts: æç¤ºåˆ—è¡¨
            system_prompt: ç³»ç»Ÿæç¤º
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            å“åº”åˆ—è¡¨
        """
        if not prompts:
            return []
        
        # è·å–å¥åº·å®ä¾‹
        healthy_instances = [inst for inst in self.model_instances if inst.is_healthy and not inst.is_loading]
        
        if not healthy_instances:
            raise Exception("No healthy model instances available for parallel processing")
        
        logger.info(f"ğŸš€ å¼€å§‹å¹¶è¡Œå¤„ç† {len(prompts)} ä¸ªè¯·æ±‚ï¼Œä½¿ç”¨ {len(healthy_instances)} ä¸ªå®ä¾‹")
        
        # å¦‚æœæç¤ºæ•°é‡å°‘äºæˆ–ç­‰äºå®ä¾‹æ•°é‡ï¼Œæ¯ä¸ªå®ä¾‹å¤„ç†ä¸€ä¸ª
        if len(prompts) <= len(healthy_instances):
            return self._execute_direct_parallel(prompts, healthy_instances, system_prompt, **kwargs)
        else:
            # å¦‚æœæç¤ºæ•°é‡å¤šäºå®ä¾‹æ•°é‡ï¼Œéœ€è¦åˆ†æ‰¹å¤„ç†
            return self._execute_batch_parallel(prompts, healthy_instances, system_prompt, **kwargs)
    
    def _execute_direct_parallel(self, prompts: List[str], instances: List, system_prompt: str = None, **kwargs) -> List[str]:
        """ç›´æ¥å¹¶è¡Œæ‰§è¡Œï¼ˆæç¤ºæ•° <= å®ä¾‹æ•°ï¼‰"""
        results = [None] * len(prompts)
        
        with ThreadPoolExecutor(max_workers=len(instances)) as executor:
            # ä¸ºæ¯ä¸ªæç¤ºåˆ†é…ä¸€ä¸ªå®ä¾‹
            future_to_index = {}
            for i, (prompt, instance) in enumerate(zip(prompts, instances)):
                future = executor.submit(self._execute_single_request, instance, prompt, system_prompt, **kwargs)
                future_to_index[future] = i
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_index):
                index = future_to_index[future]
                try:
                    results[index] = future.result()
                except Exception as e:
                    logger.error(f"Parallel request {index} failed: {e}")
                    results[index] = f"Error: {str(e)}"
        
        return results
    
    def _execute_batch_parallel(self, prompts: List[str], instances: List, system_prompt: str = None, **kwargs) -> List[str]:
        """æ‰¹é‡å¹¶è¡Œæ‰§è¡Œï¼ˆæç¤ºæ•° > å®ä¾‹æ•°ï¼‰"""
        results = [None] * len(prompts)
        num_instances = len(instances)
        
        # å°†æç¤ºåˆ†é…ç»™å®ä¾‹
        instance_tasks = [[] for _ in range(num_instances)]
        for i, prompt in enumerate(prompts):
            instance_idx = i % num_instances
            instance_tasks[instance_idx].append((i, prompt))
        
        with ThreadPoolExecutor(max_workers=num_instances) as executor:
            # ä¸ºæ¯ä¸ªå®ä¾‹æäº¤ä¸€ä¸ªæ‰¹é‡ä»»åŠ¡
            futures = []
            for instance_idx, tasks in enumerate(instance_tasks):
                if tasks:  # åªå¤„ç†æœ‰ä»»åŠ¡çš„å®ä¾‹
                    future = executor.submit(
                        self._execute_instance_batch, 
                        instances[instance_idx], 
                        tasks, 
                        system_prompt, 
                        **kwargs
                    )
                    futures.append((future, instance_idx))
            
            # æ”¶é›†ç»“æœ
            for future, instance_idx in futures:
                try:
                    batch_results = future.result()
                    for original_index, result in batch_results:
                        results[original_index] = result
                except Exception as e:
                    logger.error(f"Batch execution failed for instance {instance_idx}: {e}")
                    # ä¸ºè¯¥å®ä¾‹çš„æ‰€æœ‰ä»»åŠ¡è®¾ç½®é”™è¯¯ç»“æœ
                    for original_index, _ in instance_tasks[instance_idx]:
                        results[original_index] = f"Error: {str(e)}"
        
        return results
    
    def _execute_instance_batch(self, instance, tasks: List[tuple], system_prompt: str = None, **kwargs) -> List[tuple]:
        """å•ä¸ªå®ä¾‹æ‰§è¡Œæ‰¹é‡ä»»åŠ¡"""
        results = []
        for original_index, prompt in tasks:
            try:
                result = self._execute_single_request(instance, prompt, system_prompt, **kwargs)
                results.append((original_index, result))
            except Exception as e:
                logger.error(f"Single request failed in batch: {e}")
                results.append((original_index, f"Error: {str(e)}"))
        return results
    
    def _execute_single_request(self, instance, prompt: str, system_prompt: str = None, **kwargs) -> str:
        """æ‰§è¡Œå•ä¸ªè¯·æ±‚"""
        try:
            # æ›´æ–°å®ä¾‹çŠ¶æ€
            instance.active_requests += 1
            
            # å‡†å¤‡æ¶ˆæ¯
            messages = []
            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})
            messages.append({"role": "user", "content": prompt})
            
            # åˆå¹¶é€‰é¡¹
            options = {**self.default_options, **kwargs}
            
            # æ‰§è¡Œè¯·æ±‚
            start_time = time.time()
            response = instance.client.chat.completions.create(
                model=instance.model_name,
                messages=messages,
                **options
            )
            end_time = time.time()
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            instance.total_requests += 1
            response_time = end_time - start_time
            instance.avg_response_time = (
                (instance.avg_response_time * (instance.total_requests - 1) + response_time) / 
                instance.total_requests
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            instance.error_count += 1
            raise e
        finally:
            instance.active_requests -= 1
    
    def get_status(self) -> Dict[str, Any]:
        """è·å–å¤šæ¨¡å‹ç³»ç»ŸçŠ¶æ€"""
        healthy_count = sum(1 for inst in self.model_instances if inst.is_healthy)
        loading_count = sum(1 for inst in self.model_instances if inst.is_loading)
        total_requests = sum(inst.total_requests for inst in self.model_instances)
        total_errors = sum(inst.error_count for inst in self.model_instances)
        
        return {
            "total_instances": len(self.model_instances),
            "healthy_instances": healthy_count,
            "loading_instances": loading_count,
            "total_requests": total_requests,
            "total_errors": total_errors,
            "error_rate": total_errors / max(total_requests, 1),
            "load_balancing_strategy": self.load_balancing_strategy.value,
            "gpu_usage": self.gpu_manager.get_gpu_usage(),
            "instances": [
                {
                    "model_name": inst.model_name,
                    "port": inst.port,
                    "gpu_id": inst.gpu_id,
                    "is_healthy": inst.is_healthy,
                    "is_loading": inst.is_loading,
                    "active_requests": inst.active_requests,
                    "total_requests": inst.total_requests,
                    "error_count": inst.error_count,
                    "avg_response_time": inst.avg_response_time,
                    "gpu_memory_usage": inst.gpu_memory_usage,
                }
                for inst in self.model_instances
            ]
        }
    
    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return {
            'provider': 'multi_model',
            'instance_count': len(self.model_instances),
            'healthy_instances': sum(1 for inst in self.model_instances if inst.is_healthy),
            'load_balancing_strategy': self.load_balancing_strategy.value,
            'temperature': self.temperature,
            'max_tokens': self.max_tokens,
            'gpu_info': self.gpu_manager.gpu_info,
            'instances': [{
                'model_name': inst.model_name,
                'port': inst.port,
                'gpu_id': inst.gpu_id,
                'is_healthy': inst.is_healthy,
                'is_loading': inst.is_loading,
            } for inst in self.model_instances]
        }
    
    def shutdown(self):
        """å…³é—­å¤šæ¨¡å‹ç³»ç»Ÿ"""
        logger.info("Shutting down MultiModelClient...")
        
        # åœæ­¢å¥åº·æ£€æŸ¥çº¿ç¨‹
        if self.health_check_thread and self.health_check_thread.is_alive():
            # è¿™é‡Œéœ€è¦å®ç°çº¿ç¨‹åœæ­¢æœºåˆ¶
            pass
        
        # åœæ­¢æ‰€æœ‰æ¨¡å‹å®ä¾‹
        for instance in self.model_instances:
            try:
                if instance.process and instance.process.poll() is None:
                    instance.process.terminate()
                    instance.process.wait(timeout=10)
                
                # é‡Šæ”¾GPUèµ„æº
                if instance.gpu_id is not None:
                    self.gpu_manager.release_gpu(instance.gpu_id, 4096)  # å‡è®¾æ¯ä¸ªæ¨¡å‹å ç”¨4GB
                    
            except Exception as e:
                logger.error(f"Error shutting down instance {instance.model_name}: {e}")
        
        logger.info("MultiModelClient shutdown complete")


class TaskClassifier:
    """ä»»åŠ¡åˆ†ç±»å™¨ï¼Œç”¨äºæ™ºèƒ½åˆ¤æ–­æŸ¥è¯¢ç±»å‹"""
    
    def __init__(self):
        # é‡é‡çº§ä»»åŠ¡å…³é”®è¯
        self.heavy_keywords = {
            "åˆ†æ", "åˆ†æä¸€ä¸‹", "è¯¦ç»†åˆ†æ", "æ·±å…¥åˆ†æ", "ç»¼åˆåˆ†æ",
            "æ€»ç»“", "æ€»ç»“ä¸€ä¸‹", "è¯¦ç»†æ€»ç»“", "å…¨é¢æ€»ç»“",
            "è§£é‡Š", "è¯¦ç»†è§£é‡Š", "è§£é‡Šä¸€ä¸‹", "è¯´æ˜", "é˜è¿°",
            "æ¯”è¾ƒ", "å¯¹æ¯”", "æ¯”è¾ƒåˆ†æ", "å¯¹æ¯”åˆ†æ",
            "è¯„ä»·", "è¯„ä¼°", "è¯„è®º", "ç‚¹è¯„",
            "æ¨ç†", "æ¨æ–­", "åˆ¤æ–­", "é¢„æµ‹",
            "å¤æ‚", "å›°éš¾", "æ·±å±‚", "é«˜çº§",
            "å¤šæ–¹é¢", "å…¨æ–¹ä½", "ç³»ç»Ÿæ€§", "ç»¼åˆæ€§",
            "åˆ›ä½œ", "å†™ä½œ", "ç¼–å†™", "ç¿»è¯‘", "è½¬æ¢", "æ”¹å†™", "æ¶¦è‰²"
        }
        
        # è½»é‡çº§ä»»åŠ¡å…³é”®è¯
        self.light_keywords = {
            "æ˜¯ä»€ä¹ˆ", "ä»€ä¹ˆæ˜¯", "å®šä¹‰", "å«ä¹‰",
            "ç®€å•", "å¿«é€Ÿ", "ç›´æ¥", "ç®€è¦",
            "åˆ—å‡º", "åˆ—ä¸¾", "æšä¸¾",
            "æŸ¥æ‰¾", "æœç´¢", "æ‰¾åˆ°", "è·å–",
            "ç¡®è®¤", "éªŒè¯", "æ£€æŸ¥",
            "æ˜¯å¦", "æœ‰æ²¡æœ‰", "èƒ½å¦", "å¯ä»¥å—",
            "å¤šå°‘", "å‡ ä¸ª", "æ•°é‡",
            "æ—¶é—´", "æ—¥æœŸ", "å¹´ä»½",
            "åœ°ç‚¹", "ä½ç½®", "åœ¨å“ª",
            # æ·»åŠ åŸå­ç¬”è®°ç”Ÿæˆç›¸å…³å…³é”®è¯
            "åŸå­ç¬”è®°", "atomic note", "çŸ¥è¯†ç‚¹", "æå–", "extract",
            "å®ä½“", "entity", "å…³é”®è¯", "keyword", "æ¦‚å¿µ", "concept"
        }
        
        # åŸå­ç¬”è®°ç”Ÿæˆä»»åŠ¡çš„ç‰¹å¾æ¨¡å¼
        self.atomic_note_patterns = {
            "è¯·å°†ä»¥ä¸‹æ–‡æœ¬è½¬æ¢ä¸ºåŸå­ç¬”è®°",
            "è½¬æ¢ä¸ºåŸå­ç¬”è®°",
            "atomic note",
            "knowledge point",
            "extract entities",
            "extract keywords",
            "æå–å®ä½“",
            "æå–å…³é”®è¯",
            "çŸ¥è¯†æå–",
            "çŸ¥è¯†ç‚¹",
            "è½¬æ¢ä¸ºçŸ¥è¯†ç‚¹",
            "æ–‡æ¡£å†…å®¹è½¬æ¢",
            "ç»“æ„åŒ–æå–",
            "ä¿¡æ¯æå–",
            "content", "keywords", "entities",  # JSONå­—æ®µå
            "ç”Ÿæˆç»“æ„åŒ–",
            "è¾“å‡ºä¸ºJSON"
        }
        
        # ä»»åŠ¡åˆ†ç±»é˜ˆå€¼
        self.length_threshold = 100  # å­—ç¬¦é•¿åº¦é˜ˆå€¼
        self.complexity_indicators = {
            "å› ä¸º", "ç”±äº", "æ‰€ä»¥", "å› æ­¤", "ç„¶è€Œ", "ä½†æ˜¯", "ä¸è¿‡",
            "é¦–å…ˆ", "å…¶æ¬¡", "æœ€å", "å¦å¤–", "æ­¤å¤–", "è€Œä¸”",
            "å¦‚æœ", "å‡å¦‚", "å½“", "åœ¨", "é€šè¿‡", "æ ¹æ®"
        }
    
    def classify_task(self, query: str, context: str = "") -> str:
        """åˆ†ç±»ä»»åŠ¡ç±»å‹
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯
            
        Returns:
            'heavy' æˆ– 'light'
        """
        if not query:
            return "light"
        
        query_lower = query.lower()
        full_text = f"{query} {context}".strip()
        
        # ç‰¹æ®Šæ£€æµ‹ï¼šåŸå­ç¬”è®°ç”Ÿæˆä»»åŠ¡ - ä¿®æ”¹ä¸ºheavyä»»åŠ¡ä»¥ä½¿ç”¨LM Studio
        for pattern in self.atomic_note_patterns:
            if pattern.lower() in query_lower or pattern.lower() in context.lower():
                logger.debug(f"Detected atomic note generation task, classifying as heavy to use LM Studio: {pattern}")
                return "heavy"
        
        # æ£€æµ‹æ˜¯å¦åŒ…å«JSONæ ¼å¼è¦æ±‚ï¼ˆåŸå­ç¬”è®°ç”Ÿæˆçš„ç‰¹å¾ï¼‰- ä¿®æ”¹ä¸ºheavyä»»åŠ¡
        if ("json" in query_lower and ("content" in query_lower or "keywords" in query_lower or "entities" in query_lower)) or \
           ("JSONæ ¼å¼" in query or "jsonæ ¼å¼" in query_lower):
            logger.debug("Detected JSON format requirement for structured extraction, classifying as heavy to use LM Studio")
            return "heavy"
        
        # 1. å…³é”®è¯åŒ¹é…
        heavy_score = sum(1 for keyword in self.heavy_keywords if keyword in query)
        light_score = sum(1 for keyword in self.light_keywords if keyword in query)
        
        # 2. é•¿åº¦åˆ¤æ–­
        length_score = 1 if len(full_text) > self.length_threshold else 0
        
        # 3. å¤æ‚åº¦æŒ‡æ ‡
        complexity_score = sum(1 for indicator in self.complexity_indicators if indicator in query)
        
        # 4. ä¸­æ–‡å­—ç¬¦æ¯”ä¾‹ï¼ˆä¸­æ–‡æŸ¥è¯¢é€šå¸¸æ›´å¤æ‚ï¼‰
        chinese_chars = sum(1 for char in query if '\u4e00' <= char <= '\u9fff')
        chinese_ratio = chinese_chars / len(query) if query else 0
        chinese_score = 1 if chinese_ratio > 0.5 and len(query) > 50 else 0
        
        # ç»¼åˆè¯„åˆ†
        total_heavy_score = heavy_score + length_score + complexity_score + chinese_score
        
        # å†³ç­–é€»è¾‘
        if light_score > heavy_score and total_heavy_score <= 1:
            return "light"
        elif total_heavy_score >= 2:
            return "heavy"
        elif len(full_text) > 200:  # é•¿æ–‡æœ¬å€¾å‘äºé‡é‡çº§
            return "heavy"
        else:
            return "light"


class HybridLLMDispatcher:
    """æ··åˆLLMè°ƒåº¦å™¨ï¼Œæ™ºèƒ½è·¯ç”±æŸ¥è¯¢åˆ°åˆé€‚çš„æ¨¡å‹"""
    
    _instance = None
    _initialized = False
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(HybridLLMDispatcher, cls).__new__(cls)
        return cls._instance
    
    def __init__(self):
        # é¿å…é‡å¤åˆå§‹åŒ–
        if HybridLLMDispatcher._initialized:
            return
            
        self.classifier = TaskClassifier()
        self.ollama_client = None
        self.multi_model_client = None
        
        # ä»é…ç½®ä¸­è·å–æ··åˆLLMè®¾ç½®
        self.mode = config.get("llm.hybrid_llm.mode", "task_division")
        
        # åˆå§‹åŒ–è½»é‡çº§ä»»åŠ¡å®¢æˆ·ç«¯ï¼ˆOllamaï¼‰
        light_config = config.get("llm.hybrid_llm.light_tasks", {})
        if light_config:
            from .ollama_client import OllamaClient
            self.ollama_client = OllamaClient(
                base_url=light_config.get("base_url", "http://localhost:11434"),
                model=light_config.get("model", "qwen2.5:latest")
            )
        
        # åˆå§‹åŒ–é‡é‡çº§ä»»åŠ¡å®¢æˆ·ç«¯ï¼ˆLM Studioï¼‰
        heavy_config = config.get("llm.hybrid_llm.heavy_tasks", {})
        if heavy_config:
            # ä¸´æ—¶ä¿®æ”¹é…ç½®ä»¥åˆå§‹åŒ–MultiModelClient
            original_config = {
                "llm.multi_model.instance_count": config.get("llm.multi_model.instance_count"),
                "llm.multi_model.model_name": config.get("llm.multi_model.model_name"),
                "llm.multi_model.base_port": config.get("llm.multi_model.base_port")
            }
            
            # è®¾ç½®é‡é‡çº§ä»»åŠ¡çš„é…ç½®
            config.set("llm.multi_model.instance_count", heavy_config.get("instances", 2))
            config.set("llm.multi_model.model_name", heavy_config.get("model", "openai/gpt-oss-20b"))
            config.set("llm.multi_model.base_port", 1234)  # LM Studioé»˜è®¤ç«¯å£
            
            self.multi_model_client = MultiModelClient()
            
            # æ¢å¤åŸå§‹é…ç½®
            for key, value in original_config.items():
                if value is not None:
                    config.set(key, value)
        
        HybridLLMDispatcher._initialized = True
        logger.info(f"HybridLLMDispatcher initialized with mode: {self.mode}")
        logger.info(f"Ollama client: {'âœ“' if self.ollama_client else 'âœ—'}")
        logger.info(f"MultiModel client: {'âœ“' if self.multi_model_client else 'âœ—'}")
    
    def is_available(self) -> bool:
        """æ£€æŸ¥HybridLLMDispatcheræ˜¯å¦å¯ç”¨"""
        ollama_available = False
        multi_model_available = False
        
        # æ£€æŸ¥Ollamaå®¢æˆ·ç«¯å¯ç”¨æ€§
        if self.ollama_client:
            try:
                ollama_available = self.ollama_client.is_available()
            except Exception as e:
                logger.warning(f"Ollama client availability check failed: {e}")
                ollama_available = False
        
        # æ£€æŸ¥MultiModelå®¢æˆ·ç«¯å¯ç”¨æ€§
        if self.multi_model_client:
            try:
                multi_model_available = self.multi_model_client.is_available()
            except Exception as e:
                logger.warning(f"MultiModel client availability check failed: {e}")
                multi_model_available = False
        
        # è‡³å°‘æœ‰ä¸€ä¸ªå®¢æˆ·ç«¯å¯ç”¨å³è®¤ä¸ºHybridLLMDispatcherå¯ç”¨
        is_available = ollama_available or multi_model_available
        
        if not is_available:
            logger.error("HybridLLMDispatcher: No available clients")
        
        return is_available
    
    def generate(self, query: str, system_prompt: str = None, **kwargs) -> str:
        """ç”Ÿæˆæ–‡æœ¬ï¼ˆä¸»è¦æ¥å£ï¼‰"""
        return self.process_single(query, system_prompt, **kwargs)
    
    def process_single(self, query: str, system_prompt: str = None, **kwargs) -> str:
        """å¤„ç†å•ä¸ªæŸ¥è¯¢"""
        if self.mode == "task_division":
            return self._process_task_division(query, system_prompt, **kwargs)
        elif self.mode == "competitive":
            return self._process_competitive(query, system_prompt, **kwargs)
        else:
            raise ValueError(f"Unsupported hybrid mode: {self.mode}")
    
    def process_batch(self, queries: List[str], system_prompt: str = None, **kwargs) -> List[str]:
        """æ‰¹é‡å¤„ç†æŸ¥è¯¢"""
        if not queries:
            return []
        
        # åˆ†ç±»æ‰€æœ‰æŸ¥è¯¢
        light_queries = []
        heavy_queries = []
        query_mapping = {}  # è®°å½•åŸå§‹ç´¢å¼•
        
        for i, query in enumerate(queries):
            task_type = self.classifier.classify_task(query)
            if task_type == "light":
                light_queries.append((i, query))
            else:
                heavy_queries.append((i, query))
            query_mapping[i] = task_type
        
        logger.info(f"Batch processing: {len(light_queries)} light tasks, {len(heavy_queries)} heavy tasks")
        
        # å¹¶è¡Œå¤„ç†
        results = [None] * len(queries)
        
        with ThreadPoolExecutor(max_workers=2) as executor:
            futures = []
            
            # æäº¤è½»é‡çº§ä»»åŠ¡
            if light_queries and self.ollama_client:
                light_prompts = [q[1] for q in light_queries]
                future_light = executor.submit(
                    self.ollama_client.batch_generate,
                    light_prompts,
                    system_prompt,
                    **kwargs
                )
                futures.append(("light", future_light, light_queries))
            
            # æäº¤é‡é‡çº§ä»»åŠ¡
            if heavy_queries and self.multi_model_client:
                heavy_prompts = [q[1] for q in heavy_queries]
                future_heavy = executor.submit(
                    self.multi_model_client.generate_parallel,
                    heavy_prompts,
                    system_prompt,
                    **kwargs
                )
                futures.append(("heavy", future_heavy, heavy_queries))
            
            # æ”¶é›†ç»“æœ
            for task_type, future, query_list in futures:
                try:
                    task_results = future.result()
                    for (original_idx, _), result in zip(query_list, task_results):
                        results[original_idx] = result
                except Exception as e:
                    logger.error(f"Batch {task_type} processing failed: {e}")
                    for original_idx, _ in query_list:
                        results[original_idx] = f"Error: {str(e)}"
        
        return results
    
    def _process_task_division(self, query: str, system_prompt: str = None, **kwargs) -> str:
        """ä»»åŠ¡åˆ’åˆ†æ¨¡å¼å¤„ç†"""
        task_type = self.classifier.classify_task(query)
        
        try:
            if task_type == "light" and self.ollama_client:
                logger.debug(f"Routing to Ollama (light): {query[:50]}...")
                return self.ollama_client.generate(query, system_prompt, **kwargs)
            elif task_type == "heavy" and self.multi_model_client:
                logger.debug(f"Routing to LM Studio (heavy): {query[:50]}...")
                return self.multi_model_client.generate(query, system_prompt, **kwargs)
            else:
                # å›é€€æœºåˆ¶
                return self._fallback_generate(query, system_prompt, **kwargs)
        except Exception as e:
            logger.error(f"Task division processing failed: {e}")
            return self._fallback_generate(query, system_prompt, **kwargs)
    
    def _process_competitive(self, query: str, system_prompt: str = None, **kwargs) -> str:
        """ç«äº‰æ¨¡å¼å¤„ç†ï¼ˆè¿”å›æœ€å¿«çš„ç»“æœï¼‰"""
        if not (self.ollama_client and self.multi_model_client):
            return self._fallback_generate(query, system_prompt, **kwargs)
        
        with ThreadPoolExecutor(max_workers=2) as executor:
            # åŒæ—¶æäº¤åˆ°ä¸¤ä¸ªå®¢æˆ·ç«¯
            future_ollama = executor.submit(self.ollama_client.generate, query, system_prompt, **kwargs)
            future_multi = executor.submit(self.multi_model_client.generate, query, system_prompt, **kwargs)
            
            # è¿”å›æœ€å…ˆå®Œæˆçš„ç»“æœ
            for future in as_completed([future_ollama, future_multi]):
                try:
                    result = future.result()
                    if result and result.strip():  # ç¡®ä¿ç»“æœéç©º
                        return result
                except Exception as e:
                    logger.warning(f"Competitive processing error: {e}")
                    continue
        
        # å¦‚æœéƒ½å¤±è´¥ï¼Œä½¿ç”¨å›é€€æœºåˆ¶
        return self._fallback_generate(query, system_prompt, **kwargs)
    
    def _fallback_generate(self, query: str, system_prompt: str = None, **kwargs) -> str:
        """å›é€€ç”Ÿæˆæœºåˆ¶"""
        # ä¼˜å…ˆå°è¯•Ollamaï¼ˆæ›´ç¨³å®šï¼‰
        if self.ollama_client:
            try:
                return self.ollama_client.generate(query, system_prompt, **kwargs)
            except Exception as e:
                logger.warning(f"Ollama fallback failed: {e}")
        
        # ç„¶åå°è¯•MultiModel
        if self.multi_model_client:
            try:
                return self.multi_model_client.generate(query, system_prompt, **kwargs)
            except Exception as e:
                logger.warning(f"MultiModel fallback failed: {e}")
        
        # æœ€åçš„å›é€€
        logger.error("All LLM clients failed")
        return "Error: No available LLM client"
    
    def generate_final_answer(self, prompt: str, **kwargs) -> str:
        """Generate final answer using appropriate model based on task classification"""
        try:
            # Classify the task to determine which model to use
            task_type = self.classifier.classify_task(prompt)
            
            if task_type == "light" and self.ollama_client:
                logger.info(f"Routing final answer generation to light model (Ollama): {prompt[:100]}...")
                return self.ollama_client.generate(prompt, FINAL_ANSWER_SYSTEM_PROMPT, **kwargs)
            elif task_type == "heavy" and self.multi_model_client:
                logger.info(f"Routing final answer generation to heavy model (LM Studio): {prompt[:100]}...")
                return self.multi_model_client.generate(prompt, FINAL_ANSWER_SYSTEM_PROMPT, **kwargs)
            else:
                return self._fallback_generate(prompt, FINAL_ANSWER_SYSTEM_PROMPT, **kwargs)
        except Exception as e:
            logger.error(f"Final answer generation failed: {e}")
            return self._fallback_generate(prompt, FINAL_ANSWER_SYSTEM_PROMPT, **kwargs)
    
    def evaluate_answer(self, question: str, answer: str, context: str = "") -> dict:
        """Evaluate answer quality using the heavy model for comprehensive analysis"""
        try:
            # Always use heavy model for evaluation as it requires sophisticated reasoning
            logger.info(f"Using heavy model for answer evaluation: {question[:100]}...")
            
            # Prepare evaluation prompt
            eval_prompt = EVALUATE_ANSWER_PROMPT.format(
                query=question,
                answer=answer,
                context=context
            )
            
            if self.multi_model_client:
                response = self.multi_model_client.generate(eval_prompt, EVALUATE_ANSWER_SYSTEM_PROMPT)
            elif self.ollama_client:
                response = self.ollama_client.generate(eval_prompt, EVALUATE_ANSWER_SYSTEM_PROMPT)
            else:
                raise Exception("No available LLM client")
            
            # Parse the response (assuming it returns JSON format)
            try:
                import json
                return json.loads(response)
            except json.JSONDecodeError:
                # If not JSON, return a basic structure
                return {
                    "score": 0.7,
                    "reasoning": response,
                    "confidence": 0.5
                }
                
        except Exception as e:
            logger.error(f"Answer evaluation failed: {e}")
            # Return a basic evaluation structure on failure
            return {
                "score": 0.0,
                "reasoning": f"Evaluation failed: {str(e)}",
                "confidence": 0.0
            }
    
    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ··åˆæ¨¡å‹ä¿¡æ¯"""
        info = {
            'provider': 'hybrid_llm',
            'mode': self.mode,
            'light_client': 'available' if self.ollama_client else 'unavailable',
            'heavy_client': 'available' if self.multi_model_client else 'unavailable'
        }
        
        if self.ollama_client:
            info['light_model'] = getattr(self.ollama_client, 'model', 'unknown')
        
        if self.multi_model_client:
            multi_info = self.multi_model_client.get_model_info()
            info['heavy_model_info'] = multi_info
        
        return info
    
    def shutdown(self):
        """å…³é—­æ··åˆLLMè°ƒåº¦å™¨"""
        logger.info("Shutting down HybridLLMDispatcher...")
        
        if self.multi_model_client:
            try:
                self.multi_model_client.shutdown()
            except Exception as e:
                logger.error(f"Error shutting down multi_model_client: {e}")
        
        # Ollama clienté€šå¸¸ä¸éœ€è¦ç‰¹æ®Šçš„å…³é—­æ“ä½œ
        logger.info("HybridLLMDispatcher shutdown complete")


# ä¸ºäº†å‘åå…¼å®¹ï¼Œä¿ç•™åŸæœ‰çš„ç±»ååˆ«å
MultiLMStudioClient = MultiModelClient