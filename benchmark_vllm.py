#!/usr/bin/env python3
"""
vLLM å‹æµ‹è„šæœ¬

ä½¿ç”¨ asyncio + aiohttp å¹¶å‘è¯·æ±‚æµ‹è¯• vLLM æ€§èƒ½
ç»Ÿè®¡ p50ã€p95ã€å¹³å‡å»¶è¿Ÿå’Œååé‡
"""

import asyncio
import aiohttp
import time
import json
import statistics
import argparse
from typing import List, Dict, Any
from dataclasses import dataclass
from loguru import logger


@dataclass
class BenchmarkResult:
    """å‹æµ‹ç»“æœæ•°æ®ç±»"""
    total_requests: int
    successful_requests: int
    failed_requests: int
    total_time: float
    avg_latency: float
    p50_latency: float
    p95_latency: float
    p99_latency: float
    throughput: float  # requests per second
    error_rate: float
    latencies: List[float]


class VLLMBenchmark:
    """vLLM å‹æµ‹ç±»"""
    
    def __init__(self, base_url: str = "http://127.0.0.1:8001/v1", model: str = "qwen2_5_0_5b"):
        self.base_url = base_url
        self.model = model
        self.api_key = "EMPTY"
        
        # æµ‹è¯•ç”¨çš„æ¶ˆæ¯æ¨¡æ¿
        self.test_messages = [
            "è¯·ç®€å•ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½ã€‚",
            "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
            "æ·±åº¦å­¦ä¹ æœ‰å“ªäº›åº”ç”¨ï¼Ÿ",
            "è¯·è§£é‡Šä¸€ä¸‹ç¥ç»ç½‘ç»œçš„å·¥ä½œåŸç†ã€‚",
            "è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯æœ‰å“ªäº›ï¼Ÿ",
            "è®¡ç®—æœºè§†è§‰çš„ä¸»è¦ä»»åŠ¡æ˜¯ä»€ä¹ˆï¼Ÿ",
            "å¼ºåŒ–å­¦ä¹ æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿ",
            "å¤§è¯­è¨€æ¨¡å‹çš„ä¼˜åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ",
            "AI åœ¨åŒ»ç–—é¢†åŸŸæœ‰å“ªäº›åº”ç”¨ï¼Ÿ",
            "è¯·è°ˆè°ˆ AI çš„å‘å±•å‰æ™¯ã€‚"
        ]
    
    async def single_request(self, session: aiohttp.ClientSession, message: str, 
                           max_tokens: int = 256, temperature: float = 0.0) -> Dict[str, Any]:
        """å‘é€å•ä¸ªè¯·æ±‚
        
        Args:
            session: aiohttp ä¼šè¯
            message: è¯·æ±‚æ¶ˆæ¯
            max_tokens: æœ€å¤§ token æ•°
            temperature: æ¸©åº¦å‚æ•°
            
        Returns:
            åŒ…å«å“åº”æ—¶é—´å’Œç»“æœçš„å­—å…¸
        """
        url = f"{self.base_url}/chat/completions"
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": self.model,
            "messages": [{"role": "user", "content": message}],
            "max_tokens": max_tokens,
            "temperature": temperature,
            "stream": False
        }
        
        start_time = time.time()
        
        try:
            async with session.post(url, headers=headers, json=payload, timeout=60) as response:
                end_time = time.time()
                latency = end_time - start_time
                
                if response.status == 200:
                    result = await response.json()
                    content = result['choices'][0]['message']['content']
                    return {
                        'success': True,
                        'latency': latency,
                        'response_length': len(content),
                        'content': content[:100] + '...' if len(content) > 100 else content
                    }
                else:
                    error_text = await response.text()
                    return {
                        'success': False,
                        'latency': latency,
                        'error': f"HTTP {response.status}: {error_text}"
                    }
                    
        except Exception as e:
            end_time = time.time()
            latency = end_time - start_time
            return {
                'success': False,
                'latency': latency,
                'error': str(e)
            }
    
    async def run_benchmark(self, concurrency: int = 32, total_requests: int = 100, 
                          max_tokens: int = 256, temperature: float = 0.0) -> BenchmarkResult:
        """è¿è¡Œå‹æµ‹
        
        Args:
            concurrency: å¹¶å‘æ•°
            total_requests: æ€»è¯·æ±‚æ•°
            max_tokens: æœ€å¤§ token æ•°
            temperature: æ¸©åº¦å‚æ•°
            
        Returns:
            å‹æµ‹ç»“æœ
        """
        print(f"\nå¼€å§‹å‹æµ‹: å¹¶å‘æ•°={concurrency}, æ€»è¯·æ±‚æ•°={total_requests}")
        print(f"å‚æ•°: max_tokens={max_tokens}, temperature={temperature}")
        print("-" * 60)
        
        # åˆ›å»ºè¯·æ±‚ä»»åŠ¡
        messages = [self.test_messages[i % len(self.test_messages)] for i in range(total_requests)]
        
        # é™åˆ¶å¹¶å‘æ•°
        semaphore = asyncio.Semaphore(concurrency)
        
        async def bounded_request(session, message):
            async with semaphore:
                return await self.single_request(session, message, max_tokens, temperature)
        
        # æ‰§è¡Œå‹æµ‹
        start_time = time.time()
        
        connector = aiohttp.TCPConnector(limit=concurrency * 2, limit_per_host=concurrency * 2)
        timeout = aiohttp.ClientTimeout(total=120)
        
        async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
            tasks = [bounded_request(session, message) for message in messages]
            results = await asyncio.gather(*tasks, return_exceptions=True)
        
        end_time = time.time()
        total_time = end_time - start_time
        
        # ç»Ÿè®¡ç»“æœ
        successful_results = []
        failed_results = []
        latencies = []
        
        for result in results:
            if isinstance(result, Exception):
                failed_results.append({'error': str(result), 'latency': 0})
            elif result['success']:
                successful_results.append(result)
                latencies.append(result['latency'])
            else:
                failed_results.append(result)
                latencies.append(result['latency'])
        
        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        successful_count = len(successful_results)
        failed_count = len(failed_results)
        
        if latencies:
            avg_latency = statistics.mean(latencies)
            p50_latency = statistics.median(latencies)
            p95_latency = statistics.quantiles(latencies, n=20)[18] if len(latencies) >= 20 else max(latencies)
            p99_latency = statistics.quantiles(latencies, n=100)[98] if len(latencies) >= 100 else max(latencies)
        else:
            avg_latency = p50_latency = p95_latency = p99_latency = 0
        
        throughput = successful_count / total_time if total_time > 0 else 0
        error_rate = failed_count / total_requests * 100
        
        return BenchmarkResult(
            total_requests=total_requests,
            successful_requests=successful_count,
            failed_requests=failed_count,
            total_time=total_time,
            avg_latency=avg_latency,
            p50_latency=p50_latency,
            p95_latency=p95_latency,
            p99_latency=p99_latency,
            throughput=throughput,
            error_rate=error_rate,
            latencies=latencies
        )
    
    def print_result(self, result: BenchmarkResult, concurrency: int):
        """æ‰“å°å‹æµ‹ç»“æœ
        
        Args:
            result: å‹æµ‹ç»“æœ
            concurrency: å¹¶å‘æ•°
        """
        print(f"\nğŸ“Š å‹æµ‹ç»“æœ (å¹¶å‘æ•°: {concurrency})")
        print("=" * 60)
        print(f"æ€»è¯·æ±‚æ•°:     {result.total_requests}")
        print(f"æˆåŠŸè¯·æ±‚æ•°:   {result.successful_requests}")
        print(f"å¤±è´¥è¯·æ±‚æ•°:   {result.failed_requests}")
        print(f"é”™è¯¯ç‡:       {result.error_rate:.2f}%")
        print(f"æ€»è€—æ—¶:       {result.total_time:.2f}s")
        print("")
        print("ğŸ“ˆ å»¶è¿Ÿç»Ÿè®¡:")
        print(f"å¹³å‡å»¶è¿Ÿ:     {result.avg_latency:.3f}s")
        print(f"P50 å»¶è¿Ÿ:     {result.p50_latency:.3f}s")
        print(f"P95 å»¶è¿Ÿ:     {result.p95_latency:.3f}s")
        print(f"P99 å»¶è¿Ÿ:     {result.p99_latency:.3f}s")
        print("")
        print("ğŸš€ ååé‡:")
        print(f"QPS:          {result.throughput:.2f} requests/s")
        print(f"æ¯åˆ†é’Ÿè¯·æ±‚æ•°: {result.throughput * 60:.0f} requests/min")
    
    async def progressive_benchmark(self, concurrency_levels: List[int] = None, 
                                  requests_per_level: int = 100):
        """æ¸è¿›å¼å‹æµ‹
        
        Args:
            concurrency_levels: å¹¶å‘çº§åˆ«åˆ—è¡¨
            requests_per_level: æ¯ä¸ªçº§åˆ«çš„è¯·æ±‚æ•°
        """
        if concurrency_levels is None:
            concurrency_levels = [1, 4, 8, 16, 32, 64, 128]
        
        print("ğŸ”¥ å¼€å§‹æ¸è¿›å¼å‹æµ‹")
        print(f"å¹¶å‘çº§åˆ«: {concurrency_levels}")
        print(f"æ¯çº§åˆ«è¯·æ±‚æ•°: {requests_per_level}")
        
        results = []
        
        for concurrency in concurrency_levels:
            try:
                result = await self.run_benchmark(
                    concurrency=concurrency,
                    total_requests=requests_per_level
                )
                results.append((concurrency, result))
                self.print_result(result, concurrency)
                
                # æ£€æŸ¥é”™è¯¯ç‡ï¼Œå¦‚æœå¤ªé«˜å°±åœæ­¢
                if result.error_rate > 50:
                    print(f"\nâš ï¸  é”™è¯¯ç‡è¿‡é«˜ ({result.error_rate:.1f}%)ï¼Œåœæ­¢æµ‹è¯•")
                    break
                
                # çŸ­æš‚ä¼‘æ¯
                await asyncio.sleep(2)
                
            except Exception as e:
                print(f"\nâŒ å¹¶å‘æ•° {concurrency} æµ‹è¯•å¤±è´¥: {e}")
                break
        
        # è¾“å‡ºæ±‡æ€»
        self.print_summary(results)
    
    def print_summary(self, results: List[tuple]):
        """æ‰“å°æ±‡æ€»ç»“æœ
        
        Args:
            results: ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º (concurrency, BenchmarkResult)
        """
        if not results:
            return
        
        print("\n" + "=" * 80)
        print("ğŸ“‹ å‹æµ‹æ±‡æ€»")
        print("=" * 80)
        print(f"{'å¹¶å‘æ•°':<8} {'QPS':<10} {'å¹³å‡å»¶è¿Ÿ':<12} {'P95å»¶è¿Ÿ':<12} {'é”™è¯¯ç‡':<10} {'çŠ¶æ€':<8}")
        print("-" * 80)
        
        best_qps = 0
        best_concurrency = 0
        
        for concurrency, result in results:
            status = "âœ…" if result.error_rate < 5 else "âš ï¸" if result.error_rate < 20 else "âŒ"
            
            print(f"{concurrency:<8} {result.throughput:<10.2f} {result.avg_latency:<12.3f} "
                  f"{result.p95_latency:<12.3f} {result.error_rate:<10.1f}% {status:<8}")
            
            if result.error_rate < 10 and result.throughput > best_qps:
                best_qps = result.throughput
                best_concurrency = concurrency
        
        print("-" * 80)
        if best_concurrency > 0:
            print(f"ğŸ† æœ€ä½³é…ç½®: å¹¶å‘æ•° {best_concurrency}, QPS {best_qps:.2f}")
        
        print("\nğŸ’¡ å»ºè®®:")
        print("- ç›‘æ§ GPU ä½¿ç”¨ç‡: nvidia-smi æˆ– nvtop")
        print("- è§‚å¯Ÿå†…å­˜ä½¿ç”¨æƒ…å†µ")
        print("- æ ¹æ®é”™è¯¯ç‡è°ƒæ•´å¹¶å‘æ•°")
        print("- è€ƒè™‘å¢åŠ  --tensor-parallel-size ä»¥åˆ©ç”¨å¤šGPU")


async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="vLLM å‹æµ‹å·¥å…·")
    parser.add_argument("--base-url", default="http://127.0.0.1:8001/v1", help="vLLM API åŸºç¡€URL")
    parser.add_argument("--model", default="qwen2_5_0_5b", help="æ¨¡å‹åç§°")
    parser.add_argument("--concurrency", type=int, help="å•æ¬¡æµ‹è¯•å¹¶å‘æ•°")
    parser.add_argument("--requests", type=int, default=100, help="æ€»è¯·æ±‚æ•°")
    parser.add_argument("--progressive", action="store_true", help="æ¸è¿›å¼å‹æµ‹")
    parser.add_argument("--max-tokens", type=int, default=256, help="æœ€å¤§tokenæ•°")
    parser.add_argument("--temperature", type=float, default=0.0, help="æ¸©åº¦å‚æ•°")
    
    args = parser.parse_args()
    
    benchmark = VLLMBenchmark(base_url=args.base_url, model=args.model)
    
    # é¦–å…ˆæ£€æŸ¥æœåŠ¡å¯ç”¨æ€§
    print("ğŸ” æ£€æŸ¥ vLLM æœåŠ¡å¯ç”¨æ€§...")
    try:
        async with aiohttp.ClientSession() as session:
            test_result = await benchmark.single_request(session, "Hello", 10, 0.0)
            if not test_result['success']:
                print(f"âŒ vLLM æœåŠ¡ä¸å¯ç”¨: {test_result.get('error', 'Unknown error')}")
                print("\nè¯·ç¡®ä¿ vLLM æœåŠ¡æ­£åœ¨è¿è¡Œ:")
                print(f"curl {args.base_url.replace('/v1', '')}/v1/models")
                return
            else:
                print("âœ… vLLM æœåŠ¡å¯ç”¨")
    except Exception as e:
        print(f"âŒ æ— æ³•è¿æ¥åˆ° vLLM æœåŠ¡: {e}")
        return
    
    if args.progressive:
        # æ¸è¿›å¼å‹æµ‹
        await benchmark.progressive_benchmark(requests_per_level=args.requests)
    elif args.concurrency:
        # å•æ¬¡å‹æµ‹
        result = await benchmark.run_benchmark(
            concurrency=args.concurrency,
            total_requests=args.requests,
            max_tokens=args.max_tokens,
            temperature=args.temperature
        )
        benchmark.print_result(result, args.concurrency)
    else:
        # é»˜è®¤æ¸è¿›å¼å‹æµ‹
        await benchmark.progressive_benchmark(requests_per_level=args.requests)


if __name__ == "__main__":
    asyncio.run(main())