#!/usr/bin/env python3
"""
vLLM Provider æµ‹è¯•è„šæœ¬

ç”¨äºæµ‹è¯• vLLM OpenAI å…¼å®¹æ¥å£çš„è¿æ¥å’ŒåŠŸèƒ½
"""

import asyncio
import time
from typing import List
from loguru import logger

from llm.factory import LLMFactory
from llm.providers.vllm_openai import VLLMOpenAIProvider
from config import config


def test_vllm_connection():
    """æµ‹è¯• vLLM è¿æ¥"""
    print("\n=== æµ‹è¯• vLLM è¿æ¥ ===")
    
    try:
        # ç›´æ¥åˆ›å»º vLLM provider
        provider = VLLMOpenAIProvider(
            base_url="http://127.0.0.1:8001/v1",
            model="qwen2_5_0_5b",
            api_key="EMPTY"
        )
        
        # æ£€æŸ¥å¯ç”¨æ€§
        is_available = provider.is_available()
        print(f"vLLM æœåŠ¡å¯ç”¨æ€§: {is_available}")
        
        if is_available:
            # è·å–æ¨¡å‹åˆ—è¡¨
            models = provider.list_models()
            print(f"å¯ç”¨æ¨¡å‹: {models}")
            
            # è·å–æ¨¡å‹ä¿¡æ¯
            info = provider.get_model_info()
            print(f"æ¨¡å‹ä¿¡æ¯: {info}")
        
        return is_available
        
    except Exception as e:
        print(f"è¿æ¥æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_vllm_chat():
    """æµ‹è¯• vLLM èŠå¤©åŠŸèƒ½"""
    print("\n=== æµ‹è¯• vLLM èŠå¤©åŠŸèƒ½ ===")
    
    try:
        provider = VLLMOpenAIProvider(
            base_url="http://127.0.0.1:8001/v1",
            model="qwen2_5_0_5b",
            api_key="EMPTY"
        )
        
        # æµ‹è¯•ç®€å•å¯¹è¯
        messages = [
            {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ç®€å•ä»‹ç»ä¸€ä¸‹è‡ªå·±ã€‚"}
        ]
        
        start_time = time.time()
        response = provider.chat(messages)
        end_time = time.time()
        
        print(f"å“åº”æ—¶é—´: {end_time - start_time:.2f}ç§’")
        print(f"å›å¤å†…å®¹: {response}")
        
        return True
        
    except Exception as e:
        print(f"èŠå¤©æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_vllm_stream():
    """æµ‹è¯• vLLM æµå¼è¾“å‡º"""
    print("\n=== æµ‹è¯• vLLM æµå¼è¾“å‡º ===")
    
    try:
        provider = VLLMOpenAIProvider(
            base_url="http://127.0.0.1:8001/v1",
            model="qwen2_5_0_5b",
            api_key="EMPTY"
        )
        
        messages = [
            {"role": "user", "content": "è¯·å†™ä¸€é¦–å…³äºäººå·¥æ™ºèƒ½çš„çŸ­è¯—ã€‚"}
        ]
        
        print("æµå¼è¾“å‡º:")
        start_time = time.time()
        full_response = ""
        
        for chunk in provider.stream(messages):
            print(chunk, end="", flush=True)
            full_response += chunk
        
        end_time = time.time()
        print(f"\n\nå“åº”æ—¶é—´: {end_time - start_time:.2f}ç§’")
        print(f"å®Œæ•´å›å¤é•¿åº¦: {len(full_response)} å­—ç¬¦")
        
        return True
        
    except Exception as e:
        print(f"æµå¼è¾“å‡ºæµ‹è¯•å¤±è´¥: {e}")
        return False


async def test_vllm_async():
    """æµ‹è¯• vLLM å¼‚æ­¥åŠŸèƒ½"""
    print("\n=== æµ‹è¯• vLLM å¼‚æ­¥åŠŸèƒ½ ===")
    
    try:
        provider = VLLMOpenAIProvider(
            base_url="http://127.0.0.1:8001/v1",
            model="qwen2_5_0_5b",
            api_key="EMPTY"
        )
        
        # å¹¶å‘æµ‹è¯•
        messages_list = [
            [{"role": "user", "content": f"è¯·ç”¨ä¸€å¥è¯æè¿°æ•°å­— {i}"}]
            for i in range(1, 6)
        ]
        
        start_time = time.time()
        
        # å¹¶å‘æ‰§è¡Œ
        tasks = [provider.async_chat(messages) for messages in messages_list]
        responses = await asyncio.gather(*tasks)
        
        end_time = time.time()
        
        print(f"å¹¶å‘è¯·æ±‚æ•°: {len(tasks)}")
        print(f"æ€»å“åº”æ—¶é—´: {end_time - start_time:.2f}ç§’")
        print(f"å¹³å‡å“åº”æ—¶é—´: {(end_time - start_time) / len(tasks):.2f}ç§’")
        
        for i, response in enumerate(responses, 1):
            print(f"å“åº” {i}: {response}")
        
        return True
        
    except Exception as e:
        print(f"å¼‚æ­¥æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_factory_integration():
    """æµ‹è¯•å·¥å‚æ¨¡å¼é›†æˆ"""
    print("\n=== æµ‹è¯•å·¥å‚æ¨¡å¼é›†æˆ ===")
    
    try:
        # é€šè¿‡å·¥å‚åˆ›å»º vLLM provider
        provider = LLMFactory.create_provider('vllm_openai')
        
        # æµ‹è¯•åŸºæœ¬åŠŸèƒ½
        response = provider.generate("è¯·è¯´ä¸€å¥è¯")
        print(f"å·¥å‚åˆ›å»ºçš„ provider å“åº”: {response}")
        
        # æµ‹è¯•å¯ç”¨æ€§æ£€æŸ¥
        available_providers = LLMFactory.get_available_providers()
        print(f"å¯ç”¨çš„ providers: {available_providers}")
        
        # è·å–æœ€ä½³ provider
        best_provider = LLMFactory.get_best_available_provider()
        print(f"æœ€ä½³ provider: {best_provider}")
        
        return True
        
    except Exception as e:
        print(f"å·¥å‚é›†æˆæµ‹è¯•å¤±è´¥: {e}")
        return False


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("vLLM Provider æµ‹è¯•å¼€å§‹")
    print("=" * 50)
    
    # æµ‹è¯•ç»“æœ
    results = {}
    
    # 1. è¿æ¥æµ‹è¯•
    results['connection'] = test_vllm_connection()
    
    # åªæœ‰è¿æ¥æˆåŠŸæ‰ç»§ç»­å…¶ä»–æµ‹è¯•
    if results['connection']:
        # 2. èŠå¤©åŠŸèƒ½æµ‹è¯•
        results['chat'] = test_vllm_chat()
        
        # 3. æµå¼è¾“å‡ºæµ‹è¯•
        results['stream'] = test_vllm_stream()
        
        # 4. å¼‚æ­¥åŠŸèƒ½æµ‹è¯•
        results['async'] = asyncio.run(test_vllm_async())
        
        # 5. å·¥å‚é›†æˆæµ‹è¯•
        results['factory'] = test_factory_integration()
    else:
        print("\nâš ï¸  vLLM æœåŠ¡ä¸å¯ç”¨ï¼Œè·³è¿‡å…¶ä»–æµ‹è¯•")
        print("è¯·ç¡®ä¿ vLLM æœåŠ¡æ­£åœ¨è¿è¡Œï¼š")
        print("python -m vllm.entrypoints.openai.api_server \\")
        print("  --model Qwen/Qwen2.5-0.5B-Instruct \\")
        print("  --served-model-name qwen2_5_0_5b \\")
        print("  --dtype float16 \\")
        print("  --max-model-len 4096 \\")
        print("  --gpu-memory-utilization 0.80 \\")
        print("  --port 8001")
    
    # è¾“å‡ºæµ‹è¯•ç»“æœ
    print("\n" + "=" * 50)
    print("æµ‹è¯•ç»“æœæ±‡æ€»:")
    for test_name, result in results.items():
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"  {test_name}: {status}")
    
    # æ€»ä½“ç»“æœ
    passed_tests = sum(results.values())
    total_tests = len(results)
    print(f"\næ€»ä½“ç»“æœ: {passed_tests}/{total_tests} æµ‹è¯•é€šè¿‡")
    
    if passed_tests == total_tests:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼vLLM provider é›†æˆæˆåŠŸ")
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®å’ŒæœåŠ¡çŠ¶æ€")


if __name__ == "__main__":
    main()