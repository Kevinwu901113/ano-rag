# vLLM 原子笔记生成迁移说明

## 迁移概述

本次迁移将原子笔记生成从 Ollama/LM Studio 迁移到 vLLM 双实例架构，以提升3万条短笔记生成的批吞吐量。

## 变更边界

### 变更范围
- **仅影响原子笔记生成链路**：只修改原子笔记生成相关的 Provider 和调用点
- **保持最终答案生成不变**：final answer 生成继续使用 LM Studio 的 gpt-oss 模型
- **保留旧 Provider**：保留 Ollama/LMStudio 客户端以便回滚

### 不变更部分
- 最终答案生成流程和配置
- 现有的业务逻辑和输出协议
- 其他 LLM 调用点（如查询重写、实体提取等）

## 技术架构

### 目标架构
- **双 GPU 部署**：两张 4090 各运行一个 vLLM 实例
- **端口分配**：
  - GPU0: http://127.0.0.1:8000/v1
  - GPU1: http://127.0.0.1:8001/v1
- **模型**：Qwen2.5-7B-Instruct
- **负载均衡**：多端点轮询与并发限流

### 性能优化
- **长度分桶调度**：按 token 长度分桶，提升批处理效率
- **并发控制**：每端点独立的 Semaphore 限流
- **失败切换**：端点故障时自动切换到其他可用端点

## 实施计划

1. **T0**: 分支与范围管控 ✅
2. **T1**: vLLM 双实例服务部署
3. **T2**: 配置层增加 vLLM Provider
4. **T3**: 实现 VllmOpenAIClient
5. **T4**: 长度分桶调度器
6. **T5**: 流水线集成

## 回滚策略

- **配置回滚**：修改 config.yaml 中的 provider 字段
- **代码回滚**：git revert 相关 PR
- **服务回滚**：停止 vLLM 服务，启动原有服务

## 风险控制

- **端口冲突**：准备备用端口 8002, 8003
- **驱动兼容性**：确认 CUDA/驱动版本
- **请求风暴**：通过 Semaphore 和全局并发限流防抖
- **输出一致性**：确保结构一致、语义等价

## 验收标准

- 双实例服务健康运行
- 原子笔记生成吞吐量提升 ≥15%
- 输出格式与旧链路完全兼容
- 异常时能断点恢复